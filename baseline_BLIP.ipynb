{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolazilio0/deepRiccy/blob/main/baseline_BLIP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOvgNpY7LwSU"
      },
      "source": [
        "# Deep riccy project\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "! tar -zxf /content/drive/MyDrive/refcocog.tar.gz\n",
        "! pip3 install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt \n",
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "local_path = '/content/refcocog/images/'\n",
        "local_annotations = '/content/refcocog/annotations/'"
      ],
      "metadata": {
        "id": "Ok2A7tepPI8K",
        "outputId": "8e04b304-8594-45de-e672-a6f3c190c1e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-a5fjf55v\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-a5fjf55v\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a9b1bf5920416aaeaec965c25dd9e8f98c864f16\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.65.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.15.2+cu118)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (16.0.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torchmetrics transformers"
      ],
      "metadata": {
        "id": "jAUH_mLxQGhz",
        "outputId": "4ae2a3a9-2747-4810-f5b3-7cbe4fc3095c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (0.11.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.1+cu118)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CzySrjXzHhN"
      },
      "source": [
        "## Env creation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiKX4oZgzKLR"
      },
      "outputs": [],
      "source": [
        "local_path = 'refcocog/images/'\n",
        "local_annotations = 'refcocog/annotations/'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install stanza"
      ],
      "metadata": {
        "id": "oEgkDAmlQZrW",
        "outputId": "b5c47bed-3057-4f06-b027-140fa2c8c26c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: stanza in /usr/local/lib/python3.10/dist-packages (1.5.0)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from stanza) (2.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stanza) (1.22.4)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from stanza) (3.20.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stanza) (2.27.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from stanza) (1.16.0)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stanza) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.3.0->stanza) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.3.0->stanza) (16.0.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->stanza) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "R4ZuKVoVzdFH"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "import torch\n",
        "from PIL import Image, ImageDraw\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from pkg_resources import packaging\n",
        "import clip\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import skimage\n",
        "import IPython.display\n",
        "\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "import torchmetrics as tm\n",
        "import torchvision\n",
        "from torchvision import ops\n",
        "import stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8K-ozdJYLrqS",
        "outputId": "7bcdbacd-8491-438e-9675-0e86f3ab6955",
        "colab": {
          "referenced_widgets": [
            "9ca574561552418e9e6a71a9cfecbb35",
            "8edfbe8be8634e76826647da0396ad55",
            "8c31faf9a0044bfc848eb393cef7df91",
            "7aa8ca9d2b6c40b39effdc4ed8a51705",
            "4064543a40ae466cb9701f15e51e7736",
            "d84ccafd58a7434db9f6e11084c53613",
            "c801569689364c44b5ff10736605e9b4",
            "04206e3fb54142ddbb6cd4b5170293ee",
            "88b42abf98b0482e9b273cf15d6e3e3e",
            "000f9f83cb6b405080dd166ce8e81a3f",
            "509db7be00f8438c999b78ce466b1f96",
            "57893c05392d4cf3996ddf9ad04dbf69",
            "bedb0d71d3fd4567a49e61dbc1f462ef",
            "8adc279d11ba48949a640793ed8c11be",
            "8072426ded574864bfea53fc90cd5ed6",
            "a5f7d673fba84c698905d27a6ce44d17",
            "231157d83caf41af975fb62278be3126",
            "4ed4d73f9383471ea1d66408009731dd",
            "76d3d8d5131541f4852147f327b672e8",
            "3b56d314e29846a89c94d4ebc744567c",
            "bdd15256a1624905a22de9d1ff08e977",
            "a4b30c0debc2408ab8725c3425760b03"
          ],
          "base_uri": "https://localhost:8080/",
          "height": 899
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ca574561552418e9e6a71a9cfecbb35"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloading these customized packages for language: en (English)...\n",
            "==============================\n",
            "| Processor       | Package  |\n",
            "------------------------------\n",
            "| tokenize        | partut   |\n",
            "| mwt             | partut   |\n",
            "| pos             | partut   |\n",
            "| lemma           | partut   |\n",
            "| depparse        | partut   |\n",
            "| pretrain        | partut   |\n",
            "| backward_charlm | 1billion |\n",
            "| forward_charlm  | 1billion |\n",
            "==============================\n",
            "\n",
            "INFO:stanza:File exists: /root/stanza_resources/en/tokenize/partut.pt\n",
            "INFO:stanza:File exists: /root/stanza_resources/en/mwt/partut.pt\n",
            "INFO:stanza:File exists: /root/stanza_resources/en/pos/partut.pt\n",
            "INFO:stanza:File exists: /root/stanza_resources/en/lemma/partut.pt\n",
            "INFO:stanza:File exists: /root/stanza_resources/en/depparse/partut.pt\n",
            "INFO:stanza:File exists: /root/stanza_resources/en/pretrain/partut.pt\n",
            "INFO:stanza:File exists: /root/stanza_resources/en/backward_charlm/1billion.pt\n",
            "INFO:stanza:File exists: /root/stanza_resources/en/forward_charlm/1billion.pt\n",
            "INFO:stanza:Finished downloading models and saved to /root/stanza_resources.\n",
            "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57893c05392d4cf3996ddf9ad04dbf69"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Loading these models for language: en (English):\n",
            "============================\n",
            "| Processor    | Package   |\n",
            "----------------------------\n",
            "| tokenize     | combined  |\n",
            "| pos          | combined  |\n",
            "| lemma        | combined  |\n",
            "| constituency | wsj       |\n",
            "| depparse     | combined  |\n",
            "| sentiment    | sstplus   |\n",
            "| ner          | ontonotes |\n",
            "============================\n",
            "\n",
            "INFO:stanza:Using device: cuda\n",
            "INFO:stanza:Loading: tokenize\n",
            "INFO:stanza:Loading: pos\n",
            "INFO:stanza:Loading: lemma\n",
            "INFO:stanza:Loading: constituency\n",
            "INFO:stanza:Loading: depparse\n",
            "INFO:stanza:Loading: sentiment\n",
            "INFO:stanza:Loading: ner\n",
            "INFO:stanza:Done loading processors!\n"
          ]
        }
      ],
      "source": [
        "stanza.download('en',package='partut')\n",
        "nlp = stanza.Pipeline(lang='en')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKNYTNptzWrx"
      },
      "source": [
        "## Baseline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "418bIOjizjPc"
      },
      "source": [
        "### Utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0KmNv2ieLrqU"
      },
      "outputs": [],
      "source": [
        "def remove_of(sentence):\n",
        " if \"the side of\" in sentence:\n",
        "    index=sentence.find(\"the side of\")\n",
        "    sentence=sentence[index+11:]\n",
        "    return sentence\n",
        " if \"the handle of\" in sentence:\n",
        "    index=sentence.find(\"the handle of\")\n",
        "    sentence=sentence[index+13:]\n",
        " if \"the bunch of\" in sentence:\n",
        "    index=sentence.find(\"the bunch of\")\n",
        "    sentence=sentence[index+12:]\n",
        "    return sentence   \n",
        " if \"the corner of\" in sentence:\n",
        "    index=sentence.find(\"the corner of\")\n",
        "    sentence=sentence[index+13:]\n",
        "    return sentence\n",
        " if \"the end of\" in sentence:\n",
        "    index=sentence.find(\"the end of\")\n",
        "    sentence=sentence[index+10:]\n",
        "    return sentence\n",
        " if \"the half of\" in sentence:\n",
        "    index=sentence.find(\"the half of\")\n",
        "    sentence=sentence[index+11:]\n",
        "    return sentence    \n",
        " if \"the edge of\" in sentence:\n",
        "    index=sentence.find(\"the edge of\")\n",
        "    sentence=sentence[index+11:]    \n",
        "    return sentence\n",
        " if \"the back of\" in sentence:\n",
        "    index=sentence.find(\"the back of\")\n",
        "    sentence=sentence[index+11:]\n",
        "    return sentence   \n",
        " if \"the smaller of\" in sentence:\n",
        "    index=sentence.find(\"the smaller of\")\n",
        "    sentence=sentence[index+14:]\n",
        "    return sentence    \n",
        " if \"the piece of\" in sentence:\n",
        "    index=sentence.find(\"the piece of\")\n",
        "    sentence=sentence[index+16:]\n",
        "    return sentence \n",
        " if \"the wing of\" in sentence:\n",
        "    index=sentence.find(\"the wing of\")\n",
        "    sentence=sentence[index+11:]    \n",
        "\n",
        " if \"the front of\" in sentence:\n",
        "    index=sentence.find(\"the front of\")\n",
        "    sentence=sentence[index+12:]   \n",
        "    return sentence \n",
        " if \"the back side of\" in sentence:\n",
        "    index=sentence.find(\"the back side of\")\n",
        "    sentence=sentence[index+16:]   \n",
        "    return sentence\n",
        " if \"the front side of\" in sentence:\n",
        "    index=sentence.find(\"the front side of\")\n",
        "    sentence=sentence[index+17:]   \n",
        "    return sentence \n",
        " if \"the left side of\" in sentence:\n",
        "    index=sentence.find(\"the left side of\")\n",
        "    sentence=sentence[index+16:]   \n",
        "    return sentence\n",
        " if \"the right side of\" in sentence:\n",
        "    index=sentence.find(\"the back side of\")\n",
        "    sentence=sentence[index+17:]   \n",
        "    return sentence\n",
        "  \n",
        " if \"the pile of\" in sentence:\n",
        "    index=sentence.find(\"the pile of\")\n",
        "    sentence=sentence[index+11:]    \n",
        "    return sentence\n",
        " if \"the pair of\" in sentence:\n",
        "    index=sentence.find(\"the pair of\")\n",
        "    sentence=sentence[index+11:] \n",
        "    return sentence   \n",
        " if \"the pieces of\" in sentence:\n",
        "    index=sentence.find(\"the pieces of\")\n",
        "    sentence=sentence[index+13:]   \n",
        "    return sentence \n",
        " if \"the intersection of\" in sentence:\n",
        "    index=sentence.find(\"the intersection of\")\n",
        "    sentence=sentence[index+19:]  \n",
        "    return sentence  \n",
        " if \"the middle of\" in sentence:\n",
        "    index=sentence.find(\"the middle of\")\n",
        "    sentence=sentence[index+13:]\n",
        "    return sentence    \n",
        " if \"the patch of\" in sentence:\n",
        "    index=sentence.find(\"the patch of\")\n",
        "    sentence=sentence[index+12:]    \n",
        "    return sentence\n",
        " if \"the couple of\" in sentence:\n",
        "    index=sentence.find(\"the couple of\")\n",
        "    sentence=sentence[index+12:]    \n",
        "    return sentence\n",
        " if \"the slice of\" in sentence:\n",
        "    index=sentence.find(\"the slice of\")\n",
        "    sentence=sentence[index+12:]    \n",
        "    return sentence\n",
        " if \"the tallest of\" in sentence:\n",
        "    index=sentence.find(\"the tallest of\")\n",
        "    sentence=sentence[index+14:]    \n",
        "    return sentence\n",
        " if \"the kind of\" in sentence:\n",
        "    index=sentence.find(\"the kind of\")\n",
        "    sentence=sentence[index+11:]\n",
        "    return sentence    \n",
        " if \"that is\" in sentence:\n",
        "    index=sentence.find(\"that is\")\n",
        "    sentence=sentence[:index]\n",
        "    return sentence\n",
        " if \"the part of\" in sentence:\n",
        "    index=sentence.find(\"the part of\")\n",
        "    sentence=sentence[index+11:]\n",
        "    return sentence\n",
        " if \"the corner of\" in sentence:\n",
        "    index=sentence.find(\"the corner of\")\n",
        "    sentence=sentence[index+13:]\n",
        "    return sentence\n",
        " if \"the half of\" in sentence:\n",
        "    index=sentence.find(\"the half of\")\n",
        "    sentence=sentence[index+11:]\n",
        "    return sentence\n",
        " if \"the top of\" in sentence:\n",
        "    index=sentence.find(\"the top of\")\n",
        "    sentence=sentence[index+10:]\n",
        "    return sentence\n",
        " return sentence\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fQp4X9aELrqV"
      },
      "outputs": [],
      "source": [
        "def sent_stanza_processing(sentence):\n",
        "    sentence = sentence.lower()\n",
        "    if sentence.startswith('there is '):\n",
        "        sentence = 'the ' + sentence[9:]\n",
        "    if sentence.startswith('this is '):\n",
        "        sentence = sentence[9:]\n",
        "    sentence = remove_of(sentence)\n",
        "\n",
        "    nlp_sent = sentence.lower()\n",
        "\n",
        "    # put the sentence in lower case\n",
        "    # if the sentence does not start with \"a\" or \"the\" insert it\n",
        "    x = nlp_sent.split(\" \")\n",
        "    if (x[0] != \"the\" and x[0] != \"a\"):\n",
        "        nlp_sent = \"the \" + nlp_sent\n",
        "\n",
        "    doc = nlp(nlp_sent)\n",
        "    # print nlp dependencies\n",
        "    # doc.sentences[0].print_dependencies()\n",
        "    # print(input[\"sentences\"][\"raw\"])\n",
        "    root = ''\n",
        "    phrase_upos = []\n",
        "    # get heads of words\n",
        "    heads = [sent.words[word.head -\n",
        "                        1].text for sent in doc.sentences for word in sent.words]\n",
        "    for sent in doc.sentences:\n",
        "        for word in sent.words:\n",
        "            # if it is a verbal phrase then take the nominal subject of the phrase\n",
        "            if (word.deprel == 'nsubj' or word.deprel == 'nsubj:pass'):\n",
        "                root = word.text\n",
        "                return word.text\n",
        "                # print(word.text)\n",
        "                break\n",
        "            # print(word)\n",
        "            phrase_upos.append(word)\n",
        "            # else take the root of the phrase\n",
        "            if (word.head == 0):\n",
        "                # print(word.text)\n",
        "                return word.text\n",
        "                # root=word.text\n",
        "                # if the root is a verb\n",
        "                if (word.upos == 'VERB'):\n",
        "                    for w in reversed(phrase_upos):\n",
        "                        # go back until you get a noun\n",
        "                        if (w.upos == 'NN'):\n",
        "                            return word.text\n",
        "                            # print(w.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QKBX0Bc7LrqW"
      },
      "outputs": [],
      "source": [
        "def get_root(yolo_output, sentence, model, yolo):\n",
        "    root = sent_stanza_processing(sentence)\n",
        "    # print(root)\n",
        "    prompt_tokens = clip.tokenize(\n",
        "        root, context_length=77, truncate=True).cuda()\n",
        "    with torch.no_grad():\n",
        "        prompt_features = model.encode_text(prompt_tokens).float()\n",
        "\n",
        "    names = []\n",
        "    for a in range(len(yolo_output.xyxy[0])):\n",
        "        class_index = int(yolo_output.pred[0][a][5])\n",
        "        label = yolo.names[class_index]\n",
        "        names.append(label)\n",
        "    tokens = clip.tokenize(names, context_length=77, truncate=True).cuda()\n",
        "    with torch.no_grad():\n",
        "        classes_features = model.encode_text(tokens).float()\n",
        "    prompt_features /= prompt_features.norm(dim=-1, keepdim=True)\n",
        "    classes_features /= classes_features.norm(dim=-1, keepdim=True)\n",
        "    prompt_similarity = classes_features.cpu().numpy() @ prompt_features.cpu().numpy().T\n",
        "    if prompt_similarity.shape[0] == 0:\n",
        "        return \"empty\"\n",
        "    rappresentation = np.argmax(prompt_similarity)\n",
        "\n",
        "    interested_class = names[rappresentation]\n",
        "    return interested_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "W9Uubvdxz5wG"
      },
      "outputs": [],
      "source": [
        "# given an image pad the image to fit the dimension of clip and fill the padding with the median color of the image\n",
        "\n",
        "def padd_image(img):\n",
        "    avg_color_per_row = np.average(img, axis=0)\n",
        "    avg_color = np.average(avg_color_per_row, axis=0)\n",
        "    old_image_width, old_image_height = img.size\n",
        "    # create new image of desired size and color (blue) for padding\n",
        "    if (old_image_height > 224 or old_image_width > 224):\n",
        "        if (old_image_height > old_image_width):\n",
        "            new_image_width = old_image_height\n",
        "            new_image_height = old_image_height\n",
        "\n",
        "        else:\n",
        "            new_image_width = old_image_width\n",
        "            new_image_height = old_image_width\n",
        "\n",
        "        color = avg_color\n",
        "        # color = (255,0,255)\n",
        "\n",
        "        result = np.full((new_image_height, new_image_width, 3),\n",
        "                         color, dtype=np.uint8)\n",
        "\n",
        "        # compute center offset\n",
        "        x_center = (new_image_width - old_image_width) // 2\n",
        "        y_center = (new_image_height - old_image_height) // 2\n",
        "\n",
        "        # copy img image into center of result image\n",
        "        result[y_center:y_center+old_image_height,\n",
        "               x_center:x_center+old_image_width] = img\n",
        "\n",
        "    else:\n",
        "        new_image_width = 224\n",
        "        new_image_height = 224\n",
        "\n",
        "        color = avg_color\n",
        "        result = np.full((new_image_height, new_image_width, 3),\n",
        "                         color, dtype=np.uint8)\n",
        "        # compute center offset\n",
        "        x_center = (new_image_width - old_image_width) // 2\n",
        "        y_center = (new_image_height - old_image_height) // 2\n",
        "\n",
        "        # copy img image into center of result image\n",
        "        result[y_center:y_center+old_image_height,\n",
        "               x_center:x_center+old_image_width] = img\n",
        "\n",
        "    img = Image.fromarray(result)\n",
        "    img = img.resize((224, 224))\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ExVxvNtacKZS"
      },
      "outputs": [],
      "source": [
        "# remove the id in the image name string\n",
        "def split_string(string):\n",
        "    string = string.split(\"_\")\n",
        "    string = string[:-1]\n",
        "    string = \"_\".join(string)\n",
        "    append = \".jpg\"\n",
        "    string = string + append\n",
        "\n",
        "    return string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "O4rfPXV_zzHz"
      },
      "outputs": [],
      "source": [
        "# convert yolo format bbox into standard type\n",
        "def convert_bbox(bbox, img):\n",
        "    x1, y1, width, height = bbox\n",
        "    x2, y2 = x1 + width, y1 + height\n",
        "\n",
        "    # Verify coordinates\n",
        "    if x1 < 0 or y1 < 0 or x2 > img.width or y2 > img.height:\n",
        "        print(\"Bounding box fuori dai limiti dell'immagine!\")\n",
        "    else:\n",
        "        return x1, y1, x2, y2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dVAC33cB5OhY"
      },
      "outputs": [],
      "source": [
        "# yolo bbox include class and precision, drop them\n",
        "def convert_yolo_bbox(bbox):\n",
        "    return bbox[:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "S5wPfVu-mkeO"
      },
      "outputs": [],
      "source": [
        "# test model on a number N of images and print metrics each tenth of the images\n",
        "def test_on_n_images(model, dataset, N):\n",
        "    model.reset_metrics()\n",
        "\n",
        "    for i in range(0, N):\n",
        "        input = dataset[i]\n",
        "        image_path = split_string(input[\"file_name\"])\n",
        "        sentence = input[\"sentences\"][\"raw\"]\n",
        "        gt = input[\"bbox\"]\n",
        "        original_img = Image.open(local_path + image_path).convert(\"RGB\")\n",
        "        # print img dimensions and box coordinates\n",
        "        bbox, img = model.evaluate(image_path, sentence, gt, original_img)\n",
        "\n",
        "        if i % (N/10) == 0:\n",
        "            print(\"Iteration: \", i)\n",
        "            print(model.get_metrics())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "T9qGI-Oxq9MQ"
      },
      "outputs": [],
      "source": [
        "# Validation function if given a dataframe runs the model parameter for the whole dataframe and prints the metrics\n",
        "def validate(model, dataframe):\n",
        "    model.reset_metrics()\n",
        "    for i in range(0, len(dataframe)):\n",
        "        input = dataframe.iloc[i]\n",
        "        image_path = split_string(input[\"file_name\"])\n",
        "        sentence = input[\"sentences\"][\"raw\"]\n",
        "        gt = input[\"bbox\"]\n",
        "        original_img = Image.open(local_path + image_path).convert(\"RGB\")\n",
        "        # print img dimensions and box coordinates\n",
        "        model.evaluate(image_path, sentence, gt, original_img)\n",
        "\n",
        "    print(model.get_metrics())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLRfZYnqz4zD"
      },
      "source": [
        "### Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-soPgP6nt47D"
      },
      "outputs": [],
      "source": [
        "# definition of recall metric\n",
        "class Recall:\n",
        "    def __init__(self):\n",
        "        self.true_positives = 0\n",
        "        self.false_negatives = 0\n",
        "\n",
        "    def update(self, correct):\n",
        "        if correct:\n",
        "            self.true_positives += 1\n",
        "        else:\n",
        "            self.false_negatives += 1\n",
        "\n",
        "    def compute(self):\n",
        "        return self.true_positives / (self.true_positives + self.false_negatives)\n",
        "\n",
        "    def reset(self):\n",
        "        self.true_positives = 0\n",
        "        self.false_negatives = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "yN9vbC82z8CI"
      },
      "outputs": [],
      "source": [
        "class Metrics:\n",
        "    def __init__(self, model, trasform, treshold):\n",
        "        self.transform = trasform\n",
        "        # initialize torch tensor\n",
        "        self.iou = torch.tensor([]).cuda()\n",
        "        self.recall = Recall()\n",
        "        self.model = model\n",
        "        self.treshold = treshold\n",
        "        self.cosine_similarity = torch.tensor([]).cuda()\n",
        "        self.euclidean_distance = torch.tensor([]).cuda()\n",
        "\n",
        "    def update(self, predicted_bbox, target_bbox, predicted_image, target_image):\n",
        "        predicted_bbox = torch.tensor(predicted_bbox)\n",
        "        target_bbox = torch.tensor(target_bbox)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Preprocess the predicted image and compute the predicted embedding\n",
        "            predicted_image = padd_image(predicted_image)\n",
        "            image_tensor = self.transform(predicted_image)\n",
        "            image_tensor = image_tensor.unsqueeze(\n",
        "                0).cuda()  # Add batch dimension\n",
        "            predicted_embedding = self.model.encode_image(image_tensor)\n",
        "\n",
        "            # Preprocess the target image and compute the target embedding\n",
        "            target_image = padd_image(target_image)\n",
        "            target_image_tensor = self.transform(target_image)\n",
        "            target_image_tensor = target_image_tensor.unsqueeze(\n",
        "                0).cuda()  # Add batch dimension\n",
        "            target_embedding = self.model.encode_image(target_image_tensor)\n",
        "\n",
        "        similarity = torch.nn.functional.cosine_similarity(\n",
        "            predicted_embedding, target_embedding)\n",
        "        distance = torch.nn.functional.pairwise_distance(\n",
        "            predicted_embedding, target_embedding)\n",
        "\n",
        "        # convert bboxes into torch tensors\n",
        "        predicted_bbox = torch.tensor(predicted_bbox)\n",
        "        target_bbox = torch.tensor(target_bbox)\n",
        "        predicted_bbox = convert_yolo_bbox(predicted_bbox)\n",
        "        actual_iou = ops.box_iou(predicted_bbox.unsqueeze(\n",
        "            0).cuda(), target_bbox.unsqueeze(0).cuda())\n",
        "        self.iou = torch.cat((self.iou, actual_iou), 0)\n",
        "        # get iou value of the predicted bbox and the target bbox\n",
        "        if actual_iou > self.treshold:\n",
        "            self.recall.update(True)\n",
        "        else:\n",
        "            self.recall.update(False)\n",
        "        self.cosine_similarity = torch.cat(\n",
        "            (self.cosine_similarity, similarity), 0)\n",
        "        self.euclidean_distance = torch.cat(\n",
        "            (self.euclidean_distance, distance), 0)\n",
        "\n",
        "    def to_string(self):\n",
        "        mean_iou = torch.mean(self.iou)\n",
        "        recall_at_05_iou = self.recall.compute()\n",
        "        mean_cosine_similarity = torch.mean(self.cosine_similarity)\n",
        "        mean_euclidean_distance = torch.mean(self.euclidean_distance)\n",
        "\n",
        "        return f\"Mean IoU: {mean_iou:.4f}, Recall@0.5 IoU: {recall_at_05_iou:.4f}, Mean Cosine Similarity: {mean_cosine_similarity:.4f}, Mean Euclidean Distance: {mean_euclidean_distance:.4f}\"\n",
        "\n",
        "    def reset(self):\n",
        "        self.iou = torch.tensor([]).cuda()\n",
        "        self.recall.reset()\n",
        "        self.cosine_similarity = torch.tensor([]).cuda()\n",
        "        self.euclidean_distance = torch.tensor([]).cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx39cinC0BJb"
      },
      "source": [
        "### Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FjC1bIj90D3I"
      },
      "outputs": [],
      "source": [
        "# dataset class definition\n",
        "class Coco(Dataset):\n",
        "    def __init__(self, path_json, path_pickle, train=True):\n",
        "        self.path_json = path_json\n",
        "        self.path_pickle = path_pickle\n",
        "        self.train = train\n",
        "\n",
        "        # load images and annotations\n",
        "        with open(self.path_json) as json_data:\n",
        "            data = json.load(json_data)\n",
        "            self.ann_frame = pd.DataFrame(data['annotations'])\n",
        "            self.ann_frame = self.ann_frame.reset_index(drop=False)\n",
        "\n",
        "        with open(self.path_pickle, 'rb') as pickle_data:\n",
        "            data = pickle.load(pickle_data)\n",
        "            self.refs_frame = pd.DataFrame(data)\n",
        "\n",
        "        # separate each sentence in dataframe\n",
        "        self.refs_frame = self.refs_frame.explode('sentences')\n",
        "        self.refs_frame = self.refs_frame.reset_index(drop=False)\n",
        "\n",
        "        self.size = self.refs_frame.shape[0]\n",
        "\n",
        "        # merge the dataframes\n",
        "        self.dataset = pd.merge(\n",
        "            self.refs_frame, self.ann_frame, left_on='ann_id', right_on='id')\n",
        "        # drop useless columns for cleaner and smaller dataset\n",
        "        self.dataset = self.dataset.drop(columns=['segmentation', 'id', 'category_id_y', 'ref_id', 'index_x',\n",
        "                                         'iscrowd', 'image_id_y', 'image_id_x', 'category_id_x', 'ann_id', 'sent_ids', 'index_y', 'area'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.dataset.iloc[idx]\n",
        "\n",
        "    def get_annotation(self, idx):\n",
        "        return self.ann_frame.iloc[idx]\n",
        "\n",
        "    def get_imgframe(self, idx):\n",
        "        return self.img_frame.iloc[idx]\n",
        "\n",
        "    def get_validation(self):\n",
        "        return self.dataset[self.dataset['split'] == 'val']\n",
        "\n",
        "    def get_test(self):\n",
        "        return self.dataset[self.dataset['split'] == 'test']\n",
        "\n",
        "    def get_train(self):\n",
        "        return self.dataset[self.dataset['split'] == 'train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peTMO0l70KRz",
        "outputId": "921ab52e-a502-427f-e68c-959b418c8ff1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "95010\n"
          ]
        }
      ],
      "source": [
        "# dataset loading test and dimension check\n",
        "dataset = Coco(local_annotations + 'instances.json',\n",
        "               local_annotations + \"refs(umd).p\")\n",
        "\n",
        "print(len(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "ZDV7Hyel0KmQ",
        "outputId": "7637dd12-f8cf-4096-ca96-b611a782bf67"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       split                                          sentences  \\\n",
              "0       test  {'tokens': ['the', 'man', 'in', 'yellow', 'coa...   \n",
              "1       test  {'tokens': ['skiier', 'in', 'red', 'pants'], '...   \n",
              "2       test  {'tokens': ['there', 'is', 'red', 'colored', '...   \n",
              "3       test  {'tokens': ['a', 'shiny', 'red', 'vintage', 'p...   \n",
              "4       test  {'tokens': ['a', 'apple', 'desktop', 'computer...   \n",
              "...      ...                                                ...   \n",
              "95005  train  {'tokens': ['the', 'larger', 'banana', 'is', '...   \n",
              "95006  train  {'tokens': ['a', 'guy', 'in', 'black', 'jacket...   \n",
              "95007  train  {'tokens': ['the', 'man', 'in', 'the', 'dark',...   \n",
              "95008  train  {'tokens': ['a', 'person', 'in', 'red', 'dress...   \n",
              "95009  train  {'tokens': ['man', 'wearing', 'a', 'red', 'cos...   \n",
              "\n",
              "                                     file_name  \\\n",
              "0       COCO_train2014_000000380440_491042.jpg   \n",
              "1       COCO_train2014_000000380440_491042.jpg   \n",
              "2       COCO_train2014_000000419645_398406.jpg   \n",
              "3       COCO_train2014_000000419645_398406.jpg   \n",
              "4       COCO_train2014_000000478885_124383.jpg   \n",
              "...                                        ...   \n",
              "95005  COCO_train2014_000000003518_1042682.jpg   \n",
              "95006   COCO_train2014_000000302199_473946.jpg   \n",
              "95007   COCO_train2014_000000302199_473946.jpg   \n",
              "95008   COCO_train2014_000000573297_472971.jpg   \n",
              "95009   COCO_train2014_000000573297_472971.jpg   \n",
              "\n",
              "                                  bbox  \n",
              "0      [374.31, 65.06, 136.04, 201.94]  \n",
              "1      [374.31, 65.06, 136.04, 201.94]  \n",
              "2       [93.95, 83.29, 504.61, 290.57]  \n",
              "3       [93.95, 83.29, 504.61, 290.57]  \n",
              "4       [338.8, 82.19, 147.34, 157.37]  \n",
              "...                                ...  \n",
              "95005   [141.7, 20.68, 359.02, 144.57]  \n",
              "95006  [141.34, 86.71, 115.33, 288.75]  \n",
              "95007  [141.34, 86.71, 115.33, 288.75]  \n",
              "95008   [31.71, 67.03, 352.29, 572.97]  \n",
              "95009   [31.71, 67.03, 352.29, 572.97]  \n",
              "\n",
              "[95010 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-05f03c1f-fb5f-4c53-8699-b14a70d91d47\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>split</th>\n",
              "      <th>sentences</th>\n",
              "      <th>file_name</th>\n",
              "      <th>bbox</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>test</td>\n",
              "      <td>{'tokens': ['the', 'man', 'in', 'yellow', 'coa...</td>\n",
              "      <td>COCO_train2014_000000380440_491042.jpg</td>\n",
              "      <td>[374.31, 65.06, 136.04, 201.94]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>test</td>\n",
              "      <td>{'tokens': ['skiier', 'in', 'red', 'pants'], '...</td>\n",
              "      <td>COCO_train2014_000000380440_491042.jpg</td>\n",
              "      <td>[374.31, 65.06, 136.04, 201.94]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>test</td>\n",
              "      <td>{'tokens': ['there', 'is', 'red', 'colored', '...</td>\n",
              "      <td>COCO_train2014_000000419645_398406.jpg</td>\n",
              "      <td>[93.95, 83.29, 504.61, 290.57]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>test</td>\n",
              "      <td>{'tokens': ['a', 'shiny', 'red', 'vintage', 'p...</td>\n",
              "      <td>COCO_train2014_000000419645_398406.jpg</td>\n",
              "      <td>[93.95, 83.29, 504.61, 290.57]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>test</td>\n",
              "      <td>{'tokens': ['a', 'apple', 'desktop', 'computer...</td>\n",
              "      <td>COCO_train2014_000000478885_124383.jpg</td>\n",
              "      <td>[338.8, 82.19, 147.34, 157.37]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95005</th>\n",
              "      <td>train</td>\n",
              "      <td>{'tokens': ['the', 'larger', 'banana', 'is', '...</td>\n",
              "      <td>COCO_train2014_000000003518_1042682.jpg</td>\n",
              "      <td>[141.7, 20.68, 359.02, 144.57]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95006</th>\n",
              "      <td>train</td>\n",
              "      <td>{'tokens': ['a', 'guy', 'in', 'black', 'jacket...</td>\n",
              "      <td>COCO_train2014_000000302199_473946.jpg</td>\n",
              "      <td>[141.34, 86.71, 115.33, 288.75]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95007</th>\n",
              "      <td>train</td>\n",
              "      <td>{'tokens': ['the', 'man', 'in', 'the', 'dark',...</td>\n",
              "      <td>COCO_train2014_000000302199_473946.jpg</td>\n",
              "      <td>[141.34, 86.71, 115.33, 288.75]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95008</th>\n",
              "      <td>train</td>\n",
              "      <td>{'tokens': ['a', 'person', 'in', 'red', 'dress...</td>\n",
              "      <td>COCO_train2014_000000573297_472971.jpg</td>\n",
              "      <td>[31.71, 67.03, 352.29, 572.97]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95009</th>\n",
              "      <td>train</td>\n",
              "      <td>{'tokens': ['man', 'wearing', 'a', 'red', 'cos...</td>\n",
              "      <td>COCO_train2014_000000573297_472971.jpg</td>\n",
              "      <td>[31.71, 67.03, 352.29, 572.97]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>95010 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-05f03c1f-fb5f-4c53-8699-b14a70d91d47')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-05f03c1f-fb5f-4c53-8699-b14a70d91d47 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-05f03c1f-fb5f-4c53-8699-b14a70d91d47');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# to test prints the dataframe that composes the dataset\n",
        "dataset.dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJq1udMa0WW-"
      },
      "source": [
        "### Model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_yolo(yolo_output, img, index):\n",
        "    x1 = yolo_output.xyxy[0][index][0].cpu().numpy()\n",
        "    x1 = np.rint(x1)\n",
        "    y1 = yolo_output.xyxy[0][index][1].cpu().numpy()\n",
        "    y1 = np.rint(y1)\n",
        "    x2 = yolo_output.xyxy[0][index][2].cpu().numpy()\n",
        "    x2 = np.rint(x2)\n",
        "    y2 = yolo_output.xyxy[0][index][3].cpu().numpy()\n",
        "    y2 = np.rint(y2)\n",
        "\n",
        "    cropped_img = img.crop((x1, y1, x2, y2))\n",
        "\n",
        "    return cropped_img"
      ],
      "metadata": {
        "id": "RSsolp-pi2BH"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model_blip = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(\"cuda\")"
      ],
      "metadata": {
        "id": "idP049FAha0U"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "DkfKW4cL0ToQ"
      },
      "outputs": [],
      "source": [
        "# class that defines the baseline model\n",
        "\n",
        "class VisualGrounding(torch.nn.Module):\n",
        "    def __init__(self, yolo_version, clip_version, local_path, img_path):\n",
        "        super(VisualGrounding, self).__init__()\n",
        "        self.local_path = local_path\n",
        "        self.img_path = img_path\n",
        "        # initialize models\n",
        "        self.yolo = torch.hub.load(\n",
        "            'ultralytics/yolov5', yolo_version, pretrained=True)\n",
        "        self.clip, self.preprocess = clip.load(clip_version)\n",
        "\n",
        "        # define metrics\n",
        "        treshold = 0.5\n",
        "        transform = torchvision.transforms.Compose([\n",
        "            torchvision.transforms.ToTensor(),\n",
        "            torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                             std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        self.metrics = Metrics(self.clip, transform, treshold)\n",
        "\n",
        "    def forward(self, img_path, sentence):\n",
        "        similarity = torch.tensor([]).to(\"cuda\")\n",
        "\n",
        "        yolo_output = self.yolo(self.local_path+img_path)\n",
        "        original_img = Image.open(self.local_path+img_path).convert(\"RGB\")\n",
        "\n",
        "        root = get_root(yolo_output, sentence, self.clip, self.yolo)\n",
        "\n",
        "        sentence_tokens = clip.tokenize(sentence).to(\"cuda\")\n",
        "        embedding_sent = self.clip.encode_text(sentence_tokens).to(\"cuda\")\n",
        "\n",
        "        no_bbox=True\n",
        "\n",
        "        for i in range(len(yolo_output.xyxy[0])):\n",
        "            if root != \"empty\" and self.yolo.names[int(yolo_output.pred[0][i][5])] != root:\n",
        "                continue\n",
        "\n",
        "            with torch.no_grad():\n",
        "                no_bbox=False\n",
        "                #crop the image based on the yolo output\n",
        "                img_cropped = crop_yolo(yolo_output, original_img, i)\n",
        "                #generate caption\n",
        "                inputs = processor(images=img_cropped, return_tensors=\"pt\").to(\"cuda\")\n",
        "                pixel_values = inputs.pixel_values\n",
        "                generated_ids = model_blip.generate(pixel_values=pixel_values, max_length=50)\n",
        "                generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "                caption = clip.tokenize(generated_caption).to(\"cuda\")\n",
        "                enbedding_gen = self.clip.encode_text(caption).to(\"cuda\")\n",
        "\n",
        "                #cosine similarity bwteen caption and sentence\n",
        "                similarity = torch.cat((similarity, torch.nn.functional.cosine_similarity(enbedding_gen, embedding_sent)), 0)\n",
        "            \n",
        "        if no_bbox or (similarity.numel()==0):\n",
        "            #set bbox to the whole image\n",
        "            max_bbox = [0, 0, original_img.width, original_img.height]\n",
        "            max_image = original_img\n",
        "\n",
        "        #argmax to get the most similar caption\n",
        "        index = torch.argmax(similarity)\n",
        "        max_bbox = yolo_output.xyxy[0][index]\n",
        "        max_image = crop_yolo(yolo_output, original_img, index)\n",
        "\n",
        "        return max_bbox, max_image\n",
        "\n",
        "    def evaluate(self, img_path, sentence, gt, original_img):\n",
        "        bbox = convert_bbox(gt, original_img)\n",
        "        gt_crop = original_img.crop(bbox)\n",
        "        prediction_bbox, prediction_img = self.forward(img_path, sentence)\n",
        "        self.metrics.update(prediction_bbox, bbox, prediction_img, gt_crop)\n",
        "        return prediction_bbox, prediction_img\n",
        "\n",
        "    def reset_metrics(self):\n",
        "        self.metrics.reset()\n",
        "\n",
        "    def get_metrics(self):\n",
        "        return self.metrics.to_string()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHcFXg1R0cgv"
      },
      "source": [
        "### Model evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Csy-JLatEhkf"
      },
      "outputs": [],
      "source": [
        "# dataset load\n",
        "dataset = Coco(local_annotations + 'instances.json',\n",
        "               local_annotations + \"refs(umd).p\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXTbixSEqcQe",
        "outputId": "06b26f5e-1cbd-401e-d909-12b1f18fa734"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 🚀 2023-6-6 Python-3.10.11 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5x summary: 444 layers, 86705005 parameters, 0 gradients\n",
            "Adding AutoShape... \n"
          ]
        }
      ],
      "source": [
        "# model load\n",
        "model = VisualGrounding('yolov5x', 'ViT-B/32', local_path, local_annotations)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(similarity)"
      ],
      "metadata": {
        "id": "-MpkCLJQ-rjq",
        "outputId": "6ea2648d-38c2-4ed4-b0d7-bfeb6066fdb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-3b3b22b075fb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'similarity' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "PvdyH-aLLrqb",
        "outputId": "abe36416-c9f8-48a3-bb0c-9787a1006976",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-9d7e5d55e6e9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-e627467c5d31>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(model, dataframe)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moriginal_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# print img dimensions and box coordinates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-52ae38ca41b8>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, img_path, sentence, gt, original_img)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_bbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mgt_crop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mprediction_bbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_bbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_crop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprediction_bbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_img\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-52ae38ca41b8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img_path, sentence)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m#argmax to get the most similar caption\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mmax_bbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myolo_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxyxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mmax_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrop_yolo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myolo_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: argmax(): Expected reduction dim to be specified for input.numel() == 0."
          ]
        }
      ],
      "source": [
        "validate(model, dataset.get_test())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9ca574561552418e9e6a71a9cfecbb35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8edfbe8be8634e76826647da0396ad55",
              "IPY_MODEL_8c31faf9a0044bfc848eb393cef7df91",
              "IPY_MODEL_7aa8ca9d2b6c40b39effdc4ed8a51705"
            ],
            "layout": "IPY_MODEL_4064543a40ae466cb9701f15e51e7736"
          }
        },
        "8edfbe8be8634e76826647da0396ad55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d84ccafd58a7434db9f6e11084c53613",
            "placeholder": "​",
            "style": "IPY_MODEL_c801569689364c44b5ff10736605e9b4",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json: "
          }
        },
        "8c31faf9a0044bfc848eb393cef7df91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04206e3fb54142ddbb6cd4b5170293ee",
            "max": 30101,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_88b42abf98b0482e9b273cf15d6e3e3e",
            "value": 30101
          }
        },
        "7aa8ca9d2b6c40b39effdc4ed8a51705": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_000f9f83cb6b405080dd166ce8e81a3f",
            "placeholder": "​",
            "style": "IPY_MODEL_509db7be00f8438c999b78ce466b1f96",
            "value": " 216k/? [00:00&lt;00:00, 4.97MB/s]"
          }
        },
        "4064543a40ae466cb9701f15e51e7736": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d84ccafd58a7434db9f6e11084c53613": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c801569689364c44b5ff10736605e9b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04206e3fb54142ddbb6cd4b5170293ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88b42abf98b0482e9b273cf15d6e3e3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "000f9f83cb6b405080dd166ce8e81a3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "509db7be00f8438c999b78ce466b1f96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57893c05392d4cf3996ddf9ad04dbf69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bedb0d71d3fd4567a49e61dbc1f462ef",
              "IPY_MODEL_8adc279d11ba48949a640793ed8c11be",
              "IPY_MODEL_8072426ded574864bfea53fc90cd5ed6"
            ],
            "layout": "IPY_MODEL_a5f7d673fba84c698905d27a6ce44d17"
          }
        },
        "bedb0d71d3fd4567a49e61dbc1f462ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_231157d83caf41af975fb62278be3126",
            "placeholder": "​",
            "style": "IPY_MODEL_4ed4d73f9383471ea1d66408009731dd",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json: "
          }
        },
        "8adc279d11ba48949a640793ed8c11be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76d3d8d5131541f4852147f327b672e8",
            "max": 30101,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3b56d314e29846a89c94d4ebc744567c",
            "value": 30101
          }
        },
        "8072426ded574864bfea53fc90cd5ed6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdd15256a1624905a22de9d1ff08e977",
            "placeholder": "​",
            "style": "IPY_MODEL_a4b30c0debc2408ab8725c3425760b03",
            "value": " 216k/? [00:00&lt;00:00, 9.69MB/s]"
          }
        },
        "a5f7d673fba84c698905d27a6ce44d17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "231157d83caf41af975fb62278be3126": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ed4d73f9383471ea1d66408009731dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76d3d8d5131541f4852147f327b672e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b56d314e29846a89c94d4ebc744567c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bdd15256a1624905a22de9d1ff08e977": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4b30c0debc2408ab8725c3425760b03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}