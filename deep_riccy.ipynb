{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOvgNpY7LwSU"
      },
      "source": [
        "# Deep riccy project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CzySrjXzHhN"
      },
      "source": [
        "## Env creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiKX4oZgzKLR",
        "outputId": "c6656df7-b7d4-4ba3-b69e-1dc8b4ecb371"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "! tar -zxf /content/drive/MyDrive/Uni/DeepRiccy/refcocog.tar.gz\n",
        "! pip3 install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt  --quiet\n",
        "! pip install ftfy regex tqdm --quiet\n",
        "! pip install git+https://github.com/openai/CLIP.git --quiet\n",
        "! pip install rouge-metric --quiet\n",
        "! pip install torchmetrics --quiet\n",
        "! pip install torchvision --quiet\n",
        "\n",
        "local_path = '/content/refcocog/images/'\n",
        "local_annotations = '/content/refcocog/annotations/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "R4ZuKVoVzdFH"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "import torch\n",
        "from PIL import Image, ImageDraw\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from pkg_resources import packaging\n",
        "import clip\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import skimage\n",
        "import IPython.display\n",
        "\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "import torchmetrics as tm\n",
        "import torchvision\n",
        "from torchvision import ops "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKNYTNptzWrx"
      },
      "source": [
        "## Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "418bIOjizjPc"
      },
      "source": [
        "### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def padd_image(img):\n",
        "  avg_color_per_row = np.average(img, axis=0)\n",
        "  avg_color = np.average(avg_color_per_row, axis=0)\n",
        "  old_image_width,old_image_height  = img.size\n",
        "  # create new image of desired size and color (blue) for padding\n",
        "  if(old_image_height>224 or old_image_width>224):\n",
        "    if(old_image_height>old_image_width):\n",
        "          new_image_width = old_image_height\n",
        "          new_image_height = old_image_height\n",
        "\n",
        "\n",
        "    else:\n",
        "          new_image_width = old_image_width\n",
        "          new_image_height = old_image_width\n",
        "\n",
        "    \n",
        "    color=avg_color\n",
        "    #color = (255,0,255)\n",
        "\n",
        "    result = np.full((new_image_height,new_image_width, 3), color, dtype=np.uint8)\n",
        "\n",
        "    # compute center offset\n",
        "    x_center = (new_image_width - old_image_width) // 2\n",
        "    y_center = (new_image_height - old_image_height) // 2\n",
        "\n",
        "    # copy img image into center of result image\n",
        "    result[y_center:y_center+old_image_height, \n",
        "          x_center:x_center+old_image_width] = img   \n",
        "\n",
        "        \n",
        "  else:\n",
        "    new_image_width = 224\n",
        "    new_image_height = 224\n",
        "    '''if(old_image_height>old_image_width):\n",
        "          new_image_width = old_image_height\n",
        "          new_image_height = old_image_height\n",
        "\n",
        "\n",
        "    else:\n",
        "          new_image_width = old_image_width\n",
        "          new_image_height = old_image_width'''\n",
        " \n",
        "    #color = (255,0,255)\n",
        "    color=avg_color\n",
        "    result = np.full((new_image_height,new_image_width, 3), color, dtype=np.uint8)\n",
        "\n",
        "    # compute center offset\n",
        "    x_center = (new_image_width - old_image_width) // 2\n",
        "    y_center = (new_image_height - old_image_height) // 2\n",
        "\n",
        "    # copy img image into center of result image\n",
        "    result[y_center:y_center+old_image_height, \n",
        "          x_center:x_center+old_image_width] = img\n",
        "\n",
        "\n",
        "  img= Image.fromarray(result)\n",
        "  img=img.resize((224,224))\n",
        "  return img"
      ],
      "metadata": {
        "id": "W9Uubvdxz5wG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ExVxvNtacKZS"
      },
      "outputs": [],
      "source": [
        "def split_string(string):\n",
        "    string = string.split(\"_\")\n",
        "    string = string[:-1]\n",
        "    string = \"_\".join(string)\n",
        "    append = \".jpg\"\n",
        "    string = string + append\n",
        "    \n",
        "    return string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "O4rfPXV_zzHz"
      },
      "outputs": [],
      "source": [
        "def convert_bbox(bbox, img):\n",
        "  x1, y1, width, height = bbox\n",
        "  x2, y2 = x1 + width, y1 + height\n",
        "\n",
        "  # Verify coordinates\n",
        "  if x1 < 0 or y1 < 0 or x2 > img.width or y2 > img.height:\n",
        "      print(\"Bounding box fuori dai limiti dell'immagine!\")\n",
        "  else:\n",
        "    return x1, y1, x2, y2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_bbox_no_img(bbox):\n",
        "  x1, y1, width, height = bbox\n",
        "  x2, y2 = x1 + width, y1 + height\n",
        "\n",
        "  return x1, y1, x2, y2"
      ],
      "metadata": {
        "id": "vNFzkqWd2KQx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_yolo_bbox(bbox):\n",
        "  return bbox[:4]"
      ],
      "metadata": {
        "id": "dVAC33cB5OhY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLRfZYnqz4zD"
      },
      "source": [
        "### Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-soPgP6nt47D"
      },
      "outputs": [],
      "source": [
        "class Recall:\n",
        "    def __init__(self):\n",
        "        self.true_positives = 0\n",
        "        self.false_negatives = 0\n",
        "\n",
        "    def update(self, correct):\n",
        "        if correct:\n",
        "            self.true_positives += 1\n",
        "        else:\n",
        "            self.false_negatives += 1\n",
        "    \n",
        "    def compute(self):\n",
        "        return self.true_positives / (self.true_positives + self.false_negatives)\n",
        "    \n",
        "    def reset(self):\n",
        "        self.true_positives = 0\n",
        "        self.false_negatives = 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W3Zx_PTlLHyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "yN9vbC82z8CI"
      },
      "outputs": [],
      "source": [
        "class Metrics:\n",
        "    def __init__(self, model, trasform, treshold):\n",
        "        self.transform = trasform\n",
        "        #initialize torch tensor\n",
        "        self.iou = torch.tensor([]).cuda()\n",
        "        self.recall = Recall()\n",
        "        self.model = model\n",
        "        self.treshold = treshold\n",
        "        self.cosine_similarity = torch.tensor([]).cuda()\n",
        "        self.euclidean_distance = torch.tensor([]).cuda()\n",
        "\n",
        "    def update(self, predicted_bbox, target_bbox, predicted_image, target_image):\n",
        "        predicted_bbox = torch.tensor(predicted_bbox)\n",
        "        target_bbox = torch.tensor(target_bbox)\n",
        "        target_bbox = convert_bbox_no_img(target_bbox)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Preprocess the predicted image and compute the predicted embedding\n",
        "            predicted_image = padd_image(predicted_image)\n",
        "            image_tensor = self.transform(predicted_image)\n",
        "            image_tensor = image_tensor.unsqueeze(0).cuda()  # Add batch dimension\n",
        "            predicted_embedding = self.model.encode_image(image_tensor)\n",
        "\n",
        "            # Preprocess the target image and compute the target embedding\n",
        "            target_image = padd_image(target_image)\n",
        "            target_image_tensor = self.transform(target_image)\n",
        "            target_image_tensor = target_image_tensor.unsqueeze(0).cuda()  # Add batch dimension\n",
        "            target_embedding = self.model.encode_image(target_image_tensor)\n",
        "\n",
        "        similarity = torch.nn.functional.cosine_similarity(predicted_embedding, target_embedding)\n",
        "        distance = torch.nn.functional.pairwise_distance(predicted_embedding, target_embedding)\n",
        "        \n",
        "        #convert bboxes into torch tensors\n",
        "        predicted_bbox = torch.tensor(predicted_bbox)\n",
        "        target_bbox = torch.tensor(target_bbox)\n",
        "        predicted_bbox = convert_yolo_bbox(predicted_bbox)\n",
        "        actual_iou = ops.box_iou(predicted_bbox.unsqueeze(0).cuda(), target_bbox.unsqueeze(0).cuda())\n",
        "        self.iou = torch.cat((self.iou, actual_iou), 0)\n",
        "\n",
        "        #get iou value of the predicted bbox and the target bbox\n",
        "        if actual_iou > self.treshold:\n",
        "          self.recall.update(False)\n",
        "        else:\n",
        "          self.recall.update(True)\n",
        "        self.cosine_similarity = torch.cat((self.cosine_similarity, similarity), 0)\n",
        "        self.euclidean_distance = torch.cat((self.euclidean_distance, distance), 0)\n",
        "\n",
        "    def to_string(self):\n",
        "        mean_iou = torch.mean(self.iou)\n",
        "        recall_at_05_iou = self.recall.compute()\n",
        "        mean_cosine_similarity = torch.mean(self.cosine_similarity)\n",
        "        mean_euclidean_distance = torch.mean(self.euclidean_distance)\n",
        "\n",
        "        return f\"Mean IoU: {mean_iou:.4f}, Recall@0.5 IoU: {recall_at_05_iou:.4f}, Mean Cosine Similarity: {mean_cosine_similarity:.4f}, Mean Euclidean Distance: {mean_euclidean_distance:.4f}\"\n",
        "\n",
        "    def reset(self):\n",
        "        self.iou = torch.tensor([]).cuda()\n",
        "        self.recall.reset()\n",
        "        self.cosine_similarity = torch.tensor([]).cuda()\n",
        "        self.euclidean_distance= torch.tensor([]).cuda()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx39cinC0BJb"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FjC1bIj90D3I"
      },
      "outputs": [],
      "source": [
        "class Coco(Dataset):\n",
        "    def __init__(self, path_json, path_pickle, train=True):\n",
        "        self.path_json = path_json\n",
        "        self.path_pickle = path_pickle\n",
        "        self.train = train\n",
        "\n",
        "        #load images and annotations\n",
        "        with open(self.path_json) as json_data:\n",
        "            data = json.load(json_data)\n",
        "            self.ann_frame = pd.DataFrame(data['annotations'])\n",
        "            self.ann_frame = self.ann_frame.reset_index(drop=False)\n",
        "\n",
        "\n",
        "        #load annotations\n",
        "        with open(self.path_pickle, 'rb') as pickle_data:\n",
        "            data = pickle.load(pickle_data)\n",
        "            self.refs_frame = pd.DataFrame(data)\n",
        "\n",
        "        #separate each sentence in dataframe\n",
        "        self.refs_frame = self.refs_frame.explode('sentences')\n",
        "\n",
        "        self.refs_frame = self.refs_frame.reset_index(drop=False)\n",
        "\n",
        "        self.size = self.refs_frame.shape[0]\n",
        "\n",
        "        self.dataset = pd.merge(self.refs_frame, self.ann_frame, left_on='ann_id', right_on='id')\n",
        "        self.dataset = self.dataset.drop(columns=['segmentation', 'id', 'category_id_y','ref_id', 'index_x', 'iscrowd', 'image_id_y', 'image_id_x', 'category_id_x', 'ann_id', 'sent_ids', 'index_y', 'area'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.dataset.iloc[idx]\n",
        "\n",
        "    def get_annotation(self, idx):\n",
        "      return self.ann_frame.iloc[idx]\n",
        "    \n",
        "    def get_imgframe(self, idx):\n",
        "      return self.img_frame.iloc[idx]\n",
        "\n",
        "    def get_validation(self):\n",
        "        #return the dataframe that has as attribute the validation\n",
        "        return self.dataset[self.dataset['split'] == 'val']\n",
        "    \n",
        "    def get_test(self):\n",
        "        #return the dataframe that has as attribute the test\n",
        "        return self.dataset[self.dataset['split'] == 'test']\n",
        "    \n",
        "    def get_train(self):\n",
        "        #return the dataframe that has as attribute the train\n",
        "        return self.dataset[self.dataset['split'] == 'train']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peTMO0l70KRz",
        "outputId": "3caf6b73-1c40-4f28-9538-192f0c8446ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "95010\n"
          ]
        }
      ],
      "source": [
        "#test dataset\n",
        "\n",
        "dataset = Coco(local_annotations + 'instances.json', local_annotations + \"refs(umd).p\")\n",
        "\n",
        "print(len(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ZDV7Hyel0KmQ",
        "outputId": "8d8567a0-2c4b-4450-a739-32055238a902"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       split                                          sentences  \\\n",
              "0       test  {'tokens': ['the', 'man', 'in', 'yellow', 'coa...   \n",
              "1       test  {'tokens': ['skiier', 'in', 'red', 'pants'], '...   \n",
              "2       test  {'tokens': ['there', 'is', 'red', 'colored', '...   \n",
              "3       test  {'tokens': ['a', 'shiny', 'red', 'vintage', 'p...   \n",
              "4       test  {'tokens': ['a', 'apple', 'desktop', 'computer...   \n",
              "...      ...                                                ...   \n",
              "95005  train  {'tokens': ['the', 'larger', 'banana', 'is', '...   \n",
              "95006  train  {'tokens': ['a', 'guy', 'in', 'black', 'jacket...   \n",
              "95007  train  {'tokens': ['the', 'man', 'in', 'the', 'dark',...   \n",
              "95008  train  {'tokens': ['a', 'person', 'in', 'red', 'dress...   \n",
              "95009  train  {'tokens': ['man', 'wearing', 'a', 'red', 'cos...   \n",
              "\n",
              "                                     file_name  \\\n",
              "0       COCO_train2014_000000380440_491042.jpg   \n",
              "1       COCO_train2014_000000380440_491042.jpg   \n",
              "2       COCO_train2014_000000419645_398406.jpg   \n",
              "3       COCO_train2014_000000419645_398406.jpg   \n",
              "4       COCO_train2014_000000478885_124383.jpg   \n",
              "...                                        ...   \n",
              "95005  COCO_train2014_000000003518_1042682.jpg   \n",
              "95006   COCO_train2014_000000302199_473946.jpg   \n",
              "95007   COCO_train2014_000000302199_473946.jpg   \n",
              "95008   COCO_train2014_000000573297_472971.jpg   \n",
              "95009   COCO_train2014_000000573297_472971.jpg   \n",
              "\n",
              "                                  bbox  \n",
              "0      [374.31, 65.06, 136.04, 201.94]  \n",
              "1      [374.31, 65.06, 136.04, 201.94]  \n",
              "2       [93.95, 83.29, 504.61, 290.57]  \n",
              "3       [93.95, 83.29, 504.61, 290.57]  \n",
              "4       [338.8, 82.19, 147.34, 157.37]  \n",
              "...                                ...  \n",
              "95005   [141.7, 20.68, 359.02, 144.57]  \n",
              "95006  [141.34, 86.71, 115.33, 288.75]  \n",
              "95007  [141.34, 86.71, 115.33, 288.75]  \n",
              "95008   [31.71, 67.03, 352.29, 572.97]  \n",
              "95009   [31.71, 67.03, 352.29, 572.97]  \n",
              "\n",
              "[95010 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-528dc1f0-5244-4323-9144-92e4e49cdc8b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>split</th>\n",
              "      <th>sentences</th>\n",
              "      <th>file_name</th>\n",
              "      <th>bbox</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>test</td>\n",
              "      <td>{'tokens': ['the', 'man', 'in', 'yellow', 'coa...</td>\n",
              "      <td>COCO_train2014_000000380440_491042.jpg</td>\n",
              "      <td>[374.31, 65.06, 136.04, 201.94]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>test</td>\n",
              "      <td>{'tokens': ['skiier', 'in', 'red', 'pants'], '...</td>\n",
              "      <td>COCO_train2014_000000380440_491042.jpg</td>\n",
              "      <td>[374.31, 65.06, 136.04, 201.94]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>test</td>\n",
              "      <td>{'tokens': ['there', 'is', 'red', 'colored', '...</td>\n",
              "      <td>COCO_train2014_000000419645_398406.jpg</td>\n",
              "      <td>[93.95, 83.29, 504.61, 290.57]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>test</td>\n",
              "      <td>{'tokens': ['a', 'shiny', 'red', 'vintage', 'p...</td>\n",
              "      <td>COCO_train2014_000000419645_398406.jpg</td>\n",
              "      <td>[93.95, 83.29, 504.61, 290.57]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>test</td>\n",
              "      <td>{'tokens': ['a', 'apple', 'desktop', 'computer...</td>\n",
              "      <td>COCO_train2014_000000478885_124383.jpg</td>\n",
              "      <td>[338.8, 82.19, 147.34, 157.37]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95005</th>\n",
              "      <td>train</td>\n",
              "      <td>{'tokens': ['the', 'larger', 'banana', 'is', '...</td>\n",
              "      <td>COCO_train2014_000000003518_1042682.jpg</td>\n",
              "      <td>[141.7, 20.68, 359.02, 144.57]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95006</th>\n",
              "      <td>train</td>\n",
              "      <td>{'tokens': ['a', 'guy', 'in', 'black', 'jacket...</td>\n",
              "      <td>COCO_train2014_000000302199_473946.jpg</td>\n",
              "      <td>[141.34, 86.71, 115.33, 288.75]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95007</th>\n",
              "      <td>train</td>\n",
              "      <td>{'tokens': ['the', 'man', 'in', 'the', 'dark',...</td>\n",
              "      <td>COCO_train2014_000000302199_473946.jpg</td>\n",
              "      <td>[141.34, 86.71, 115.33, 288.75]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95008</th>\n",
              "      <td>train</td>\n",
              "      <td>{'tokens': ['a', 'person', 'in', 'red', 'dress...</td>\n",
              "      <td>COCO_train2014_000000573297_472971.jpg</td>\n",
              "      <td>[31.71, 67.03, 352.29, 572.97]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95009</th>\n",
              "      <td>train</td>\n",
              "      <td>{'tokens': ['man', 'wearing', 'a', 'red', 'cos...</td>\n",
              "      <td>COCO_train2014_000000573297_472971.jpg</td>\n",
              "      <td>[31.71, 67.03, 352.29, 572.97]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>95010 rows Ã— 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-528dc1f0-5244-4323-9144-92e4e49cdc8b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-528dc1f0-5244-4323-9144-92e4e49cdc8b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-528dc1f0-5244-4323-9144-92e4e49cdc8b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "dataset.dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJq1udMa0WW-"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "DkfKW4cL0ToQ"
      },
      "outputs": [],
      "source": [
        "class VisualGrounding(torch.nn.Module):\n",
        "    def __init__(self, yolo_version, clip_version, local_path, img_path):\n",
        "        super(VisualGrounding, self).__init__()\n",
        "        self.local_path = local_path\n",
        "        self.img_path = img_path\n",
        "        #initialize models\n",
        "        self.yolo = torch.hub.load('ultralytics/yolov5', yolo_version , pretrained=True)\n",
        "        self.clip, self.preprocess = clip.load(clip_version)\n",
        "        \n",
        "        #define metrics\n",
        "        treshold = 0.5\n",
        "        transform = torchvision.transforms.Compose([\n",
        "          torchvision.transforms.ToTensor(),\n",
        "          torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                          std=[0.229, 0.224, 0.225])\n",
        "          ])\n",
        "        self.metrics = Metrics(self.clip, transform, treshold)\n",
        "        \n",
        "    \n",
        "    def forward(self, img_path, sentence):\n",
        "      max_similarity = 0\n",
        "      max_image = None\n",
        "      max_bbox = None\n",
        "\n",
        "      yolo_output = self.yolo(self.local_path+img_path)\n",
        "\n",
        "      original_img = Image.open(self.local_path+img_path).convert(\"RGB\")\n",
        "\n",
        "      for i in range(len(yolo_output.xyxy[0])):\n",
        "          x1 = yolo_output.xyxy[0][i][0].cpu().numpy()\n",
        "          x1 = np.rint(x1)\n",
        "          y1 = yolo_output.xyxy[0][i][1].cpu().numpy()\n",
        "          y1 = np.rint(y1)\n",
        "          x2 = yolo_output.xyxy[0][i][2].cpu().numpy()\n",
        "          x2 = np.rint(x2)\n",
        "          y2 = yolo_output.xyxy[0][i][3].cpu().numpy()\n",
        "          y2 = np.rint(y2)\n",
        "\n",
        "          img_cropped = original_img.crop((x1, y1, x2, y2))\n",
        "\n",
        "          img = self.preprocess(img_cropped).cuda().unsqueeze(0)\n",
        "          text = clip.tokenize([sentence]).cuda()\n",
        "\n",
        "          with torch.no_grad():\n",
        "              image_features = self.clip.encode_image(img).float()\n",
        "              text_features = self.clip.encode_text(text).float()\n",
        "\n",
        "          image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "          text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "          similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
        "\n",
        "          if similarity > max_similarity:\n",
        "              max_similarity = similarity\n",
        "              max_image = img_cropped\n",
        "              max_bbox = yolo_output.xyxy[0][i]\n",
        "\n",
        "          return max_bbox, max_image\n",
        "\n",
        "        \n",
        "    def evaluate(self, img_path, sentence, gt, original_img):\n",
        "      bbox = convert_bbox(gt, original_img)\n",
        "      gt_crop = original_img.crop(bbox)\n",
        "      prediction_bbox, prediction_img = self.forward(img_path, sentence)\n",
        "      self.metrics.update(prediction_bbox, bbox, prediction_img, gt_crop)\n",
        "      return prediction_bbox, prediction_img\n",
        "    \n",
        "    def reset_metrics(self):\n",
        "      self.metrics.reset()\n",
        "\n",
        "    def get_metrics(self):\n",
        "      return self.metrics.to_string()\n",
        "          \n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHcFXg1R0cgv"
      },
      "source": [
        "### Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2e_3XlHHEozB"
      },
      "outputs": [],
      "source": [
        "dataset = Coco(local_annotations + 'instances.json', local_annotations + \"refs(umd).p\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Csy-JLatEhkf",
        "outputId": "2dc50ad8-a3d7-42fc-b5cf-cc7424f0b63b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 ðŸš€ 2023-5-9 Python-3.10.11 torch-2.0.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31m\u001b[1mrequirements:\u001b[0m /root/.cache/torch/hub/requirements.txt not found, check failed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5x summary: 444 layers, 86705005 parameters, 0 gradients\n",
            "Adding AutoShape... \n"
          ]
        }
      ],
      "source": [
        "model = VisualGrounding('yolov5x','ViT-B/32', local_path, local_annotations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJXjlmnb0a80",
        "outputId": "35d35803-808e-4c00-dac3-7952c76cd1e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:  0\n",
            "Mean IoU: 0.0000, Recall@0.5 IoU: 1.0000, Mean Cosine Similarity: 0.8789, Mean Euclidean Distance: 5.0664\n",
            "Iteration:  100\n",
            "Mean IoU: 0.1636, Recall@0.5 IoU: 0.8812, Mean Cosine Similarity: 0.8378, Mean Euclidean Distance: 5.6156\n",
            "Iteration:  200\n",
            "Mean IoU: 0.1388, Recall@0.5 IoU: 0.9005, Mean Cosine Similarity: 0.8160, Mean Euclidean Distance: 6.0979\n",
            "Iteration:  300\n",
            "Mean IoU: 0.1567, Recall@0.5 IoU: 0.9070, Mean Cosine Similarity: 0.8210, Mean Euclidean Distance: 5.9462\n",
            "Iteration:  400\n",
            "Mean IoU: 0.1545, Recall@0.5 IoU: 0.9202, Mean Cosine Similarity: 0.8176, Mean Euclidean Distance: 5.9997\n",
            "Iteration:  500\n",
            "Mean IoU: 0.1716, Recall@0.5 IoU: 0.9062, Mean Cosine Similarity: 0.8211, Mean Euclidean Distance: 5.8832\n",
            "Iteration:  600\n",
            "Mean IoU: 0.1806, Recall@0.5 IoU: 0.8985, Mean Cosine Similarity: 0.8260, Mean Euclidean Distance: 5.7603\n",
            "Iteration:  700\n",
            "Mean IoU: 0.1783, Recall@0.5 IoU: 0.8959, Mean Cosine Similarity: 0.8245, Mean Euclidean Distance: 5.7852\n",
            "Iteration:  800\n",
            "Mean IoU: 0.1784, Recall@0.5 IoU: 0.8964, Mean Cosine Similarity: 0.8235, Mean Euclidean Distance: 5.8134\n",
            "Iteration:  900\n",
            "Mean IoU: 0.1781, Recall@0.5 IoU: 0.8968, Mean Cosine Similarity: 0.8228, Mean Euclidean Distance: 5.8300\n"
          ]
        }
      ],
      "source": [
        "#evaluation of the model\n",
        "model.reset_metrics()\n",
        "\n",
        "for i in range(0, 1000):\n",
        "    input = dataset[i]\n",
        "    image_path = split_string(input[\"file_name\"])\n",
        "    sentence = input[\"sentences\"][\"raw\"]\n",
        "    gt = input[\"bbox\"]\n",
        "    original_img = Image.open(local_path + image_path).convert(\"RGB\")\n",
        "    # print img dimensions and box coordinates\n",
        "    bbox, img = model.evaluate(image_path, sentence, gt, original_img)\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print(\"Iteration: \", i)\n",
        "        print(model.get_metrics())\n",
        "        "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}