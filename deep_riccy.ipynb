{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOvgNpY7LwSU"
      },
      "source": [
        "## Deep riccy project"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Colab dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vd2FccpXL6WE",
        "outputId": "9fac43a8-a985-441f-c5fc-88e8efb1385c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyIe9EVpLwSW"
      },
      "outputs": [],
      "source": [
        "! tar -zxvf /content/drive/MyDrive/Uni/DeepRiccy/refcocog.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "#set local path to the data\n",
        "\n",
        "#this is for colab\n",
        "#local_path = '/content/refcocog/images/'\n",
        "#local_annotations = '/content/refcocog/annotations/'\n",
        "\n",
        "#this is for local\n",
        "local_path = './refcocog/images/' \n",
        "local_annotations = './refcocog/annotations/' "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "#import section\n",
        "import json\n",
        "import pickle\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "#class creation for the items\n",
        "class CocogImage():\n",
        "    def __init__(self, id, json_file, referece):\n",
        "        self.id = id\n",
        "        self.data = json_file['images'][id]\n",
        "        self.bboxes = []\n",
        "        self.img_id = self.data['id']\n",
        "\n",
        "        for i in json_file['annotations']:\n",
        "            if i['image_id'] == self.img_id:\n",
        "                self.bboxes.append(i)\n",
        "                \n",
        "        self.annotations = []\n",
        "        \n",
        "        for i in referece:\n",
        "            if i['image_id'] == self.id:\n",
        "                self.annotations.append(i)\n",
        "    \n",
        "    def show(self):\n",
        "        img = Image.open(local_path + self.data['file_name'])\n",
        "        plt.imshow(img)\n",
        "        plt.show()\n",
        "\n",
        "    def get_image(self, transform = transforms.Compose([transforms.PILToTensor()])):\n",
        "        img = Image.open(local_path + self.data['file_name'])\n",
        "        return transform(img)\n",
        "    \n",
        "    def file_name(self):\n",
        "        return local_path+self.data['file_name']\n",
        "\n",
        "    def id(self):\n",
        "        return self.data['id']\n",
        "\n",
        "    def width(self):\n",
        "        return self.data['width']\n",
        "\n",
        "    def height(self):\n",
        "        return self.data['height']\n",
        "\n",
        "    def bbox(self):\n",
        "        return self.bboxes\n",
        "    \n",
        "    def annotations(self):\n",
        "        return self.annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sl3AHk-xMSG6",
        "outputId": "2fbc378a-608a-4ad2-e788-579459aba5db"
      },
      "outputs": [],
      "source": [
        "class Coco(Dataset):\n",
        "  def __init__(self, size):\n",
        "    super(Coco, self).__init__()\n",
        "    self.size = size\n",
        "    #load json file\n",
        "    with open(local_annotations + 'instances.json', 'r') as f:\n",
        "      self.data = json.load(f)\n",
        "    #load references(umd)\n",
        "    with open(local_annotations + 'refs(umd).p', 'rb') as f:\n",
        "      self.references = pickle.load(f)\n",
        "      f.close()\n",
        "    #load references(google)\n",
        "    with open(local_annotations + 'refs(google).p', 'rb') as f:\n",
        "      self.references_google = pickle.load(f)\n",
        "      f.close()\n",
        "    \n",
        "    \n",
        "  def __getitem__(self, idx):\n",
        "    if idx >= self.size:\n",
        "      raise IndexError()\n",
        "    #import image form json\n",
        "    image = CocogImage(idx, self.data, self.references)\n",
        "    return image\n",
        "    \n",
        "    return image.get_image(), image.bbox(), image.annotations()\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.size\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "#test\n",
        "dataset = Coco(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "#print the first image\n",
        "dataset[0].show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### yolo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "#install dependencies\n",
        "! pip install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /Users/pappol/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 ðŸš€ 2023-4-8 Python-3.10.6 torch-1.13.0 CPU\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
            "Adding AutoShape... \n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'COCO_train2014_000000131074.jpg'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/pappol/main/uni/deep/nowe/deepRiccy/deep_riccy.ipynb Cella 15\u001b[0m in \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pappol/main/uni/deep/nowe/deepRiccy/deep_riccy.ipynb#X25sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m imgs \u001b[39m=\u001b[39m dataset[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mfile_name() \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pappol/main/uni/deep/nowe/deepRiccy/deep_riccy.ipynb#X25sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Inference\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/pappol/main/uni/deep/nowe/deepRiccy/deep_riccy.ipynb#X25sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m results \u001b[39m=\u001b[39m model(imgs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pappol/main/uni/deep/nowe/deepRiccy/deep_riccy.ipynb#X25sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Results\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pappol/main/uni/deep/nowe/deepRiccy/deep_riccy.ipynb#X25sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m results\u001b[39m.\u001b[39mprint()\n",
            "File \u001b[0;32m/opt/anaconda3/envs/ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m/opt/anaconda3/envs/ai/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:683\u001b[0m, in \u001b[0;36mAutoShape.forward\u001b[0;34m(self, ims, size, augment, profile)\u001b[0m\n\u001b[1;32m    681\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m  \u001b[39m# filename\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(im, (\u001b[39mstr\u001b[39m, Path)):  \u001b[39m# filename or uri\u001b[39;00m\n\u001b[0;32m--> 683\u001b[0m     im, f \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(requests\u001b[39m.\u001b[39;49mget(im, stream\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39;49mraw \u001b[39mif\u001b[39;49;00m \u001b[39mstr\u001b[39;49m(im)\u001b[39m.\u001b[39;49mstartswith(\u001b[39m'\u001b[39;49m\u001b[39mhttp\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39melse\u001b[39;49;00m im), im\n\u001b[1;32m    684\u001b[0m     im \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(exif_transpose(im))\n\u001b[1;32m    685\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(im, Image\u001b[39m.\u001b[39mImage):  \u001b[39m# PIL Image\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/ai/lib/python3.10/site-packages/PIL/Image.py:3092\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3089\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[1;32m   3091\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[0;32m-> 3092\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   3093\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   3095\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'COCO_train2014_000000131074.jpg'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "\n",
        "# Images\n",
        "imgs = dataset[0].file_name() \n",
        "# Inference\n",
        "results = model(imgs)\n",
        "\n",
        "# Results\n",
        "results.print()\n",
        "results.save()  # or .show()\n",
        "\n",
        "results.xyxy[0]  # img1 predictions (tensor)\n",
        "results.pandas().xyxy[0]  # img1 predictions (pandas)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
