{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YOvgNpY7LwSU"
      },
      "source": [
        "# Deep riccy project"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3CzySrjXzHhN"
      },
      "source": [
        "## Env creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiKX4oZgzKLR",
        "outputId": "36a5acca-ffd7-4e72-bc04-c039c1a3477d"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "! tar -zxf /content/drive/MyDrive/Uni/DeepRiccy/refcocog.tar.gz\n",
        "! pip3 install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt  --quiet\n",
        "! pip install ftfy regex tqdm --quiet\n",
        "! pip install git+https://github.com/openai/CLIP.git --quiet\n",
        "! pip install rouge-metric --quiet\n",
        "\n",
        "local_path = '/content/refcocog/images/'\n",
        "local_annotations = '/content/refcocog/annotations/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4ZuKVoVzdFH"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "import torch\n",
        "from PIL import Image, ImageDraw\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from pkg_resources import packaging\n",
        "import clip\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import skimage\n",
        "import IPython.display\n",
        "\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "import torchmetrics as tm"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PKNYTNptzWrx"
      },
      "source": [
        "## Baseline"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "418bIOjizjPc"
      },
      "source": [
        "### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExVxvNtacKZS"
      },
      "outputs": [],
      "source": [
        "def split_string(string):\n",
        "    string = string.split(\"_\")\n",
        "    string = string[:-1]\n",
        "    string = \"_\".join(string)\n",
        "    append = \".jpg\"\n",
        "    string = string + append\n",
        "    \n",
        "    return string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4rfPXV_zzHz"
      },
      "outputs": [],
      "source": [
        "def convert_bbox(bbox, img):\n",
        "  x1, y1, width, height = bbox\n",
        "  x2, y2 = x1 + width, y1 + height\n",
        "\n",
        "  # Verify coordinates\n",
        "  if x1 < 0 or y1 < 0 or x2 > img.width or y2 > img.height:\n",
        "      print(\"Bounding box fuori dai limiti dell'immagine!\")\n",
        "  else:\n",
        "    return x1, y1, x2, y2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MLRfZYnqz4zD"
      },
      "source": [
        "### Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Recall:\n",
        "    def __init__(self):\n",
        "        self.true_positives = 0\n",
        "        self.false_negatives = 0\n",
        "\n",
        "    def update(self, correct):\n",
        "        if correct:\n",
        "            self.true_positives += 1\n",
        "        else:\n",
        "            self.false_negatives += 1\n",
        "    \n",
        "    def compute(self):\n",
        "        return self.true_positives / (self.true_positives + self.false_negatives)\n",
        "    \n",
        "    def reset(self):\n",
        "        self.true_positives = 0\n",
        "        self.false_negatives = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yN9vbC82z8CI"
      },
      "outputs": [],
      "source": [
        "class Metrics:\n",
        "    def __init__(self, model, trasform):\n",
        "        self.transform = trasform\n",
        "        self.iou = tm.JaccardIndex()\n",
        "        self.recall = Recall()\n",
        "        self.model = model\n",
        "\n",
        "    def update(self, predicted_bbox, target_bbox, predicted_image, target_image):\n",
        "        predicted_bbox = torch.tensor(predicted_bbox)\n",
        "        target_bbox = torch.tensor(target_bbox)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Preprocess the predicted image and compute the predicted embedding\n",
        "            image = Image.fromarray(predicted_image)\n",
        "            image_tensor = self.transform(image)\n",
        "            image_tensor = image_tensor.unsqueeze(0)  # Add batch dimension\n",
        "            predicted_embedding = self.model.encode_image(image_tensor)\n",
        "\n",
        "            # Preprocess the target image and compute the target embedding\n",
        "            target_image_tensor = self.transform(target_image)\n",
        "            target_image_tensor = target_image_tensor.unsqueeze(0)  # Add batch dimension\n",
        "            target_embedding = self.model.encode_image(target_image_tensor)\n",
        "\n",
        "        cosine_similarity = torch.nn.functional.cosine_similarity(predicted_embedding, target_embedding)\n",
        "        euclidean_distance = torch.nn.functional.pairwise_distance(predicted_embedding, target_embedding)\n",
        "\n",
        "        self.iou.update(predicted_bbox, target_bbox)\n",
        "        #get iou value of the predicted bbox and the target bbox\n",
        "        iou= tm.JaccardIndex(predicted_bbox, target_bbox)\n",
        "        if iou > 0.5:\n",
        "          self.recall.update(1)\n",
        "\n",
        "        self.cosine_similarity.update(cosine_similarity)\n",
        "        self.euclidean_distance.update(euclidean_distance)\n",
        "\n",
        "    def to_string(self):\n",
        "        mean_iou = self.iou.compute()\n",
        "        recall_at_05_iou = self.recall.compute()\n",
        "        mean_cosine_similarity = self.cosine_similarity.compute()\n",
        "        mean_euclidean_distance = self.euclidean_distance.compute()\n",
        "\n",
        "        return f\"Mean IoU: {mean_iou:.4f}, Recall@0.5 IoU: {recall_at_05_iou:.4f}, Mean Cosine Similarity: {mean_cosine_similarity:.4f}, Mean Euclidean Distance: {mean_euclidean_distance:.4f}\"\n",
        "\n",
        "    def reset(self):\n",
        "        self.iou.reset()\n",
        "        self.recall.reset()\n",
        "        self.cosine_similarity.reset()\n",
        "        self.euclidean_distance.reset()\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx39cinC0BJb"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjC1bIj90D3I"
      },
      "outputs": [],
      "source": [
        "class Coco(Dataset):\n",
        "    def __init__(self, path_json, path_pickle, train=True):\n",
        "        self.path_json = path_json\n",
        "        self.path_pickle = path_pickle\n",
        "        self.train = train\n",
        "\n",
        "        #load images and annotations\n",
        "        with open(self.path_json) as json_data:\n",
        "            data = json.load(json_data)\n",
        "            self.ann_frame = pd.DataFrame(data['annotations'])\n",
        "            self.ann_frame = self.ann_frame.reset_index(drop=False)\n",
        "\n",
        "\n",
        "        #load annotations\n",
        "        with open(self.path_pickle, 'rb') as pickle_data:\n",
        "            data = pickle.load(pickle_data)\n",
        "            self.refs_frame = pd.DataFrame(data)\n",
        "\n",
        "        #separate each sentence in dataframe\n",
        "        self.refs_frame = self.refs_frame.explode('sentences')\n",
        "\n",
        "        self.refs_frame = self.refs_frame.reset_index(drop=False)\n",
        "\n",
        "        self.size = self.refs_frame.shape[0]\n",
        "\n",
        "        self.dataset = pd.merge(self.refs_frame, self.ann_frame, left_on='ann_id', right_on='id')\n",
        "        self.dataset = self.dataset.drop(columns=['segmentation', 'id', 'category_id_y','ref_id', 'index_x', 'iscrowd', 'image_id_y', 'image_id_x', 'category_id_x', 'ann_id', 'sent_ids', 'index_y', 'area'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.dataset.iloc[idx]\n",
        "\n",
        "    def get_annotation(self, idx):\n",
        "      return self.ann_frame.iloc[idx]\n",
        "    \n",
        "    def get_imgframe(self, idx):\n",
        "      return self.img_frame.iloc[idx]\n",
        "\n",
        "    def get_validation(self):\n",
        "        #return the dataframe that has as attribute the validation\n",
        "        return self.dataset[self.dataset['split'] == 'val']\n",
        "    \n",
        "    def get_test(self):\n",
        "        #return the dataframe that has as attribute the test\n",
        "        return self.dataset[self.dataset['split'] == 'test']\n",
        "    \n",
        "    def get_train(self):\n",
        "        #return the dataframe that has as attribute the train\n",
        "        return self.dataset[self.dataset['split'] == 'train']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peTMO0l70KRz",
        "outputId": "78475edb-6c61-40a4-ffbe-6b3c3b214e75"
      },
      "outputs": [],
      "source": [
        "#test dataset\n",
        "\n",
        "dataset = Coco(local_annotations + 'instances.json', local_annotations + \"refs(umd).p\")\n",
        "\n",
        "print(len(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "ZDV7Hyel0KmQ",
        "outputId": "fed6a100-757c-4be9-ee21-f06c292c8176"
      },
      "outputs": [],
      "source": [
        "dataset.dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JJq1udMa0WW-"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkfKW4cL0ToQ"
      },
      "outputs": [],
      "source": [
        "class VisualGrounding(torch.nn.Module):\n",
        "    def __init__(self, yolo_version, clip_version, local_path, img_path):\n",
        "        super(VisualGrounding, self).__init__()\n",
        "        self.local_path = local_path\n",
        "        self.img_path = img_path\n",
        "        #initialize models\n",
        "        self.yolo = torch.hub.load('ultralytics/yolov5', yolo_version , pretrained=True)\n",
        "        self.clip, self.preprocess = clip.load(clip_version)\n",
        "        \n",
        "        #define metrics\n",
        "        self.metrics = Metrics()\n",
        "        \n",
        "    \n",
        "    def forward(self, img_path, sentence):\n",
        "      max_similarity = 0\n",
        "      max_image = None\n",
        "      max_bbox = None\n",
        "\n",
        "      yolo_output = self.yolo(self.local_path+img_path)\n",
        "\n",
        "      original_img = Image.open(self.local_path+img_path).convert(\"RGB\")\n",
        "\n",
        "      for i in range(len(yolo_output.xyxy[0])):\n",
        "          x1 = yolo_output.xyxy[0][i][0].cpu().numpy()\n",
        "          x1 = np.rint(x1)\n",
        "          y1 = yolo_output.xyxy[0][i][1].cpu().numpy()\n",
        "          y1 = np.rint(y1)\n",
        "          x2 = yolo_output.xyxy[0][i][2].cpu().numpy()\n",
        "          x2 = np.rint(x2)\n",
        "          y2 = yolo_output.xyxy[0][i][3].cpu().numpy()\n",
        "          y2 = np.rint(y2)\n",
        "\n",
        "          img_cropped = original_img.crop((x1, y1, x2, y2))\n",
        "\n",
        "          img = self.preprocess(img_cropped).cuda().unsqueeze(0)\n",
        "          text = clip.tokenize([sentence]).cuda()\n",
        "\n",
        "          with torch.no_grad():\n",
        "              image_features = self.clip.encode_image(img).float()\n",
        "              text_features = self.clip.encode_text(text).float()\n",
        "\n",
        "          image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "          text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "          similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
        "\n",
        "          if similarity > max_similarity:\n",
        "              max_similarity = similarity\n",
        "              max_image = img_cropped\n",
        "              max_bbox = yolo_output.xyxy[0][i]\n",
        "\n",
        "          return max_bbox, max_image\n",
        "\n",
        "        \n",
        "    def evaluate(self, img_path, sentence, gt, original_img):\n",
        "      bbox = convert_bbox(gt, original_img)\n",
        "      gt_crop = original_img.crop(bbox)\n",
        "      prediction_bbox, prediction_img = self.forward(img_path, sentence)\n",
        "      self.metrics.update(prediction_bbox, bbox, prediction_img, gt_crop)\n",
        "      return prediction_bbox, prediction_img\n",
        "\n",
        "    def get_metrics(self):\n",
        "      return self.metrics.get()\n",
        "    \n",
        "    def reset_metrics(self):\n",
        "      self.metrics.reset()\n",
        "          \n",
        "        "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IHcFXg1R0cgv"
      },
      "source": [
        "### Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2e_3XlHHEozB"
      },
      "outputs": [],
      "source": [
        "dataset = Coco(local_annotations + 'instances.json', local_annotations + \"refs(umd).p\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Csy-JLatEhkf",
        "outputId": "b83fbaf1-3781-4e7c-f674-9dad9be0e647"
      },
      "outputs": [],
      "source": [
        "model = VisualGrounding('yolov5x','ViT-B/32', local_path, local_annotations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OJXjlmnb0a80",
        "outputId": "6412444b-9c25-47f8-812d-8cf2c568e5d1"
      },
      "outputs": [],
      "source": [
        "#evaluation of the model\n",
        "for i in range(0, 101):\n",
        "    input = dataset[i]\n",
        "    image_path = split_string(input[\"file_name\"])\n",
        "    sentence = input[\"sentences\"][\"raw\"]\n",
        "    gt = input[\"bbox\"]\n",
        "    original_img = Image.open(local_path + image_path).convert(\"RGB\")\n",
        "    # print img dimensions and box coordinates\n",
        "    bbox, img = model.evaluate(image_path, sentence, gt, original_img)\n",
        "    if model.get_metrics()[1] < 0.10:\n",
        "        %matplotlib inline\n",
        "        plt.imshow(original_img)\n",
        "        x1, y1, width, height = gt\n",
        "        x2, y2 = x1 + width, y1 + height\n",
        "        plt.gca().add_patch(plt.Rectangle((x1, y1), width, height, fill=False, edgecolor='red', linewidth=2))\n",
        "        bbox_cpu = bbox.cpu()  # Move the CUDA tensor to CPU\n",
        "        plt.gca().add_patch(plt.Rectangle((bbox_cpu[0], bbox_cpu[1]), bbox_cpu[2] - bbox_cpu[0], bbox_cpu[3] - bbox_cpu[1], fill=False, edgecolor='blue', linewidth=2))\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    if i % 20 == 0:\n",
        "        print(\"Iteration: \", i)\n",
        "        print(model.get_metrics())\n",
        "        "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
