{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOvgNpY7LwSU"
      },
      "source": [
        "## Deep riccy project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laAyWSRs-9vE"
      },
      "source": [
        "## Colab dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vd2FccpXL6WE",
        "outputId": "9a516044-6913-4636-b278-2a165843fb94"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyIe9EVpLwSW",
        "outputId": "c5a48eaa-8cdf-4bd1-f089-df2b17bec7e4"
      },
      "outputs": [],
      "source": [
        "! tar -zxvf /content/drive/MyDrive/Uni/DeepRiccy/refcocog.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-65_zCKW-9vG"
      },
      "outputs": [],
      "source": [
        "#set local path to the data\n",
        "\n",
        "#this is for colab\n",
        "#local_path = '/content/refcocog/images/'\n",
        "#local_annotations = '/content/refcocog/annotations/'\n",
        "\n",
        "#this is for local\n",
        "local_path = './refcocog/images/' \n",
        "local_annotations = './refcocog/annotations/' "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qg2Om1j0-9vG"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2VhMPW-s-9vG"
      },
      "outputs": [],
      "source": [
        "#import section\n",
        "import json\n",
        "import pickle\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Coco(Dataset):\n",
        "    def __init__(self, path_json, path_pickle, train=True):\n",
        "        self.path = path_json\n",
        "        self.path_pickle = path_pickle\n",
        "        self.train = train\n",
        "\n",
        "        #load images\n",
        "        with open(self.path) as json_data:\n",
        "            data = json.load(json_data)\n",
        "            self.img_frame = pd.DataFrame(data['images'])\n",
        "            self.img_frame = self.img_frame.set_index('id')\n",
        "\n",
        "        #load annotations\n",
        "        with open(self.path) as json_data:\n",
        "            data = json.load(json_data)\n",
        "            self.ann_frame = pd.DataFrame(data['annotations'])\n",
        "            self.ann_frame = self.ann_frame.set_index('id')\n",
        "\n",
        "        #load annotations\n",
        "        with open(self.path_pickle, 'rb') as pickle_data:\n",
        "            data = pickle.load(pickle_data)\n",
        "            self.refs_frame = pd.DataFrame(data)\n",
        "\n",
        "        self.size = self.refs_frame.shape[0]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        #get annotation\n",
        "        annotation = self.refs_frame.loc[idx]\n",
        "\n",
        "        #get image\n",
        "        img_id = annotation['image_id']\n",
        "        img_name = self.img_frame.iloc[img_id]['file_name']\n",
        "        img = Image.open(local_path+img_name)\n",
        "        \n",
        "        #get sentence\n",
        "        sentence = annotation['sentences'][0]\n",
        "\n",
        "        #ground truth\n",
        "        region_id = annotation['region_id']\n",
        "        gt = self.img_frame.iloc[img_id]['regions'][region_id]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "#load dataset with pandas\n",
        "with open(local_annotations+'instances.json') as json_data:\n",
        "    data = json.load(json_data)\n",
        "    img_frame = pd.DataFrame(data['images'])\n",
        "    img_frame = img_frame.set_index('id')\n",
        "\n",
        "#load annotations with pandas and pickle\n",
        "with open(local_annotations+\"refs(umd).p\", 'rb') as pickle_data:\n",
        "    data = pickle.load(pickle_data)\n",
        "    refs_frame = pd.DataFrame(data)\n",
        "\n",
        "with open(local_annotations+'instances.json') as json_data:\n",
        "            data = json.load(json_data)\n",
        "            ann_frame = pd.DataFrame(data['annotations'])\n",
        "            ann_frame = ann_frame.set_index('image_id')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "COCO_train2014_000000380440.jpg\n",
            "image_id\n",
            "380440     [228.98, 41.4, 141.95, 270.38]\n",
            "380440    [374.31, 65.06, 136.04, 201.94]\n",
            "380440     [244.21, 251.3, 141.07, 90.63]\n",
            "380440    [291.38, 190.07, 185.19, 68.27]\n",
            "380440      [416.16, 95.57, 66.98, 58.28]\n",
            "380440       [288.6, 75.67, 54.76, 27.93]\n",
            "Name: bbox, dtype: object\n",
            "{'tokens': ['the', 'man', 'in', 'yellow', 'coat'], 'raw': 'the man in yellow coat', 'sent_id': 8, 'sent': 'the man in yellow coat'}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "img_id = refs_frame.iloc[0]['image_id']\n",
        "sentence = refs_frame.loc[0]['sentences'][0]\n",
        "\n",
        "\n",
        "annotation = ann_frame.loc[img_id]\n",
        "bbox = annotation['bbox']\n",
        "\n",
        "img_name = img_frame.loc[img_id]['file_name']\n",
        "\n",
        "print(img_name)\n",
        "print(bbox)\n",
        "print(sentence)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### json standard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xDnIQI1-9vH"
      },
      "outputs": [],
      "source": [
        "#class creation for the items\n",
        "class CocogImage():\n",
        "    def __init__(self, id, json_file, referece, refg):\n",
        "        self.refg = refg\n",
        "        self.id = id\n",
        "        self.data = json_file['images'][id]\n",
        "        self.bboxes = []\n",
        "        self.img_id = self.data['id']\n",
        "\n",
        "        for i in json_file['annotations']:\n",
        "            if i['image_id'] == self.img_id:\n",
        "                self.bboxes.append(i[\"bbox\"])\n",
        "                \n",
        "        self.annotations = []\n",
        "\n",
        "        for i in referece:\n",
        "            if i['image_id'] == self.img_id:\n",
        "                self.annotations.append(i[\"sentences\"])\n",
        "        \n",
        "        self.refg = []\n",
        "        for i in refg:\n",
        "            if i['image_id'] == self.img_id:\n",
        "                self.refg.append(i)\n",
        "                    \n",
        "    def show(self):\n",
        "        img = Image.open(local_path + self.data['file_name'])\n",
        "        plt.imshow(img)\n",
        "        plt.show()\n",
        "\n",
        "    def show_bbox(self):\n",
        "        #print image with bbox\n",
        "        img = self.get_image()\n",
        "        plt.imshow(img)\n",
        "        for i in self.bboxes:\n",
        "            plt.gca().add_patch(plt.Rectangle((i[0], i[1]), i[2], i[3], fill=False, edgecolor='red', linewidth=2))\n",
        "        plt.show()\n",
        "\n",
        "    def get_image_tensor(self, transform = transforms.Compose([transforms.PILToTensor()])):\n",
        "        img = Image.open(local_path + self.data['file_name'])\n",
        "        return transform(img)\n",
        "    \n",
        "    def get_image(self):\n",
        "        return Image.open(local_path + self.data['file_name'])\n",
        "\n",
        "    def file_name(self):\n",
        "        return local_path+self.data['file_name']\n",
        "\n",
        "    def id(self):\n",
        "        return self.data['id']\n",
        "\n",
        "    def width(self):\n",
        "        return self.data['width']\n",
        "\n",
        "    def height(self):\n",
        "        return self.data['height']\n",
        "\n",
        "    def bbox(self):\n",
        "        return self.bboxes\n",
        "    \n",
        "    def annotations(self):\n",
        "        return self.annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sl3AHk-xMSG6"
      },
      "outputs": [],
      "source": [
        "class Coco(Dataset):\n",
        "  def __init__(self, size):\n",
        "    super(Coco, self).__init__()\n",
        "    self.size = size\n",
        "    #load json file\n",
        "    with open(local_annotations + 'instances.json', 'r') as f:\n",
        "      self.data = json.load(f)\n",
        "    #load references(umd)\n",
        "    with open(local_annotations + 'refs(umd).p', 'rb') as f:\n",
        "      self.references = pickle.load(f)\n",
        "      f.close()\n",
        "    #load references(google)\n",
        "    with open(local_annotations + 'refs(google).p', 'rb') as f:\n",
        "      self.references_google = pickle.load(f)\n",
        "      f.close()\n",
        "    \n",
        "    \n",
        "  def __getitem__(self, idx):\n",
        "    if idx >= self.size:\n",
        "      raise IndexError()\n",
        "    #import image form json\n",
        "    image = CocogImage(idx, self.data, self.references, self.references_google)\n",
        "    return image\n",
        "    \n",
        "    return image.get_image(), image.bbox(), image.annotations()\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.size\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vffDWMVk-9vI"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKKFGZTm-9vJ"
      },
      "outputs": [],
      "source": [
        "#test\n",
        "dataset = Coco(200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "pYlX4Et7-9vJ",
        "outputId": "b610fb14-1739-4bad-e332-ce8e73154eb3"
      },
      "outputs": [],
      "source": [
        "#print the first 10 images side by side\n",
        "fig, axs = plt.subplots(2, 5)\n",
        "fig.set_size_inches(18.5, 10.5)\n",
        "for i in range(10):\n",
        "    axs[i//5, i%5].imshow(dataset[i].get_image())\n",
        "    axs[i//5, i%5].set_title(str(dataset[i].id))\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#show image 1 with bbox\n",
        "dataset[1].show()\n",
        "print(\"cacca\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test dataset and show bboxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#get how many annotation have the img_id equal to 131074\n",
        "count = 0\n",
        "\n",
        "for i in dataset[0].refg:\n",
        "  if i['image_id'] == 131074:\n",
        "    count += 1\n",
        "print(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(local_annotations + 'refs(umd).p', 'rb') as f:\n",
        "      references = pickle.load(f)\n",
        "\n",
        "print(references[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#find in the pickle file the annotation with img_id 131074\n",
        "for i in dataset[0].refg:\n",
        "  if i['image_id'] == 131074:\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5w6IwHR-9vJ"
      },
      "source": [
        "### yolo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCYv3MMR-9vJ",
        "outputId": "5ca61299-583d-4441-c2bd-c2704a3ba862"
      },
      "outputs": [],
      "source": [
        "#install dependencies\n",
        "! pip3 install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        },
        "id": "ApYfNX5n-9vJ",
        "outputId": "b9b78017-8d07-4298-f1bb-195ab26f9656"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "# Images\n",
        "imgs = dataset[1].file_name() \n",
        "# Inference\n",
        "results = model(imgs)\n",
        "\n",
        "# Results\n",
        "results.print()\n",
        "results.show()  # or .show()\n",
        "\n",
        "results.xyxy[0]  # img1 predictions (tensor)\n",
        "results.pandas().xyxy[0]  # img1 predictions (pandas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "#save an image for each bounding box created by yolov5\n",
        "for i in range(len(results.xyxy[0])):\n",
        "    x1 = results.xyxy[0][i][0].numpy()\n",
        "    x1 = np.rint(x1)\n",
        "    y1 = results.xyxy[0][i][1].numpy()\n",
        "    y1 = np.rint(y1)\n",
        "    x2 = results.xyxy[0][i][2].numpy()\n",
        "    x2 = np.rint(x2)\n",
        "    y2 = results.xyxy[0][i][3].numpy()\n",
        "    y2 = np.rint(y2)\n",
        "    print(x1, y1, x2, y2)\n",
        "    img = Image.open(dataset[1].file_name())\n",
        "    img = img.crop((x1, y1, x2, y2))\n",
        "    img.save(\"img\"+str(i)+\".jpg\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qloNs0X66kIx"
      },
      "source": [
        "## Clip implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rA1tgAWX6oRc",
        "outputId": "bb556b76-e9c8-4fe9-d7fc-cfcdd94dc5fa"
      },
      "outputs": [],
      "source": [
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vk2Ivk6t6o3f",
        "outputId": "3d8b0e09-4305-41c3-a402-f4c27c9c4db7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from pkg_resources import packaging\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Klqn1-W86tLN",
        "outputId": "6951d18d-b285-489c-fbb6-9e93acd7915d"
      },
      "outputs": [],
      "source": [
        "import clip\n",
        "\n",
        "clip.available_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkGJBhaa6vy5",
        "outputId": "bc7a99d3-2c1e-41f4-a607-ac60dc65e74b"
      },
      "outputs": [],
      "source": [
        "model, preprocess = clip.load(\"ViT-B/32\")\n",
        "model.cuda().eval()\n",
        "input_resolution = model.visual.input_resolution\n",
        "context_length = model.context_length\n",
        "vocab_size = model.vocab_size\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Input resolution:\", input_resolution)\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIj4IUpY6x75"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import skimage\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "# images in skimage to use and their textual descriptions\n",
        "descriptions = {\n",
        "    \"page\": \"a page of text about segmentation\",\n",
        "    \"chelsea\": \"a facial photo of a tabby cat\",\n",
        "    \"astronaut\": \"a portrait of an astronaut with the American flag\",\n",
        "    \"rocket\": \"a rocket standing on a launchpad\",\n",
        "    \"motorcycle_right\": \"a red motorcycle standing in a garage\",\n",
        "    \"camera\": \"a person looking at a camera on a tripod\",\n",
        "    \"horse\": \"a black-and-white silhouette of a horse\", \n",
        "    \"coffee\": \"a cup of coffee on a saucer\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "T5ShHvTE7AOc",
        "outputId": "434a634f-c97d-46c4-8620-0ef0bd61eb1d"
      },
      "outputs": [],
      "source": [
        "original_images = []\n",
        "images = []\n",
        "texts = []\n",
        "plt.figure(figsize=(16, 5))\n",
        "\n",
        "for filename in [filename for filename in os.listdir(skimage.data_dir) if filename.endswith(\".png\") or filename.endswith(\".jpg\")]:\n",
        "    name = os.path.splitext(filename)[0]\n",
        "    if name not in descriptions:\n",
        "        continue\n",
        "\n",
        "    image = Image.open(os.path.join(skimage.data_dir, filename)).convert(\"RGB\")\n",
        "  \n",
        "    plt.subplot(2, 4, len(images) + 1)\n",
        "    plt.imshow(image)\n",
        "    plt.title(f\"{filename}\\n{descriptions[name]}\")\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    original_images.append(image)\n",
        "    images.append(preprocess(image))\n",
        "    texts.append(descriptions[name])\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzHLgmeG7Chl"
      },
      "outputs": [],
      "source": [
        "image_input = torch.tensor(np.stack(images)).cuda()\n",
        "text_tokens = clip.tokenize([\"This is \" + desc for desc in texts]).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxPElTG37C6T"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input).float()\n",
        "    text_features = model.encode_text(text_tokens).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnsxxesN7Exq"
      },
      "outputs": [],
      "source": [
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pkvkSEqD7Gsf",
        "outputId": "d654ab33-6e76-44a5-9ce7-b4954f591ba7"
      },
      "outputs": [],
      "source": [
        "count = len(descriptions)\n",
        "\n",
        "plt.figure(figsize=(20, 14))\n",
        "plt.imshow(similarity, vmin=0.1, vmax=0.3)\n",
        "# plt.colorbar()\n",
        "plt.yticks(range(count), texts, fontsize=18)\n",
        "plt.xticks([])\n",
        "for i, image in enumerate(original_images):\n",
        "    plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\n",
        "for x in range(similarity.shape[1]):\n",
        "    for y in range(similarity.shape[0]):\n",
        "        plt.text(x, y, f\"{similarity[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\n",
        "\n",
        "for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
        "  plt.gca().spines[side].set_visible(False)\n",
        "\n",
        "plt.xlim([-0.5, count - 0.5])\n",
        "plt.ylim([count + 0.5, -2])\n",
        "\n",
        "plt.title(\"Cosine similarity between text and image features\", size=20)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
