{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "! tar -zxvf /content/drive/MyDrive/Uni/DeepRiccy/refcocog.tar.gz\n",
    "! pip3 install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt \n",
    "! pip install ftfy regex tqdm\n",
    "! pip install git+https://github.com/openai/CLIP.git\n",
    "\n",
    "local_path = '/content/refcocog/images/'\n",
    "local_annotations = '/content/refcocog/annotations/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = './refcocog/images/' \n",
    "local_annotations = './refcocog/annotations/' "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/opt/anaconda3/envs/ai/lib/python3.10/site-packages/torchvision/image.so, 0x0006): symbol not found in flat namespace '__ZN3c106detail19maybe_wrap_dim_slowExxb'\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pkg_resources import packaging\n",
    "import clip\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import skimage\n",
    "import IPython.display\n",
    "\n",
    "from collections import OrderedDict\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coco(Dataset):\n",
    "    def __init__(self, path_json, path_pickle, train=True):\n",
    "        self.path_json = path_json\n",
    "        self.path_pickle = path_pickle\n",
    "        self.train = train\n",
    "\n",
    "        #load images and annotations\n",
    "        with open(self.path_json) as json_data:\n",
    "            data = json.load(json_data)\n",
    "            self.img_frame = pd.DataFrame(data['images'])\n",
    "            self.ann_frame = pd.DataFrame(data['annotations'])\n",
    "\n",
    "        #load annotations\n",
    "        with open(self.path_pickle, 'rb') as pickle_data:\n",
    "            data = pickle.load(pickle_data)\n",
    "            self.refs_frame = pd.DataFrame(data)\n",
    "\n",
    "        self.size = self.refs_frame.shape[0]\n",
    "\n",
    "        #separate each sentence in dataframe\n",
    "        self.refs_frame = self.refs_frame.explode('sent_ids')\n",
    "        self.refs_frame = self.refs_frame.explode('sentences')\n",
    "        self.refs_frame = self.refs_frame.reset_index(drop=True)\n",
    "\n",
    "        self.dataset = pd.merge(self.refs_frame, self.img_frame, left_on='ann_id', right_on='id')\n",
    "        self.dataset = self.dataset.drop(columns=['ann_id'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset.iloc[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_string(string):\n",
    "    string = string.split(\"_\")\n",
    "    string = string[:-1]\n",
    "    string = \"_\".join(string)\n",
    "    append = \".jpg\"\n",
    "    string = string + append\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_id                                                    419645\n",
      "split                                                         test\n",
      "sentences        {'tokens': ['there', 'is', 'red', 'colored', '...\n",
      "file_name_x                 COCO_train2014_000000419645_398406.jpg\n",
      "category_id                                                      8\n",
      "sent_ids                                                        10\n",
      "ref_id                                                           1\n",
      "license                                                          2\n",
      "file_name_y                        COCO_train2014_000000398406.jpg\n",
      "coco_url                           http://mscoco.org/images/398406\n",
      "height                                                         493\n",
      "width                                                          500\n",
      "date_captured                                  2013-11-14 12:11:23\n",
      "flickr_url       http://farm4.staticflickr.com/3043/2503053592_...\n",
      "id                                                          398406\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#test dataset\n",
    "\n",
    "dataset = Coco(local_annotations + 'instances.json', local_annotations + \"refs(umd).p\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test clip\n",
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/pappol/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2023-4-8 Python-3.10.6 torch-2.0.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Couldn't load custom C++ ops. This can happen if your PyTorch and torchvision versions are incompatible, or if you had errors while compiling torchvision from source. For further information on the compatible versions, check https://github.com/pytorch/vision#installation for the compatibility matrix. Please check your PyTorch version with torch.__version__ and your torchvision version with torchvision.__version__ and verify if they are compatible, and if not please reinstall torchvision so that it matches your PyTorch install.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m yolo \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mhub\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39multralytics/yolov5\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39myolov5s\u001b[39m\u001b[39m'\u001b[39m, pretrained\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m image \u001b[39m=\u001b[39m split_string(dataset[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mfile_name_x\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m----> 8\u001b[0m yolo_output \u001b[39m=\u001b[39m yolo(local_path\u001b[39m+\u001b[39;49mimage)\n\u001b[1;32m     10\u001b[0m clip, preprocess \u001b[39m=\u001b[39m clip\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mViT-B/32\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[39m#pass each bounding box segmented image into clip with the sentence\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:708\u001b[0m, in \u001b[0;36mAutoShape.forward\u001b[0;34m(self, ims, size, augment, profile)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[39m# Post-process\u001b[39;00m\n\u001b[1;32m    707\u001b[0m \u001b[39mwith\u001b[39;00m dt[\u001b[39m2\u001b[39m]:\n\u001b[0;32m--> 708\u001b[0m     y \u001b[39m=\u001b[39m non_max_suppression(y \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdmb \u001b[39melse\u001b[39;49;00m y[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m    709\u001b[0m                             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconf,\n\u001b[1;32m    710\u001b[0m                             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miou,\n\u001b[1;32m    711\u001b[0m                             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclasses,\n\u001b[1;32m    712\u001b[0m                             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magnostic,\n\u001b[1;32m    713\u001b[0m                             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmulti_label,\n\u001b[1;32m    714\u001b[0m                             max_det\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_det)  \u001b[39m# NMS\u001b[39;00m\n\u001b[1;32m    715\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n):\n\u001b[1;32m    716\u001b[0m         scale_boxes(shape1, y[i][:, :\u001b[39m4\u001b[39m], shape0[i])\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/utils/general.py:982\u001b[0m, in \u001b[0;36mnon_max_suppression\u001b[0;34m(prediction, conf_thres, iou_thres, classes, agnostic, multi_label, labels, max_det, nm)\u001b[0m\n\u001b[1;32m    980\u001b[0m c \u001b[39m=\u001b[39m x[:, \u001b[39m5\u001b[39m:\u001b[39m6\u001b[39m] \u001b[39m*\u001b[39m (\u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m agnostic \u001b[39melse\u001b[39;00m max_wh)  \u001b[39m# classes\u001b[39;00m\n\u001b[1;32m    981\u001b[0m boxes, scores \u001b[39m=\u001b[39m x[:, :\u001b[39m4\u001b[39m] \u001b[39m+\u001b[39m c, x[:, \u001b[39m4\u001b[39m]  \u001b[39m# boxes (offset by class), scores\u001b[39;00m\n\u001b[0;32m--> 982\u001b[0m i \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mnms(boxes, scores, iou_thres)  \u001b[39m# NMS\u001b[39;00m\n\u001b[1;32m    983\u001b[0m i \u001b[39m=\u001b[39m i[:max_det]  \u001b[39m# limit detections\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[39mif\u001b[39;00m merge \u001b[39mand\u001b[39;00m (\u001b[39m1\u001b[39m \u001b[39m<\u001b[39m n \u001b[39m<\u001b[39m \u001b[39m3E3\u001b[39m):  \u001b[39m# Merge NMS (boxes merged using weighted mean)\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[39m# update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai/lib/python3.10/site-packages/torchvision/ops/boxes.py:40\u001b[0m, in \u001b[0;36mnms\u001b[0;34m(boxes, scores, iou_threshold)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_tracing():\n\u001b[1;32m     39\u001b[0m     _log_api_usage_once(nms)\n\u001b[0;32m---> 40\u001b[0m _assert_has_ops()\n\u001b[1;32m     41\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mtorchvision\u001b[39m.\u001b[39mnms(boxes, scores, iou_threshold)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai/lib/python3.10/site-packages/torchvision/extension.py:48\u001b[0m, in \u001b[0;36m_assert_has_ops\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_assert_has_ops\u001b[39m():\n\u001b[1;32m     47\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _has_ops():\n\u001b[0;32m---> 48\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m     49\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load custom C++ ops. This can happen if your PyTorch and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtorchvision versions are incompatible, or if you had errors while compiling \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     51\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtorchvision from source. For further information on the compatible versions, check \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mhttps://github.com/pytorch/vision#installation for the compatibility matrix. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     53\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPlease check your PyTorch version with torch.__version__ and your torchvision \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     54\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mversion with torchvision.__version__ and verify if they are compatible, and if not \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mplease reinstall torchvision so that it matches your PyTorch install.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     56\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Couldn't load custom C++ ops. This can happen if your PyTorch and torchvision versions are incompatible, or if you had errors while compiling torchvision from source. For further information on the compatible versions, check https://github.com/pytorch/vision#installation for the compatibility matrix. Please check your PyTorch version with torch.__version__ and your torchvision version with torchvision.__version__ and verify if they are compatible, and if not please reinstall torchvision so that it matches your PyTorch install."
     ]
    }
   ],
   "source": [
    "#define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#pass image into yolo\n",
    "yolo = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "image = split_string(dataset[0][\"file_name_x\"])\n",
    "\n",
    "yolo_output = yolo(local_path+image)\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/32\")\n",
    "model.eval()\n",
    "#pass each bounding box segmented image into clip with the sentence\n",
    "for i in range(len(yolo_output.xyxy[0])):\n",
    "\n",
    "    x1 = yolo_output.xyxy[0][i][0].numpy()\n",
    "    x1 = np.rint(x1)\n",
    "    y1 = yolo_output.xyxy[0][i][1].numpy()\n",
    "    y1 = np.rint(y1)\n",
    "    x2 = yolo_output.xyxy[0][i][2].numpy()\n",
    "    x2 = np.rint(x2)\n",
    "    y2 = yolo_output.xyxy[0][i][3].numpy()\n",
    "    y2 = np.rint(y2)\n",
    "\n",
    "    print(x1, y1, x2, y2)\n",
    "\n",
    "    img = Image.open(local_path+image)\n",
    "    img = img.crop((x1, y1, x2, y2))\n",
    "    img = preprocess(img).unsqueeze(0)\n",
    "    image_imput = torch.tensor(img)\n",
    "    sentence = dataset[0][\"sentences\"]\n",
    "    text = model.tokenize([sentence]).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = clip.encode_image(img).float()\n",
    "        text_features = clip.encode_text(text).float()\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
    "    print(similarity)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
