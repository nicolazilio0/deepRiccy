{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolazilio0/deepRiccy/blob/main/Complete_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kG4q9Z8P5lGR"
      },
      "source": [
        "<h1> VISUAL GROUNDING WITH CLIP AND STABLE DIFFUSION </h1>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FhcaV2gs6aEZ"
      },
      "source": [
        "## Abstract"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AY56aVki6dy9"
      },
      "source": [
        "### Pipeline representation \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DSFjwKMn5ysT"
      },
      "source": [
        "![picture](https://drive.google.com/uc?id=1WhLll6F3R1yMuDl1-gzZa3ZEjOdi28DA\n",
        ")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HZIZlo-WpwH-"
      },
      "source": [
        "## Setup environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtYDushgpY2q"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "#! tar -zxvf /content/drive/MyDrive/refcocog.tar.gz\n",
        "\n",
        "! pip3 install ftfy regex tqdm --quiet\n",
        "! pip3 install diffusers==0.11.1 --quiet\n",
        "! pip3 install transformers scipy ftfy accelerate --quiet\n",
        "! pip3 install stanza --quiet\n",
        "! pip3 install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt  --quiet\n",
        "! pip install ftfy regex tqdm --quiet\n",
        "! pip install git+https://github.com/openai/CLIP.git --quiet\n",
        "! pip install rouge-metric --quiet\n",
        "! pip install torchmetrics --quiet\n",
        "! pip install torchvision --quiet\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VssKwdfpn41"
      },
      "outputs": [],
      "source": [
        "local_path = './refcocog/images/' \n",
        "local_annotations = './refcocog/annotations/' "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "by2_wgHIp8CD"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pickle\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from pkg_resources import packaging\n",
        "import clip\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import skimage\n",
        "import IPython.display\n",
        "\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "import torchmetrics as tm\n",
        "import torchvision\n",
        "from torchvision import ops\n",
        "\n",
        "import os\n",
        "import skimage\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "from collections import OrderedDict\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from diffusers import DPMSolverMultistepScheduler\n",
        "import stanza\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
        "\n",
        "\n",
        "from transformers import ViTFeatureExtractor, VisionEncoderDecoderModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import RobertaTokenizerFast\n",
        "\n",
        "import requests\n",
        "from transformers import Seq2SeqTrainer\n",
        "from transformers import Seq2SeqTrainingArguments\n",
        "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
        "\n",
        "import datasets\n",
        "from transformers import default_data_collator\n",
        "import argparse\n",
        "\n",
        "#ignore warnings for cleaner output\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# remove the id in the image name string\n",
        "def split_string(string):\n",
        "    string = string.split(\"_\")\n",
        "    string = string[:-1]\n",
        "    string = \"_\".join(string)\n",
        "    append = \".jpg\"\n",
        "    string = string + append\n",
        "\n",
        "    return string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validation function if given a dataframe runs the model parameter for the whole dataframe and prints the metrics\n",
        "def validate(model, dataframe):\n",
        "    model.reset_metrics()\n",
        "    for i in tqdm(range(0, len(dataframe))):\n",
        "        input = dataframe.iloc[i]\n",
        "        image_path = split_string(input[\"file_name\"])\n",
        "        sentence = input[\"sentences\"][\"raw\"]\n",
        "        gt = input[\"bbox\"]\n",
        "        original_img = Image.open(local_path + image_path).convert(\"RGB\")\n",
        "        # print img dimensions and box coordinates\n",
        "        model.evaluate(image_path, sentence, gt, original_img, i)\n",
        "\n",
        "    model.save_metrics()\n",
        "    print(model.get_metrics())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_on_one_image(model, dataframe, index):\n",
        "    model.reset_metrics()\n",
        "\n",
        "    input = dataframe.iloc[index]\n",
        "    image_path = split_string(input[\"file_name\"])\n",
        "    sentence = input[\"sentences\"][\"raw\"]\n",
        "    gt = input[\"bbox\"]\n",
        "\n",
        "    original_img = Image.open(local_path + image_path).convert(\"RGB\")\n",
        "\n",
        "    # print img dimensions and box coordinates\n",
        "    bbox, _ =model.evaluate(image_path, sentence, gt, original_img, index)\n",
        "    bbox = bbox.cpu().numpy()\n",
        "    #show image with bbox and caption and gound truth\n",
        "    %matplotlib inline\n",
        "    plt.imshow(original_img)\n",
        "\n",
        "    x1, y1, width, height = gt\n",
        "\n",
        "    plt.gca().add_patch(plt.Rectangle((x1, y1), width, height, fill=False, edgecolor='red', linewidth=2))\n",
        "    \n",
        "    plt.gca().add_patch(plt.Rectangle((bbox[0], bbox[1]), bbox[2]-bbox[0], bbox[3]-bbox[1], fill=False, edgecolor='blue', linewidth=2))\n",
        "    print(sentence)\n",
        "    plt.show()\n",
        "\n",
        "    print(model.get_metrics())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# convert yolo format bbox into standard type\n",
        "def convert_bbox(bbox, img):\n",
        "    x1, y1, width, height = bbox\n",
        "    x2, y2 = x1 + width, y1 + height\n",
        "\n",
        "    # Verify coordinates\n",
        "    if x1 < 0 or y1 < 0 or x2 > img.width or y2 > img.height:\n",
        "        print(\"Bounding box fuori dai limiti dell'immagine!\")\n",
        "    else:\n",
        "        return x1, y1, x2, y2\n",
        "    \n",
        "# yolo bbox include class and precision, drop them\n",
        "def convert_yolo_bbox(bbox):\n",
        "    return bbox[:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clear_caption(caption):\n",
        "    caption = caption.replace('<s>', '')\n",
        "    caption = caption.replace('</s>', '')\n",
        "    return caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def crop_yolo(yolo_output, img, index):\n",
        "    x1 = yolo_output.xyxy[0][index][0].cpu().numpy()\n",
        "    x1 = np.rint(x1)\n",
        "    y1 = yolo_output.xyxy[0][index][1].cpu().numpy()\n",
        "    y1 = np.rint(y1)\n",
        "    x2 = yolo_output.xyxy[0][index][2].cpu().numpy()\n",
        "    x2 = np.rint(x2)\n",
        "    y2 = yolo_output.xyxy[0][index][3].cpu().numpy()\n",
        "    y2 = np.rint(y2)\n",
        "\n",
        "    cropped_img = img.crop((x1, y1, x2, y2))\n",
        "\n",
        "    return cropped_img"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_EbkjMszrDDJ"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMOzIbl4rHL5"
      },
      "outputs": [],
      "source": [
        "# dataset class definition\n",
        "class Coco(Dataset):\n",
        "    def __init__(self, path_json, path_pickle, train=True):\n",
        "        self.path_json = path_json\n",
        "        self.path_pickle = path_pickle\n",
        "        self.train = train\n",
        "\n",
        "        # load images and annotations\n",
        "        with open(self.path_json) as json_data:\n",
        "            data = json.load(json_data)\n",
        "            self.ann_frame = pd.DataFrame(data['annotations'])\n",
        "            self.ann_frame = self.ann_frame.reset_index(drop=False)\n",
        "\n",
        "        with open(self.path_pickle, 'rb') as pickle_data:\n",
        "            data = pickle.load(pickle_data)\n",
        "            self.refs_frame = pd.DataFrame(data)\n",
        "\n",
        "        # separate each sentence in dataframe\n",
        "        self.refs_frame = self.refs_frame.explode('sentences')\n",
        "        self.refs_frame = self.refs_frame.reset_index(drop=False)\n",
        "\n",
        "        self.size = self.refs_frame.shape[0]\n",
        "\n",
        "        # merge the dataframes\n",
        "        self.dataset = pd.merge(\n",
        "            self.refs_frame, self.ann_frame, left_on='ann_id', right_on='id')\n",
        "        # drop useless columns for cleaner and smaller dataset\n",
        "        self.dataset = self.dataset.drop(columns=['segmentation', 'id', 'category_id_y', 'ref_id', 'index_x',\n",
        "                                         'iscrowd', 'image_id_y', 'image_id_x', 'category_id_x', 'ann_id', 'sent_ids', 'index_y', 'area'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.dataset.iloc[idx]\n",
        "\n",
        "    def get_annotation(self, idx):\n",
        "        return self.ann_frame.iloc[idx]\n",
        "\n",
        "    def get_imgframe(self, idx):\n",
        "        return self.img_frame.iloc[idx]\n",
        "\n",
        "    def get_validation(self):\n",
        "        return self.dataset[self.dataset['split'] == 'val']\n",
        "\n",
        "    def get_test(self):\n",
        "        return self.dataset[self.dataset['split'] == 'test']\n",
        "\n",
        "    def get_train(self):\n",
        "        return self.dataset[self.dataset['split'] == 'train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PbA4ZfVrLTF"
      },
      "outputs": [],
      "source": [
        "#test dataset\n",
        "\n",
        "dataset = Coco(local_annotations + 'instances.json', local_annotations + \"refs(umd).p\")\n",
        "print(dataset[0])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Metrics:\n",
        "    def __init__(self, model, name):\n",
        "        self.name = name\n",
        "        self.treshold = 0.5\n",
        "        self.transform = torchvision.transforms.Compose([\n",
        "            torchvision.transforms.ToTensor(),\n",
        "            torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                             std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        # initialize torch tensor\n",
        "        self.iou = torch.tensor([]).cuda()\n",
        "        self.recall = Recall()\n",
        "        self.model = model\n",
        "        self.cosine_similarity = torch.tensor([]).cuda()\n",
        "        self.euclidean_distance = torch.tensor([]).cuda()\n",
        "\n",
        "    def update(self, predicted_bbox, target_bbox, predicted_image, target_image):\n",
        "        predicted_bbox = torch.tensor(predicted_bbox)\n",
        "        target_bbox = torch.tensor(target_bbox)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Preprocess the predicted image and compute the predicted embedding\n",
        "            predicted_image = padd_image(predicted_image)\n",
        "            image_tensor = self.transform(predicted_image)\n",
        "            image_tensor = image_tensor.unsqueeze(\n",
        "                0).cuda()  # Add batch dimension\n",
        "            predicted_embedding = self.model.encode_image(image_tensor)\n",
        "\n",
        "            # Preprocess the target image and compute the target embedding\n",
        "            target_image = padd_image(target_image)\n",
        "            target_image_tensor = self.transform(target_image)\n",
        "            target_image_tensor = target_image_tensor.unsqueeze(\n",
        "                0).cuda()  # Add batch dimension\n",
        "            target_embedding = self.model.encode_image(target_image_tensor)\n",
        "\n",
        "        similarity = torch.nn.functional.cosine_similarity(\n",
        "            predicted_embedding, target_embedding)\n",
        "        distance = torch.nn.functional.pairwise_distance(\n",
        "            predicted_embedding, target_embedding)\n",
        "\n",
        "        # convert bboxes into torch tensors\n",
        "        predicted_bbox = torch.tensor(predicted_bbox)\n",
        "        target_bbox = torch.tensor(target_bbox)\n",
        "        predicted_bbox = convert_yolo_bbox(predicted_bbox)\n",
        "        actual_iou = ops.box_iou(predicted_bbox.unsqueeze(\n",
        "            0).cuda(), target_bbox.unsqueeze(0).cuda())\n",
        "        self.iou = torch.cat((self.iou, actual_iou), 0)\n",
        "        # get iou value of the predicted bbox and the target bbox\n",
        "        if actual_iou > self.treshold:\n",
        "            self.recall.update(True)\n",
        "        else:\n",
        "            self.recall.update(False)\n",
        "        self.cosine_similarity = torch.cat(\n",
        "            (self.cosine_similarity, similarity), 0)\n",
        "        self.euclidean_distance = torch.cat(\n",
        "            (self.euclidean_distance, distance), 0)\n",
        "\n",
        "    def to_string(self):\n",
        "        mean_iou = torch.mean(self.iou)\n",
        "        recall_at_05_iou = self.recall.compute()\n",
        "        mean_cosine_similarity = torch.mean(self.cosine_similarity)\n",
        "        mean_euclidean_distance = torch.mean(self.euclidean_distance)\n",
        "\n",
        "        return f\"Mean IoU: {mean_iou:.4f}, Recall@0.5 IoU: {recall_at_05_iou:.4f}, Mean Cosine Similarity: {mean_cosine_similarity:.4f}, Mean Euclidean Distance: {mean_euclidean_distance:.4f}\"\n",
        "\n",
        "    def  save(self):\n",
        "        iou = self.iou.cpu().numpy()\n",
        "        cosine_similarity = self.cosine_similarity.cpu().numpy()\n",
        "        euclidean_distance = self.euclidean_distance.cpu().numpy()\n",
        "\n",
        "        np.savetxt(self.name+\"_iou.csv\", iou, delimiter=\",\")\n",
        "        np.savetxt(self.name+\"_cosine_similarity.csv\", cosine_similarity, delimiter=\",\")\n",
        "        np.savetxt(self.name+\"_euclidean_distance.csv\", euclidean_distance, delimiter=\",\")\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        self.iou = torch.tensor([]).cuda()\n",
        "        self.recall.reset()\n",
        "        self.cosine_similarity = torch.tensor([]).cuda()\n",
        "        self.euclidean_distance = torch.tensor([]).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# definition of recall metric\n",
        "class Recall:\n",
        "    def __init__(self):\n",
        "        self.true_positives = 0\n",
        "        self.false_negatives = 0\n",
        "\n",
        "    def update(self, correct):\n",
        "        if correct:\n",
        "            self.true_positives += 1\n",
        "        else:\n",
        "            self.false_negatives += 1\n",
        "\n",
        "    def compute(self):\n",
        "        return self.true_positives / (self.true_positives + self.false_negatives)\n",
        "\n",
        "    def reset(self):\n",
        "        self.true_positives = 0\n",
        "        self.false_negatives = 0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YC2G0s3ZrQfZ"
      },
      "source": [
        "## Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VisualGrounding_baseline(torch.nn.Module):\n",
        "    def __init__(self, yolo_version, clip_version, local_path, img_path):\n",
        "        super(VisualGrounding_baseline, self).__init__()\n",
        "        self.local_path = local_path\n",
        "        self.img_path = img_path\n",
        "\n",
        "        # initialize models\n",
        "        self.yolo = torch.hub.load(\n",
        "            'ultralytics/yolov5', yolo_version, pretrained=True)\n",
        "        self.clip, self.preprocess = clip.load(clip_version)\n",
        "\n",
        "        self.name = \"baseline\"\n",
        "        # define metrics\n",
        "        self.metrics = Metrics(self.clip, self.name)\n",
        "\n",
        "        \n",
        "\n",
        "    def forward(self, img_path, sentence):\n",
        "        max_similarity = 0\n",
        "        max_image = None\n",
        "        max_bbox = None\n",
        "\n",
        "        yolo_output = self.yolo(self.local_path+img_path)\n",
        "\n",
        "        original_img = Image.open(self.local_path+img_path).convert(\"RGB\")\n",
        "        \n",
        "        for i in range(len(yolo_output.xyxy[0])):\n",
        "            #crop the image based on the yolo output\n",
        "            img_cropped = crop_yolo(yolo_output, original_img, i)\n",
        "\n",
        "            img = self.preprocess(img_cropped).cuda().unsqueeze(0)\n",
        "            text = clip.tokenize([sentence]).cuda()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                image_features = self.clip.encode_image(img).float()\n",
        "                text_features = self.clip.encode_text(text).float()\n",
        "\n",
        "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "            similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
        "\n",
        "            if similarity > max_similarity:\n",
        "                max_similarity = similarity\n",
        "                max_image = img_cropped\n",
        "                max_bbox = yolo_output.xyxy[0][i]\n",
        "\n",
        "        if max_image is None:\n",
        "            #set bbox to the whole image\n",
        "            max_bbox = [0, 0, original_img.width, original_img.height]\n",
        "            max_image = original_img\n",
        "\n",
        "        return max_bbox, max_image\n",
        "\n",
        "    def evaluate(self, img_path, sentence, gt, original_img):\n",
        "        bbox = convert_bbox(gt, original_img)\n",
        "        gt_crop = original_img.crop(bbox)\n",
        "        prediction_bbox, prediction_img = self.forward(img_path, sentence)\n",
        "        self.metrics.update(prediction_bbox, bbox, prediction_img, gt_crop)\n",
        "        return prediction_bbox, prediction_img\n",
        "\n",
        "    def reset_metrics(self):\n",
        "        self.metrics.reset()\n",
        "\n",
        "    def get_metrics(self):\n",
        "        return self.metrics.to_string()\n",
        "\n",
        "    def save_metrics(self):\n",
        "        self.metrics.save()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uxBfW-eFrt8y"
      },
      "source": [
        "### Padd YOLO bounding boxes with average pixel value \n",
        "\n",
        "Since the bounding boxes that we obtain as output from YOLO are not in a predefined shape, and by default CLIP preprocesses them in order to have them in a size of 224x224, and feeding them into the CLIP encoder without processing them will cause them to be stretched, enlarged resulting in a loss of information. We don't want to lose information on our images, so we decided to apply a padding of the image in order to have them resized by 224x224 but mantaining also the informations. \n",
        "The colour we applied to the padding depends on the mean value of the colors of the pixels of the image. This is done in order both to reduce the loss of informaion, but also not to add noise or other informations inside the image. \n",
        "The way we apply the padding depends on the size of the image: \n",
        "- if the image has one of the two dimensions larger than 224, we create a square image with the size of the larger between width and height of the image, and with colour as the mean value of the colors. We then put our starting image at the center, and in the end it is resized to 224x224\n",
        "- if both the dimensions are lower than 224, the original image is just padded\n",
        "\n",
        "An example of the results we obtain is the following:\n",
        "![picture](https://drive.google.com/uc?id=1UgdwmBkeVIV0Zz21nKdJpYZ2NEQfk229\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKEdDlzlr4m8"
      },
      "outputs": [],
      "source": [
        "def padd_image(img):\n",
        "  print(type(img))\n",
        "  avg_color_per_row = np.average(img, axis=0)\n",
        "  avg_color = np.average(avg_color_per_row, axis=0)\n",
        "  old_image_width,old_image_height  = img.size\n",
        "  # create new image of desired size and color (blue) for padding\n",
        "  if(old_image_height>224 or old_image_width>224):\n",
        "    if(old_image_height>old_image_width):\n",
        "          new_image_width = old_image_height\n",
        "          new_image_height = old_image_height\n",
        "\n",
        "\n",
        "    else:\n",
        "          new_image_width = old_image_width\n",
        "          new_image_height = old_image_width\n",
        "\n",
        "    \n",
        "    color=avg_color\n",
        "    #color = (255,0,255)\n",
        "\n",
        "    result = np.full((new_image_height,new_image_width, 3), color, dtype=np.uint8)\n",
        "\n",
        "    # compute center offset\n",
        "    x_center = (new_image_width - old_image_width) // 2\n",
        "    y_center = (new_image_height - old_image_height) // 2\n",
        "\n",
        "    # copy img image into center of result image\n",
        "    result[y_center:y_center+old_image_height, \n",
        "          x_center:x_center+old_image_width] = img   \n",
        "\n",
        "        \n",
        "  else:\n",
        "    new_image_width = 224\n",
        "    new_image_height = 224\n",
        "    '''if(old_image_height>old_image_width):\n",
        "          new_image_width = old_image_height\n",
        "          new_image_height = old_image_height\n",
        "\n",
        "\n",
        "    else:\n",
        "          new_image_width = old_image_width\n",
        "          new_image_height = old_image_width'''\n",
        " \n",
        "    #color = (255,0,255)\n",
        "    color=avg_color\n",
        "    result = np.full((new_image_height,new_image_width, 3), color, dtype=np.uint8)\n",
        "\n",
        "    # compute center offset\n",
        "    x_center = (new_image_width - old_image_width) // 2\n",
        "    y_center = (new_image_height - old_image_height) // 2\n",
        "\n",
        "    # copy img image into center of result image\n",
        "    result[y_center:y_center+old_image_height, \n",
        "          x_center:x_center+old_image_width] = img\n",
        "\n",
        "\n",
        "  img= Image.fromarray(result)\n",
        "  img=img.resize((224,224))\n",
        "  return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyZczgDMr7R-"
      },
      "outputs": [],
      "source": [
        "original_images = []\n",
        "images = []\n",
        "texts = []\n",
        "plt.figure(figsize=(16, 5))\n",
        "\n",
        "for filename in [filename for filename in sorted(os.listdir(os.getcwd())) if filename.endswith(\".png\") or filename.endswith(\".jpg\")]:\n",
        "    print(filename)\n",
        "    name = os.path.splitext(filename)[0]\n",
        "\n",
        "    image = Image.open(os.path.join(os.getcwd(), filename)).convert(\"RGB\")\n",
        "    image=padd_image(image)\n",
        "\n",
        "  \n",
        "    plt.subplot(4, 8, len(images) + 1)\n",
        "    plt.imshow(image)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    original_images.append(image)\n",
        "    images.append(preprocess(image))    \n",
        "    texts.append(filename)\n",
        "\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bTNeb4-fsL5c"
      },
      "source": [
        "# Stanza NLP analysis "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-XGVBkcjNWXU"
      },
      "source": [
        "In order to discriminate our predictions we use some Natural Language Processing techniques in order to find the subject of our caption. \n",
        "What we decided to use is the Dependency graph. This is useful because it represent the dependencies between the various tokens inside of the sentence. \n",
        "In fact in each sentence we can define the so called **root** of the sentence, which is the token on which all other tokens in the sentence depend. This can generate two possible scenarios: \n",
        "- **The root is a noun**: this is the best case for our task since we can identify that name as the subject of our caption.\n",
        "- **The root is a verb**:  This create what is called a \"verbal phrase\", which is a sentence in which the root is the verb. Removing it from the sentence would make it lose meaning. When we have such type of sentence, there is high chance of having inside of the dependencies what is called \"NSUBJ\" or  [Nominal Subject](https://universaldependencies.org/en/dep/nsubj.html) (both active and passive). If it is present than this is the subject of our sentence. \n",
        "Otherwise if it is not present, once we have found the root of the sentence as a verb, we go back from the verb to the beginning of the sentence and we select as root of the sentence the first noun that we find. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CHgIlEY2OT-W"
      },
      "source": [
        "<img src='https://editor.analyticsvidhya.com/uploads/29920Screenshot%20(127).png' >\n",
        "Example of a dependency graph"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Sct319eOsPYF"
      },
      "source": [
        "### Install stanza dependency graph model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1Thgx_4sTeG"
      },
      "outputs": [],
      "source": [
        "#import stanza NLP model for english language\n",
        "stanza.download('en',model_dir='/models/english',package='partut')\n",
        "nlp = stanza.Pipeline(lang='en')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo6RB66UsWpm"
      },
      "source": [
        "### Preprocess input caption \n",
        "\n",
        "We noticed that there was a problem with the so called \"not well-formed\" sentences (*In natural language processing, a not well-formed sentence refers to a sentence that does not conform to the rules and conventions of the language, making it difficult or impossible to be accurately processed and understood by a computer program or a human reader*), this was creating the effect of not being able to find the correct root of the sentence. \n",
        "For example one of the cases we encountered was the sentence: \"*boy holding two bears*\" this after we analyzed it with [corenlp](https://corenlp.run/) gave us the following result: \n",
        "![picture](https://drive.google.com/uc?id=1KrY63kV1KwWWsjyyPnIlsOW1rUtU0KXM\n",
        ")\n",
        "\n",
        "This is an example of a \"not well-formed\" sentence, in fact although the root of the sentence is the verb \"holding\", when we apply the NLP parsing we cannot retrieve the noun \"boy\" as the subject of the sentence since it is recognised as an [INTJ](https://universaldependencies.org/u/pos/INTJ.html).\n",
        "\n",
        "In order to solve this problem we noticed that adding a [determiner](https://universaldependencies.org/u/pos/DET.html) at the beginning of the sentence allows us to avoid having such problems. Between all the possible determiners we chose \"*the*\" as it does not change the meaning of the sentence and it is the most general one. \n",
        "\n",
        "After this is applied we obtain this dependecy parsing: \n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1Rt4nU5kXKSn-pq6nRkaCAcyp__m7Z1cd\n",
        ")\n",
        "In which we can see that the word \"boy\" is being parsed as a [noun](https://universaldependencies.org/u/pos/NOUN.html)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jwyGr-fwXi4R"
      },
      "source": [
        "<h3> Stanza sentence preprocess </h3> \n",
        "Before analyzing a sentence, we have a preliminary step in which we exclude certain errors in our subject prediction. One observation we made is that the phrase \"that is\" often introduces positional information about the dataset, which can conflict with our objective. Consequently, we have chosen to remove such phrases. Another issue arises with specific quantifiers, such as \"the part of,\" \"the side of,\" \"the bunch of,\" and \"the piece of.\" The problem here is that the word between \"the\" and \"of\" is mistakenly recognized as the subject of the sentence, resulting in an error. By eliminating these parts of the sentence, along with anything that precedes them, we can avoid this error and correctly identify the intended subject. Consider the sentence, \"The end of a table, with a pink tablecloth, at which eight people are sitting.\" This sentence serves as a perfect example. Without applying pre-processing, the extracted subject would be \"end\" since it is a noun and the root of our sentence. However, our desired subject and root are \"table.\" Therefore, if we remove everything between \"end of,\" we can achieve our objective."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wejQsvGOQowc"
      },
      "outputs": [],
      "source": [
        "def remove_of(sentence):\n",
        " if \"the side of\" in sentence:\n",
        "    index=sentence.find(\"the side of\")\n",
        "    sentence=sentence[index+11:]\n",
        "    return sentence\n",
        " if \"the handle of\" in sentence:\n",
        "    index=sentence.find(\"the handle of\")\n",
        "    sentence=sentence[index+13:]\n",
        " if \"the bunch of\" in sentence:\n",
        "    index=sentence.find(\"the bunch of\")\n",
        "    sentence=sentence[index+12:]\n",
        "    return sentence   \n",
        " if \"the corner of\" in sentence:\n",
        "    index=sentence.find(\"the corner of\")\n",
        "    sentence=sentence[index+13:]\n",
        "    return sentence\n",
        " if \"the end of\" in sentence:\n",
        "    index=sentence.find(\"the end of\")\n",
        "    sentence=sentence[index+10:]\n",
        "    return sentence\n",
        " if \"the half of\" in sentence:\n",
        "    index=sentence.find(\"the half of\")\n",
        "    sentence=sentence[index+11:]\n",
        "    return sentence    \n",
        " if \"the edge of\" in sentence:\n",
        "    index=sentence.find(\"the edge of\")\n",
        "    sentence=sentence[index+11:]    \n",
        "    return sentence\n",
        " if \"the back of\" in sentence:\n",
        "    index=sentence.find(\"the back of\")\n",
        "    sentence=sentence[index+11:]\n",
        "    return sentence   \n",
        " if \"the smaller of\" in sentence:\n",
        "    index=sentence.find(\"the smaller of\")\n",
        "    sentence=sentence[index+14:]\n",
        "    return sentence    \n",
        " if \"the piece of\" in sentence:\n",
        "    index=sentence.find(\"the piece of\")\n",
        "    sentence=sentence[index+16:]\n",
        "    return sentence \n",
        " if \"the wing of\" in sentence:\n",
        "    index=sentence.find(\"the wing of\")\n",
        "    sentence=sentence[index+11:]    \n",
        "\n",
        " if \"the front of\" in sentence:\n",
        "    index=sentence.find(\"the front of\")\n",
        "    sentence=sentence[index+12:]   \n",
        "    return sentence \n",
        " if \"the back side of\" in sentence:\n",
        "    index=sentence.find(\"the back side of\")\n",
        "    sentence=sentence[index+16:]   \n",
        "    return sentence\n",
        " if \"the front side of\" in sentence:\n",
        "    index=sentence.find(\"the front side of\")\n",
        "    sentence=sentence[index+17:]   \n",
        "    return sentence \n",
        " if \"the left side of\" in sentence:\n",
        "    index=sentence.find(\"the left side of\")\n",
        "    sentence=sentence[index+16:]   \n",
        "    return sentence\n",
        " if \"the right side of\" in sentence:\n",
        "    index=sentence.find(\"the back side of\")\n",
        "    sentence=sentence[index+17:]   \n",
        "    return sentence\n",
        "  \n",
        " if \"the pile of\" in sentence:\n",
        "    index=sentence.find(\"the pile of\")\n",
        "    sentence=sentence[index+11:]    \n",
        "    return sentence\n",
        " if \"the pair of\" in sentence:\n",
        "    index=sentence.find(\"the pair of\")\n",
        "    sentence=sentence[index+11:] \n",
        "    return sentence   \n",
        " if \"the pieces of\" in sentence:\n",
        "    index=sentence.find(\"the pieces of\")\n",
        "    sentence=sentence[index+13:]   \n",
        "    return sentence \n",
        " if \"the intersection of\" in sentence:\n",
        "    index=sentence.find(\"the intersection of\")\n",
        "    sentence=sentence[index+19:]  \n",
        "    return sentence  \n",
        " if \"the middle of\" in sentence:\n",
        "    index=sentence.find(\"the middle of\")\n",
        "    sentence=sentence[index+13:]\n",
        "    return sentence    \n",
        " if \"the patch of\" in sentence:\n",
        "    index=sentence.find(\"the patch of\")\n",
        "    sentence=sentence[index+12:]    \n",
        "    return sentence\n",
        " if \"the couple of\" in sentence:\n",
        "    index=sentence.find(\"the couple of\")\n",
        "    sentence=sentence[index+12:]    \n",
        "    return sentence\n",
        " if \"the slice of\" in sentence:\n",
        "    index=sentence.find(\"the slice of\")\n",
        "    sentence=sentence[index+12:]    \n",
        "    return sentence\n",
        " if \"the tallest of\" in sentence:\n",
        "    index=sentence.find(\"the tallest of\")\n",
        "    sentence=sentence[index+14:]    \n",
        "    return sentence\n",
        " if \"the kind of\" in sentence:\n",
        "    index=sentence.find(\"the kind of\")\n",
        "    sentence=sentence[index+11:]\n",
        "    return sentence    \n",
        " if \"that is\" in sentence:\n",
        "    index=sentence.find(\"that is\")\n",
        "    sentence=sentence[:index]\n",
        "    return sentence\n",
        " if \"the part of\" in sentence:\n",
        "    index=sentence.find(\"the part of\")\n",
        "    sentence=sentence[index+11:]\n",
        "    return sentence\n",
        " if \"the corner of\" in sentence:\n",
        "    index=sentence.find(\"the corner of\")\n",
        "    sentence=sentence[index+13:]\n",
        "    return sentence\n",
        " if \"the half of\" in sentence:\n",
        "    index=sentence.find(\"the half of\")\n",
        "    sentence=sentence[index+11:]\n",
        "    return sentence\n",
        " if \"the top of\" in sentence:\n",
        "    index=sentence.find(\"the top of\")\n",
        "    sentence=sentence[index+10:]\n",
        "    return sentence\n",
        " return sentence"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zNsV1sxbeLbv"
      },
      "source": [
        "### Beginning of sentence processing\n",
        "\n",
        "To ensure proper analysis of sentences, it is essential to perform additional preprocessing before removing specific quantifiers. This preprocessing aims to address two specific cases:\n",
        "\n",
        "- The first case pertains to sentences lacking an article at the beginning. This poses a challenge for our parser to correctly identify the subject of the sentence. Thus, it is necessary to rectify this issue by adding an appropriate article.\n",
        "\n",
        "- The second case involves sentences starting with \"there is.\" In order to resolve this problem, we need to eliminate this phrase, as the word \"there\" is recognized as the subject (nsubj). However, it is considered an error in our context, as we initially select an nsubj within the sentence, if one exists.\n",
        "\n",
        "Therefore, prior to removing specific quantifiers, undertaking these preprocessing steps allows us to address these two cases effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bsu_69_Gscfj"
      },
      "outputs": [],
      "source": [
        "'''nlp_sent=input[\"sentences\"][\"raw\"]\n",
        "nlp_sent=nlp_sent.lower()\n",
        "x = nlp_sent.split(\" \")\n",
        "if(x[0]!=\"the\" and x[0]!=\"a\"):\n",
        "    nlp_sent=\"the \"+nlp_sent\n",
        "if nlp_sent.startswith('there is '):\n",
        "    nlp_sent = 'the ' + nlp_sent[9:]\n",
        "nlp_sent=remove_of(nlp_sent)\n",
        "\n",
        "print(nlp_sent)'''"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NucfKCZ2siTF"
      },
      "source": [
        "<h3> Analyze caption and compute features of its root </h3>\n",
        "This is the core function of our sentence processing. The process develops as follows: \n",
        "- we lowercase the sentence, this must be done in order to achieve better performances in the NLP part.\n",
        "- We apply the beginning of sentence processing as seen above\n",
        "- Then we actually decide how to process our sentence in order to extract the subject: \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kawuB8twspgm"
      },
      "outputs": [],
      "source": [
        "def sent_stanza_processing(sentence):\n",
        "\n",
        "    sentence = sentence.lower()\n",
        "    if sentence.startswith('there is '):\n",
        "        sentence = 'the ' + sentence[9:]\n",
        "    if sentence.startswith('this is '):\n",
        "        sentence = sentence[9:]\n",
        "    sentence = remove_of(sentence)\n",
        "\n",
        "    nlp_sent = sentence.lower()\n",
        "\n",
        "    # put the sentence in lower case\n",
        "    # if the sentence does not start with \"a\" or \"the\" insert it\n",
        "    x = nlp_sent.split(\" \")\n",
        "    if (x[0] != \"the\" and x[0] != \"a\"):\n",
        "        nlp_sent = \"the \" + nlp_sent\n",
        "\n",
        "    doc = nlp(nlp_sent)\n",
        "    # print nlp dependencies\n",
        "    # doc.sentences[0].print_dependencies()\n",
        "    # print(input[\"sentences\"][\"raw\"])\n",
        "    root = ''\n",
        "    phrase_upos = []\n",
        "    # get heads of words\n",
        "    heads = [sent.words[word.head -\n",
        "                        1].text for sent in doc.sentences for word in sent.words]\n",
        "    for sent in doc.sentences:\n",
        "        for word in sent.words:\n",
        "            # if it is a verbal phrase then take the nominal subject of the phrase\n",
        "            if (word.deprel == 'nsubj' or word.deprel == 'nsubj:pass'):\n",
        "                root = word.text\n",
        "                return word.text\n",
        "                # print(word.text)\n",
        "                break\n",
        "            # print(word)\n",
        "            phrase_upos.append(word)\n",
        "            # else take the root of the phrase\n",
        "            if (word.head == 0):\n",
        "                # print(word.text)\n",
        "                return word.text\n",
        "                # root=word.text\n",
        "                # if the root is a verb\n",
        "                if (word.upos == 'VERB'):\n",
        "                    for w in reversed(phrase_upos):\n",
        "                        # go back until you get a noun\n",
        "                        if (w.upos == 'NN'):\n",
        "                            return word.text\n",
        "                            # print(w.text)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_gy2H__xsqpP"
      },
      "source": [
        "## Analyze YOLO class prediction\n",
        "\n",
        "We then encode also the classes that are present inside our YOLO predictions. These encodings will be used to compute the similarity between the root of our sentence and the YOLO classes in order to bring us back to one of the original YOLO classes. Having the subject of our sentence expressed as one of the YOLO classes we can exclude all the bounding boxes that have a label that is different from the one of our subject."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgC-vmuWsyzu"
      },
      "outputs": [],
      "source": [
        "def get_root(yolo_output, sentence, model, yolo):\n",
        "    root = sent_stanza_processing(sentence)\n",
        "    # print(root)\n",
        "    prompt_tokens = clip.tokenize(\n",
        "        root, context_length=77, truncate=True).cuda()\n",
        "    with torch.no_grad():\n",
        "        prompt_features = model.encode_text(prompt_tokens).float()\n",
        "\n",
        "    names = []\n",
        "    for a in range(len(yolo_output.xyxy[0])):\n",
        "        class_index = int(yolo_output.pred[0][a][5])\n",
        "        label = yolo.names[class_index]\n",
        "        names.append(label)\n",
        "    tokens = clip.tokenize(names, context_length=77, truncate=True).cuda()\n",
        "    with torch.no_grad():\n",
        "        classes_features = model.encode_text(tokens).float()\n",
        "    prompt_features /= prompt_features.norm(dim=-1, keepdim=True)\n",
        "    classes_features /= classes_features.norm(dim=-1, keepdim=True)\n",
        "    prompt_similarity = classes_features.cpu().numpy() @ prompt_features.cpu().numpy().T\n",
        "    if prompt_similarity.shape[0] == 0:\n",
        "        return \"empty\"\n",
        "    rappresentation = np.argmax(prompt_similarity)\n",
        "\n",
        "    interested_class = names[rappresentation]\n",
        "    return interested_class"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8m31oRwTtBOb"
      },
      "source": [
        "### Take class with maximum similarity \n",
        "\n",
        "We only want to have the class with the best similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvWljUggtG2G"
      },
      "outputs": [],
      "source": [
        "# take as desired class the one with the highest similarity\n",
        "rappresentation=np.argmax(prompt_similarity)\n",
        "print(root)\n",
        "interested_class=names[rappresentation]\n",
        "\n",
        "print(interested_class)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uL51YqjIQtxf"
      },
      "source": [
        "We achieved an accuracy of 87% in predicting the subject of the sentence using Stanza. However, this result needs further explanation as it includes a 13% error rate, which can be categorized into three types of errors:\n",
        "\n",
        "- The first type of error stems from issues in the sentence itself.  These errors can occur in two cases: when the sentence contains grammatical errors such as \"The class of water in front of the bowl of bread\" or when the sentence is not well-formed such as \"Pot boiling water with green bell peppers in man's kitchen\" that is missing the \"of\" between \"Pot\" and \"boiling\". NLP parsers are primarily designed for well-formed written languages, so errors may arise when working with other types of languages. One possible solution to address this problem is to introduce a machine learning model that can correct these errors and transform the original sentence into a well-formed one without altering its intended meaning.\n",
        "\n",
        "- The second type of error is intrinsic to the dataset. In this case, the error arises from incorrect labeling of the subject class in the dataset, while our NLP analysis provides us with the correct subject. These errors can be considered as \"false negatives,\" as they do not significantly impact the final outcome of the pipeline. (esempio con l'immagine di teddybear)\n",
        "\n",
        "- The third type of error is related to our own process. This error occurs when we are unable to correctly map the predicted subject to one of the classes in YOLO. This issue arises due to the utilization of the CLIP encoder, which has a fixed context length of 77 tokens, during the process of embedding the subject of the sentence and the YOLO classes. The results we obtain are highly influenced by this context.\n",
        "To illustrate this, let's consider an example where the word \"lemon\" is the subject in our context, and we intend to associate it with the \"orange\" class. However, if the \"banana\" class is selected as the most similar, it is because the word \"banana\" appears more frequently in the analyzed context near the word \"lemon\" than the word \"orange\" does.\n",
        "The embedding process relies heavily on the surrounding context, and the choice of the most similar class depends on the co-occurrence patterns observed within that context. Therefore, variations in the embedding results can occur based on the specific context utilized by the CLIP encoder."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baseline + stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# class that defines the baseline model\n",
        "\n",
        "class VisualGrounding_stanza(torch.nn.Module):\n",
        "    def __init__(self, yolo_version, clip_version, local_path, img_path):\n",
        "        super(VisualGrounding_stanza, self).__init__()\n",
        "        self.local_path = local_path\n",
        "        self.img_path = img_path\n",
        "        # initialize models\n",
        "        self.yolo = torch.hub.load(\n",
        "            'ultralytics/yolov5', yolo_version, pretrained=True)\n",
        "        self.clip, self.preprocess = clip.load(clip_version)\n",
        "        self.name = \"stanza\"\n",
        "        # define metrics\n",
        "        self.metrics = Metrics(self.clip, self.name)\n",
        "\n",
        "        \n",
        "\n",
        "    def forward(self, img_path, sentence):\n",
        "        max_similarity = 0\n",
        "        max_image = None\n",
        "        max_bbox = None\n",
        "\n",
        "        yolo_output = self.yolo(self.local_path+img_path)\n",
        "\n",
        "        original_img = Image.open(self.local_path+img_path).convert(\"RGB\")\n",
        "\n",
        "        root = get_root(yolo_output, sentence, self.clip, self.yolo)\n",
        "\n",
        "        for i in range(len(yolo_output.xyxy[0])):\n",
        "            if root != \"empty\" and self.yolo.names[int(yolo_output.pred[0][i][5])] != root:\n",
        "                continue\n",
        "            #crop the image based on the yolo output\n",
        "            img_cropped = crop_yolo(yolo_output, original_img, i)\n",
        "\n",
        "            img = self.preprocess(img_cropped).cuda().unsqueeze(0)\n",
        "            text = clip.tokenize([sentence]).cuda()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                image_features = self.clip.encode_image(img).float()\n",
        "                text_features = self.clip.encode_text(text).float()\n",
        "\n",
        "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "            similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
        "\n",
        "            if similarity > max_similarity:\n",
        "                max_similarity = similarity\n",
        "                max_image = img_cropped\n",
        "                max_bbox = yolo_output.xyxy[0][i]\n",
        "\n",
        "        if max_image is None:\n",
        "            #set bbox to the whole image\n",
        "            max_bbox = [0, 0, original_img.width, original_img.height]\n",
        "            max_image = original_img\n",
        "\n",
        "        return max_bbox, max_image\n",
        "\n",
        "    def evaluate(self, img_path, sentence, gt, original_img):\n",
        "        bbox = convert_bbox(gt, original_img)\n",
        "        gt_crop = original_img.crop(bbox)\n",
        "        prediction_bbox, prediction_img = self.forward(img_path, sentence)\n",
        "        self.metrics.update(prediction_bbox, bbox, prediction_img, gt_crop)\n",
        "        return prediction_bbox, prediction_img\n",
        "\n",
        "    def reset_metrics(self):\n",
        "        self.metrics.reset()\n",
        "\n",
        "    def get_metrics(self):\n",
        "        return self.metrics.to_string()\n",
        "    \n",
        "    def save_metrics(self):\n",
        "        self.metrics.save()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-0A7jtrntNSp"
      },
      "source": [
        "# Stable Diffusion"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VbYtIcsyZnBP"
      },
      "source": [
        "<h3> Why stable diffusion? </h3>\n",
        "\n",
        "Before delving into the concept of Stable Diffusione, it is important to explain why we decided to introduce a generative model. Our initial focus was on analyzing the issues present in CLIP, particularly the problem of Polysemy.\n",
        "\n",
        "Polysemy can be described as the phenomenon wherein the model struggles to distinguish between the various meanings of certain words due to a lack of context. As mentioned earlier, some images in the dataset are only labeled with a class tag, without a complete textual prompt. The authors provide an example from the Oxford-IIIT Pet dataset, where the term 'boxer' can refer to either a dog breed or a type of athlete. In this case, the issue lies with the quality of the data rather than the model itself.\n",
        "\n",
        "Having identified this problem, our next step was to find a solution. Introducing a generative model was our attempt to address the challenges of Polysemy. We decided to utilize the captions provided in the dataset as prompts for generating synthetic images. These images serve two purposes in our predictions.\n",
        "\n",
        "Firstly, since they are generated based on the same description as the target image, they tend to exhibit greater similarity to the desired goal compared to other bounding boxes being analyzed. Additionally, during the image generation process, we incorporate additional information regarding the subject we aim to recognize. When these images are encoded, this extra information enhances the similarity between the generated images and the desired target image.\n",
        "\n",
        "(manca esempio polisemia e agguinta di informazioni)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iBP_vuhbtRb5"
      },
      "source": [
        "### Import stable diffusion model and create pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXC8_N7otW6r"
      },
      "outputs": [],
      "source": [
        "# Load stable diffusion mode\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16)  \n",
        "pipe = pipe.to(\"cuda\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2UmSqaU6tekv"
      },
      "source": [
        "### Generate images \n",
        "\n",
        "Since we know that Stable Diffusion has a component of randomness in the image generation we deciced to take into consideration creating more than one image. We decided to create more than one image in order to reduce the weight of a possible \"outlier\" in the images.\n",
        "\n",
        "We then "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivmsucXkteFA"
      },
      "outputs": [],
      "source": [
        "# insert the prompt the caption we are provided by the dataset\n",
        "prompt= 'Use deep learning algorithms to generate a hyper-realistic portrait of a'+   input[\"sentences\"][\"raw\"] +' Use advanced image processing techniques to make the image appear as if it were a photograph'\n",
        "\n",
        "# create the images with stable diffusion\n",
        "stable_input = pipe(prompt,num_inference_steps=50).images[0]  # image here is in [PIL format](https://pillow.readthedocs.io/en/stable/)\n",
        "stable_input2 = pipe(prompt,num_inference_steps=50).images[0]  # image here is in [PIL format](https://pillow.readthedocs.io/en/stable/)\n",
        "stable_input3 = pipe(prompt,num_inference_steps=50).images[0]  # image here is in [PIL format](https://pillow.readthedocs.io/en/stable/)\n",
        "\n",
        "stable_input"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GwkGUai1z02X"
      },
      "source": [
        "Examples of stable diffusion generated images \n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1N0pMFQD_htgEnoN3sIQEN4xhFXRALpm_ \n",
        " )![picture](https://drive.google.com/uc?id=1WRGaSHdUlKx8oYe3Gh5SwALpudP6Si3D\n",
        ")![picture](https://drive.google.com/uc?id=1dZlmX1oMfNp41HEeqqrTd7SdPrs2eJzW\n",
        ")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Nc7aFnTotopi"
      },
      "source": [
        "### Create encoding for images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MX5y34pRttVA"
      },
      "outputs": [],
      "source": [
        "\n",
        "with torch.no_grad():\n",
        "    # transform image into tensor\n",
        "    convert_tensor = transforms.ToTensor()\n",
        "    # resize image for clip\n",
        "    stable_input=stable_input.resize([224,224])   \n",
        "    stable_input2=stable_input2.resize([224,224])   \n",
        "    stable_input3=stable_input3.resize([224,224])   \n",
        "\n",
        "    # create tensors\n",
        "    image_stable=torch.tensor(np.stack(convert_tensor(stable_input))).cuda()   \n",
        "    image_stable2=torch.tensor(np.stack(convert_tensor(stable_input2))).cuda()   \n",
        "    image_stable3=torch.tensor(np.stack(convert_tensor(stable_input3))).cuda()   \n",
        "    \n",
        "    # stack images into a tensor\n",
        "    img_tens = torch.stack([image_stable,image_stable2,image_stable3])\n",
        "    \n",
        "    print(image_input.size())\n",
        "    print(img_tens.size())\n",
        "    \n",
        "    # encode stable diffusion images\n",
        "    text_features= model.encode_image(img_tens).float()\n",
        "    \n",
        "    # encode YOLO bounding boxes\n",
        "    image_features = model.encode_image(image_input).float()\n",
        "    \n",
        "    print(text_features.size())\n",
        "    print(image_features.size())\n",
        "\n",
        "    #stable_input=stable_input.resize([224,224])   \n",
        "    \n",
        "\n",
        "\n",
        "    #text_features = model.encode_text(text_tokens).float()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UdRAsmxEu3Jh"
      },
      "source": [
        "### Compute Cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBfblfN3u3t_"
      },
      "outputs": [],
      "source": [
        "# compute cosine similarity\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-9J8OwbvCQG"
      },
      "outputs": [],
      "source": [
        "# compute mean of similarities\n",
        "similarity_vec=np.mean(similarity,axis=0)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gh5M3Ewe3DAd"
      },
      "source": [
        "![picture](https://drive.google.com/uc?id=11E-447bBm4y4B35ZzqurtT__b73a3knj)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fUk4X77ovEqR"
      },
      "source": [
        "### Take only the best bounding box\n",
        "\n",
        "Once we have computed the similarity values, and obtained the mean vecotor of the similarities we want to filter out our results in order to make our prediction. The first thing we do is to filter out all the values in the similarity vector by checking if the corresponding bounding boxes have the class as the one we obtained before.\n",
        "Than inside of the \"filtered\" vector we take the maximum value that will correspond to our prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPG_fgQZvJLz"
      },
      "outputs": [],
      "source": [
        "possible_values=[]\n",
        "indexes=[]\n",
        "for i in range(len(yolo_output.xyxy[0])):\n",
        "    class_index=int(yolo_output.pred[0][i][5])\n",
        "    label=class_names[class_index]\n",
        "    # take only the YOLO predictions that have as class the one we computed in the NLP analysis\n",
        "    if(label==interested_class):\n",
        "      possible_values.append(similarity_vec[i])\n",
        "      indexes.append(i)\n",
        "# take only the best class      \n",
        "print(max(possible_values))\n",
        "index=possible_values.index(max(possible_values))\n",
        "index=indexes[index]\n",
        "print(nlp_sent)\n",
        "\n",
        "original_images[index]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "clDnajuy37IV"
      },
      "source": [
        "![picture](https://drive.google.com/uc?id=1ZSey_FP4GwBCwVhaFO1-XGX3ejMm1Hv1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_stable_images(index,clip_model, device):\n",
        "  stable_input1=Image.open(\"stable_diffusion/stable_diffusion_\"+str(index)+\"_1.jpg\")\n",
        "  stable_input2=Image.open(\"stable_diffusion/stable_diffusion_\"+str(index)+\"_2.jpg\")\n",
        "  stable_input3=Image.open(\"stable_diffusion/stable_diffusion_\"+str(index)+\"_3.jpg\")\n",
        "  \n",
        "  \n",
        "  \n",
        "  with torch.no_grad():\n",
        "    convert_tensor = transforms.ToTensor()\n",
        "\n",
        "    stable_input1=stable_input1.resize([224,224])   \n",
        "    stable_input2=stable_input2.resize([224,224])   \n",
        "    stable_input3=stable_input3.resize([224,224]) \n",
        "\n",
        "    image_stable1=torch.tensor(np.stack(convert_tensor(stable_input1))).to(device)   \n",
        "    image_stable2=torch.tensor(np.stack(convert_tensor(stable_input2))).to(device) \n",
        "    image_stable3=torch.tensor(np.stack(convert_tensor(stable_input3))).to(device) \n",
        "    \n",
        "    img_tens = torch.stack([image_stable1, image_stable2, image_stable3])\n",
        "    \n",
        "    stable_features= clip_model.encode_image(img_tens).float()\n",
        "\n",
        "  return stable_features\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stable Diffusion model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# class that defines the baseline model\n",
        "class VisualGrounding_stable_diffusion(torch.nn.Module):\n",
        "    def __init__(self, yolo_version, clip_version, local_path, img_path):\n",
        "        super(VisualGrounding_stable_diffusion, self).__init__()\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        self.local_path = local_path\n",
        "        self.img_path = img_path\n",
        "        # initialize models\n",
        "        self.yolo = torch.hub.load('ultralytics/yolov5', yolo_version, pretrained=True)\n",
        "        self.clip, self.preprocess = clip.load(clip_version)\n",
        "\n",
        "        self.name = \"stable_diffusion\"\n",
        "        self.metrics = Metrics(self.clip, self.name)\n",
        "\n",
        "\n",
        "    def forward(self, img_path, sentence,index):\n",
        "        similarities = []\n",
        "        bboxes = []\n",
        "        max_similarity = 0\n",
        "        max_image = None\n",
        "        max_bbox = None\n",
        "\n",
        "        yolo_output = self.yolo(self.local_path+img_path)\n",
        "\n",
        "        original_img = Image.open(self.local_path+img_path).convert(\"RGB\")\n",
        "\n",
        "        root = get_root(yolo_output, sentence, self.clip, self.yolo)\n",
        "        \n",
        "        stable_features=process_stable_images(index, self.clip, self.device)\n",
        "\n",
        "        no_bbox=True\n",
        "\n",
        "        for i in range(len(yolo_output.xyxy[0])):\n",
        "            if root != \"empty\" and self.yolo.names[int(yolo_output.pred[0][i][5])] != root:\n",
        "                continue\n",
        "            \n",
        "            no_bbox=False\n",
        "            img_cropped = crop_yolo(yolo_output, original_img, i)\n",
        "\n",
        "            #plt.imshow(img_cropped)\n",
        "            #plt.show()\n",
        "            img_cropped = padd_image(img_cropped)\n",
        "            img = self.preprocess(img_cropped).cuda().unsqueeze(0)\n",
        "            #text = clip.tokenize([sentence]).cuda()\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                image_features = self.clip.encode_image(img).float()\n",
        "\n",
        "                image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "                similarity = stable_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
        "                similarity = similarity.reshape(3, 1)\n",
        "\n",
        "                similarities.append(torch.tensor(similarity))\n",
        "                bboxes.append((yolo_output.xyxy[0][i][0].cpu().numpy(), yolo_output.xyxy[0][i][1].cpu().numpy(), yolo_output.xyxy[0][i][2].cpu().numpy(), yolo_output.xyxy[0][i][3].cpu().numpy()))\n",
        "\n",
        "        if no_bbox:\n",
        "            similarities.append(torch.zeros(3,1))\n",
        "            max_bbox = [0, 0, original_img.width, original_img.height]\n",
        "            max_bbox = torch.tensor(max_bbox)\n",
        "            max_image = original_img\n",
        "            return max_bbox, max_image\n",
        "            \n",
        "        stacked_similarity = torch.cat(similarities, dim=1)\n",
        "        max_indices = torch.argmax(stacked_similarity, dim=1)\n",
        "        max_count = torch.bincount(max_indices)\n",
        "    \n",
        "        #parity case\n",
        "        if torch.max(max_count) == 1:\n",
        "          column_means = torch.mean(stacked_similarity, dim=0)\n",
        "          best_bbox = torch.argmax(column_means)\n",
        "        else:\n",
        "          best_bbox = torch.argmax(max_count)\n",
        "\n",
        "        \n",
        "        max_bbox_new = yolo_output.xyxy[0][best_bbox] \n",
        "        max_image_new = img_cropped = crop_yolo(yolo_output, original_img, best_bbox)\n",
        "\n",
        "        return max_bbox_new, max_image_new\n",
        "\n",
        "\n",
        "    def evaluate(self, img_path, sentence, gt, original_img, index):\n",
        "        bbox = convert_bbox(gt, original_img)\n",
        "        gt_crop = original_img.crop(bbox)\n",
        "        prediction_bbox, prediction_img = self.forward(img_path, sentence,index)\n",
        "        self.metrics.update(prediction_bbox, bbox, prediction_img, gt_crop)\n",
        "        return prediction_bbox, prediction_img\n",
        "\n",
        "    def reset_metrics(self):\n",
        "        self.metrics.reset()\n",
        "\n",
        "    def get_metrics(self):\n",
        "        return self.metrics.to_string()\n",
        "    \n",
        "    def save_metrics(self):\n",
        "        self.metrics.save()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KpHq3rSDvTCY"
      },
      "source": [
        "# Image Captioning "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# class that defines the baseline model\n",
        "\n",
        "class VisualGrounding_ttt(torch.nn.Module):\n",
        "    def __init__(self, yolo_version, clip_version, local_path, img_path):\n",
        "        super(VisualGrounding_ttt, self).__init__()\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        self.local_path = local_path\n",
        "        self.img_path = img_path\n",
        "\n",
        "        # initialize models\n",
        "        self.yolo = torch.hub.load('ultralytics/yolov5', yolo_version, pretrained=True).to(self.device)\n",
        "        self.clip, self.preprocess = clip.load(clip_version)\n",
        "\n",
        "        # text to text section\n",
        "        self.text_tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', max_length=20)\n",
        "        self.text_feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "        self.text_model = VisionEncoderDecoderModel.from_pretrained('/home/pappol/Scrivania/deepLearning/Image_Captioning_VIT_Roberta_final_4')\n",
        "        self.text_model.to(self.device)\n",
        "        self.name = \"text to image\"\n",
        "        # define metrics\n",
        "        self.metrics = Metrics(self.clip, self.name)\n",
        "\n",
        "        \n",
        "\n",
        "    def forward(self, img_path, sentence):\n",
        "        similarity = torch.tensor([]).to(self.device)\n",
        "\n",
        "        yolo_output = self.yolo(self.local_path+img_path)\n",
        "        original_img = Image.open(self.local_path+img_path).convert(\"RGB\")\n",
        "\n",
        "        root = get_root(yolo_output, sentence, self.clip, self.yolo)\n",
        "\n",
        "        sentence_tokens = clip.tokenize([sentence]).to(device=self.device)\n",
        "        embedding_sent = self.clip.encode_text(sentence_tokens).to(self.device)\n",
        "\n",
        "        no_bbox=True\n",
        "\n",
        "        for i in range(len(yolo_output.xyxy[0])):\n",
        "            if root != \"empty\" and self.yolo.names[int(yolo_output.pred[0][i][5])] != root:\n",
        "                continue\n",
        "\n",
        "            with torch.no_grad():\n",
        "                no_bbox=False\n",
        "                #crop the image based on the yolo output\n",
        "                img_cropped = crop_yolo(yolo_output, original_img, i)\n",
        "                #generate caption\n",
        "                features = self.text_feature_extractor(img_cropped, return_tensors=\"pt\").pixel_values.to(self.device)\n",
        "                generated = self.text_model.generate(features)[0].to(self.device)\n",
        "                caption = self.text_tokenizer.decode(generated)\n",
        "                #caption = self.text_tokenizer.decode(self.text_model.generate(self.text_feature_extractor(img_cropped, return_tensors=\"pt\").pixel_values.to(self.device))[0].to(self.device))\n",
        "                caption = clear_caption(caption)\n",
        "                caption = clip.tokenize([caption]).to(device=self.device)\n",
        "                enbedding_gen = self.clip.encode_text(caption).to(self.device)\n",
        "\n",
        "                #cosine similarity bwteen caption and sentence\n",
        "                similarity = torch.cat((similarity, torch.nn.functional.cosine_similarity(enbedding_gen, embedding_sent)), 0)\n",
        "            \n",
        "        if no_bbox:\n",
        "            #set bbox to the whole image\n",
        "            max_bbox = [0, 0, original_img.width, original_img.height]\n",
        "            max_image = original_img\n",
        "\n",
        "        #argmax to get the most similar caption\n",
        "        index = torch.argmax(similarity)\n",
        "        max_bbox = yolo_output.xyxy[0][index]\n",
        "        max_image = crop_yolo(yolo_output, original_img, index)\n",
        "\n",
        "        return max_bbox, max_image\n",
        "\n",
        "    def evaluate(self, img_path, sentence, gt, original_img):\n",
        "        bbox = convert_bbox(gt, original_img)\n",
        "        gt_crop = original_img.crop(bbox)\n",
        "        prediction_bbox, prediction_img = self.forward(img_path, sentence)\n",
        "        self.metrics.update(prediction_bbox, bbox, prediction_img, gt_crop)\n",
        "        return prediction_bbox, prediction_img\n",
        "\n",
        "    def reset_metrics(self):\n",
        "        self.metrics.reset()\n",
        "\n",
        "    def get_metrics(self):\n",
        "        return self.metrics.to_string()\n",
        "    \n",
        "    def save_metrics(self):\n",
        "        self.metrics.save()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SR9MIHIkvcRT"
      },
      "source": [
        "## Fine tuning image captioning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAIN_BATCH_SIZE = 16  # input batch size for training (default: 64)\n",
        "VALID_BATCH_SIZE = 6   # input batch size for testing (default: 1000)\n",
        "\n",
        "TRAIN_EPOCHS = 45       # number of epochs to train (default: 10)\n",
        "VAL_EPOCHS = 1 \n",
        "\n",
        "LEARNING_RATE = 1e-4   # learning rate (default: 0.01)\n",
        "SEED = 42              # random seed (default: 42)\n",
        "MAX_LEN = 128          # Max length for product description\n",
        "SUMMARY_LEN = 20       # Max length for product names\n",
        "WEIGHT_DECAY = 0.01    # Weight decay (default: 1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class IAMDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer,feature_extractor, decoder_max_length = 20):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.decoder_max_length = decoder_max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # get file name + text \n",
        "        img_path = self.df['images'][idx]\n",
        "        caption = self.df['captions'][idx]\n",
        "        # prepare image (i.e. resize + normalize)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        pixel_values = self.feature_extractor(image, return_tensors=\"pt\").pixel_values\n",
        "        \n",
        "        # add labels (input_ids) by encoding the text\n",
        "        labels = self.tokenizer(caption, truncation = True,\n",
        "                                          padding=\"max_length\", \n",
        "                                          max_length=self.decoder_max_length).input_ids\n",
        "        \n",
        "        # important: make sure that PAD tokens are ignored by the loss function\n",
        "        labels = [label if label != self.tokenizer.pad_token_id else -100 for label in labels]\n",
        "\n",
        "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
        "        return encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    rouge = datasets.load_metric(\"rouge\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "    labels_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions\n",
        "\n",
        "    # all unnecessary tokens are removed\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
        "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "\n",
        "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
        "\n",
        "    return {\n",
        "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
        "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
        "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "path = \"Image_Captioning_VIT_Roberta_final_3\"\n",
        "\n",
        "print(\"Loading data\")\n",
        "df = pd.read_csv(\"RefCOCOg_cropped.csv\")\n",
        "df['cropped'] = df['cropped'].str.replace('refcocog/', '')\n",
        "df = df.rename(columns={'cropped': 'images', 'raw': 'captions'})\n",
        "df['captions'] = df['captions'].str.lower()\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', max_len=MAX_LEN)\n",
        "\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "\n",
        "batch_size=TRAIN_BATCH_SIZE\n",
        "\n",
        "train_dataset = IAMDataset(df=train_df.sample(frac=1,random_state=2).iloc[:].reset_index().drop('index',axis =1),\n",
        "                        tokenizer=tokenizer,\n",
        "                        feature_extractor= feature_extractor)\n",
        "\n",
        "test_dataset = IAMDataset(df=test_df.sample(frac=1,random_state=2)[:].reset_index().drop('index',axis =1),\n",
        "                        tokenizer=tokenizer,feature_extractor= feature_extractor)\n",
        "\n",
        "# set encoder decoder tying to True\n",
        "\n",
        "model = VisionEncoderDecoderModel.from_pretrained(path)\n",
        "# set special tokens used for creating the decoder_input_ids from the labels\n",
        "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# make sure vocab size is set correctly\n",
        "model.config.vocab_size = model.config.decoder.vocab_size\n",
        "\n",
        "# set beam search parameters\n",
        "model.config.eos_token_id = tokenizer.sep_token_id\n",
        "model.config.max_length = 20\n",
        "model.config.early_stopping = True\n",
        "model.config.no_repeat_ngram_size = 3\n",
        "model.config.length_penalty = 2.0\n",
        "model.config.num_beams = 4\n",
        "\n",
        "# load rouge for validation\n",
        "rouge = datasets.load_metric(\"rouge\")\n",
        "\n",
        "captioning_model = 'VIT_Captioning'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=captioning_model,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    predict_with_generate=True,\n",
        "    #evaluate_during_training=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    logging_steps=1024,  \n",
        "    save_steps=2048, \n",
        "    warmup_steps=1024,  \n",
        "    num_train_epochs = TRAIN_EPOCHS, #TRAIN_EPOCHS\n",
        "    overwrite_output_dir=True,\n",
        "        save_strategy=\"epoch\",\n",
        ")\n",
        "\n",
        "    # instantiate trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    tokenizer=feature_extractor,\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    data_collator=default_data_collator,\n",
        "    #save strategy\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.save_model('Image_Captioning_VIT_Roberta_final_4')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model evaluations and comparisons"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dataset load\n",
        "dataset = Coco(local_annotations + 'instances.json', local_annotations + \"refs(umd).p\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model load\n",
        "baseline = VisualGrounding_baseline('yolov5x', 'ViT-B/32', local_path, local_annotations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "validate(baseline, dataset.get_test())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_on_one_image(baseline, dataset.get_test(), 455)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### baseline + stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stanza_baseline = VisualGrounding_stanza('yolov5x', 'ViT-B/32', local_path, local_annotations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "validate(stanza_baseline, dataset.get_test())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_on_one_image(stanza_baseline, dataset.get_test(), 455)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Image captioning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_to_text = VisualGrounding_ttt('yolov5x', 'ViT-B/32', local_path, local_annotations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "validate(text_to_text, dataset.get_test())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_on_one_image(text_to_text, dataset.get_test(), 455)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Stable diffusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stable_model = VisualGrounding_stable_diffusion('yolov5x', 'ViT-B/32', local_path, local_annotations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "validate(stable_model, dataset.get_test())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_on_one_image(stable_model, dataset.get_test(), 455)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "S7Y5GTTifub-"
      },
      "source": [
        "# Bias on results "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Considerazioni sui data ( aka come pararsi il culo)\n",
        "\n",
        "When evaluating the results we obtained, it is crucial to consider the presence of bias, which stems from various sources. These biases can impact the accuracy of our outcomes. Let's discuss these factors in detail:\n",
        "\n",
        "- Bias from positional information in the dataset: When captions in the dataset primarily consist of positional information, it becomes challenging to differentiate between possible targets. In such cases, the identical subject may appear in different positions, leading to a random selection by our model. Consequently, this randomness may result in inaccurate predictions.\n",
        "\n",
        "- Bias from the intrinsic randomness of generated images: Our results heavily depend on the generated images, as they are employed to discriminate between bounding boxes. The quality of these generated images can introduce two types of \"errors\": low-quality images or images that are not suitable for our specific purposes. Both scenarios can affect the final predictions.\n",
        "\n",
        "- Propagation of errors from the Stanza NLP process: As mentioned earlier, although some errors generated during the Stanza NLP process do not impact subsequent stages, there is still a possibility of propagating errors, which can influence the final results.\n",
        "\n",
        "- Influence of YOLO in bounding box selection: Since we utilize YOLO as our method for obtaining bounding boxes, our results are constrained by the boxes generated by this algorithm. This constraint can affect the metrics, as we may correctly predict the subject but have a different bounding box compared to the ground truth due to variations in YOLO's prediction method.\n",
        "\n",
        "\n",
        "- ERRORE SU IMAGE CAPTIONER  LUCA PORCODDIO FALLO  "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNNomIUQnc56Wdb/POKHInF",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
