{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZ2R8kLGsqREFp3Tmj6zSs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolazilio0/deepRiccy/blob/main/Complete_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Load Dataset from local path or Google Drive </h1>"
      ],
      "metadata": {
        "id": "L1L6n8lApZ8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Google Drive </h3>"
      ],
      "metadata": {
        "id": "geaZDt4mprFs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtYDushgpY2q"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "! tar -zxvf /content/drive/MyDrive/refcocog.tar.gz\n",
        "! pip3 install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt \n",
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "local_path = '/content/refcocog/images/'\n",
        "local_annotations = '/content/refcocog/annotations/'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Local path </h3>"
      ],
      "metadata": {
        "id": "HZIZlo-WpwH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "local_path = './refcocog/images/' \n",
        "local_annotations = './refcocog/annotations/' "
      ],
      "metadata": {
        "id": "0VssKwdfpn41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Installations and imports </h1>"
      ],
      "metadata": {
        "id": "Ng2PBifFp2YR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Install dependencies and modules </h3>"
      ],
      "metadata": {
        "id": "CmTH0bGzqGSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install ftfy regex tqdm\n",
        "! pip3 install diffusers==0.11.1\n",
        "! pip3 install transformers scipy ftfy accelerate\n",
        "! pip3 install stanza\n"
      ],
      "metadata": {
        "id": "4A19WwIMqFAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pickle\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from pkg_resources import packaging\n",
        "import clip\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import skimage\n",
        "import IPython.display\n",
        "\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "\n",
        "import os\n",
        "import skimage\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from diffusers import DPMSolverMultistepScheduler\n",
        "import stanza\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "by2_wgHIp8CD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Dataset Process </h1>"
      ],
      "metadata": {
        "id": "_EbkjMszrDDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Coco(Dataset):\n",
        "    def __init__(self, path_json, path_pickle, train=True):\n",
        "        self.path_json = path_json\n",
        "        self.path_pickle = path_pickle\n",
        "        self.train = train\n",
        "\n",
        "        #load images and annotations\n",
        "        with open(self.path_json) as json_data:\n",
        "            data = json.load(json_data)\n",
        "            self.img_frame = pd.DataFrame(data['images'])\n",
        "            self.ann_frame = pd.DataFrame(data['annotations'])\n",
        "\n",
        "        #load annotations\n",
        "        with open(self.path_pickle, 'rb') as pickle_data:\n",
        "            data = pickle.load(pickle_data)\n",
        "            self.refs_frame = pd.DataFrame(data)\n",
        "\n",
        "        self.size = self.refs_frame.shape[0]\n",
        "\n",
        "        #separate each sentence in dataframe\n",
        "        self.refs_frame = self.refs_frame.explode('sent_ids')\n",
        "        self.refs_frame = self.refs_frame.explode('sentences')\n",
        "        self.refs_frame = self.refs_frame.reset_index(drop=True)\n",
        "\n",
        "        self.dataset = pd.merge(self.refs_frame, self.img_frame, left_on='ann_id', right_on='id')\n",
        "        self.dataset = self.dataset.drop(columns=['ann_id'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.dataset.iloc[idx]"
      ],
      "metadata": {
        "id": "yMOzIbl4rHL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_string(string):\n",
        "    string = string.split(\"_\")\n",
        "    string = string[:-1]\n",
        "    string = \"_\".join(string)\n",
        "    append = \".jpg\"\n",
        "    string = string + append\n",
        "    \n",
        "    return string"
      ],
      "metadata": {
        "id": "Bfi61CKJrIGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test dataset\n",
        "\n",
        "dataset = Coco(local_annotations + 'instances.json', local_annotations + \"refs(umd).p\")\n",
        "print(dataset[0])"
      ],
      "metadata": {
        "id": "5PbA4ZfVrLTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test clip\n",
        "clip.available_models()"
      ],
      "metadata": {
        "id": "020VQy0vrOJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Extract bounding boxes with YOLO </h1>"
      ],
      "metadata": {
        "id": "YC2G0s3ZrQfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! rm bb*\n",
        "#define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#pass image into yolo\n",
        "yolo = torch.hub.load('ultralytics/yolov5', 'yolov5x', pretrained=True)\n",
        "\n",
        "#input = dataset[55]\n",
        "input = dataset[370]\n",
        "\n",
        "image = split_string(input[\"file_name_x\"])\n",
        "\n",
        "yolo_output = yolo(local_path+image)\n",
        "\n",
        "class_names=yolo.names\n",
        "model, preprocess = clip.load(\"ViT-B/32\")\n",
        "model.eval()\n",
        "\n",
        "#pass each bounding box segmented image into clip with the sentence\n",
        "for i in range(len(yolo_output.xyxy[0])):\n",
        "\n",
        "\n",
        "\n",
        "    # convert the tensor to a numpy array\n",
        "    #numpy_array = cpu_tensor.numpy()\n",
        "    #x1 = yolo_output.xyxy[0][i][0].numpy()\n",
        "    class_index=int(yolo_output.pred[0][i][5])\n",
        "    label=class_names[class_index]\n",
        "    print(label)\n",
        "    x1 = yolo_output.xyxy[0][i][0].cpu().numpy()\n",
        "    x1 = np.rint(x1)\n",
        "\n",
        "    y1 = yolo_output.xyxy[0][i][1].cpu().numpy()\n",
        "    y1 = np.rint(y1)\n",
        "    x2 = yolo_output.xyxy[0][i][2].cpu().numpy()\n",
        "    x2 = np.rint(x2)\n",
        "    y2 = yolo_output.xyxy[0][i][3].cpu().numpy()\n",
        "    y2 = np.rint(y2)\n",
        "\n",
        "    print(x1, y1, x2, y2)\n",
        "\n",
        "    img = Image.open(local_path+image).convert(\"RGB\")    \n",
        "    img = img.crop((x1, y1, x2, y2))\n",
        "    if(i<10):\n",
        "      img.save(\"bb_0\"+str(i)+\".jpg\")\n",
        "    else:\n",
        "      img.save(\"bb_\"+str(i)+\".jpg\")\n",
        "\n",
        "    %matplotlib inline\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "    img = preprocess(img).unsqueeze(0)\n",
        "    image_imput = torch.tensor(img)\n",
        "    sentence = input[\"sentences\"][\"raw\"]\n",
        "    print(sentence)\n",
        "    text = clip.tokenize([sentence])\n",
        "    \n",
        "    ''' with torch.no_grad():\n",
        "        image_features = model.encode_image(img).float()\n",
        "        text_features = model.encode_text(text).float()\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "    similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
        "    print(similarity)'''\n"
      ],
      "metadata": {
        "id": "kASyBTB7rOpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Display image </h3>"
      ],
      "metadata": {
        "id": "pIlagUTzrnBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnds = split_string(input[\"file_name_x\"])\n",
        "\n",
        "poer=Image.open(local_path+fnds)\n",
        "poer"
      ],
      "metadata": {
        "id": "dI5tE9gUrjMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Padd YOLO bounding boxes with average pixel value </h3>\n",
        "\n",
        "Since the bounding boxes that we obtain as output from YOLO are not in a predefined shape, and by default CLIP preprocesses them in order to have them in a size of 224x224, and feeding them into the CLIP encoder without processing them will cause them to be stretched, enlarged resulting in a loss of information. We don't want to lose information on our images, so we decided to apply a padding of the image in order to have them resized by 224x224 but mantaining also the informations. \n",
        "The colour we applied to the padding depends on the mean value of the colors of the pixels of the image. This is done in order both to reduce the loss of informaion, but also not to add noise or other informations inside the image. \n",
        "The way we apply the padding depends on the size of the image: \n",
        "- if the image has one of the two dimensions larger than 224, we create a square image with the size of the larger between width and height of the image, and with colour as the mean value of the colors. We then put our starting image at the center, and in the end it is resized to 224x224\n",
        "- if both the dimensions are lower than 224, the original image is just padded\n",
        "\n",
        "An example of the results we obtain is the following:\n",
        "![picture](https://drive.google.com/uc?id=1UgdwmBkeVIV0Zz21nKdJpYZ2NEQfk229\n",
        ")\n"
      ],
      "metadata": {
        "id": "uxBfW-eFrt8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def padd_image(img):\n",
        "  print(type(img))\n",
        "  avg_color_per_row = np.average(img, axis=0)\n",
        "  avg_color = np.average(avg_color_per_row, axis=0)\n",
        "  old_image_width,old_image_height  = img.size\n",
        "  # create new image of desired size and color (blue) for padding\n",
        "  if(old_image_height>224 or old_image_width>224):\n",
        "    if(old_image_height>old_image_width):\n",
        "          new_image_width = old_image_height\n",
        "          new_image_height = old_image_height\n",
        "\n",
        "\n",
        "    else:\n",
        "          new_image_width = old_image_width\n",
        "          new_image_height = old_image_width\n",
        "\n",
        "    \n",
        "    color=avg_color\n",
        "    #color = (255,0,255)\n",
        "\n",
        "    result = np.full((new_image_height,new_image_width, 3), color, dtype=np.uint8)\n",
        "\n",
        "    # compute center offset\n",
        "    x_center = (new_image_width - old_image_width) // 2\n",
        "    y_center = (new_image_height - old_image_height) // 2\n",
        "\n",
        "    # copy img image into center of result image\n",
        "    result[y_center:y_center+old_image_height, \n",
        "          x_center:x_center+old_image_width] = img   \n",
        "\n",
        "        \n",
        "  else:\n",
        "    new_image_width = 224\n",
        "    new_image_height = 224\n",
        "    '''if(old_image_height>old_image_width):\n",
        "          new_image_width = old_image_height\n",
        "          new_image_height = old_image_height\n",
        "\n",
        "\n",
        "    else:\n",
        "          new_image_width = old_image_width\n",
        "          new_image_height = old_image_width'''\n",
        " \n",
        "    #color = (255,0,255)\n",
        "    color=avg_color\n",
        "    result = np.full((new_image_height,new_image_width, 3), color, dtype=np.uint8)\n",
        "\n",
        "    # compute center offset\n",
        "    x_center = (new_image_width - old_image_width) // 2\n",
        "    y_center = (new_image_height - old_image_height) // 2\n",
        "\n",
        "    # copy img image into center of result image\n",
        "    result[y_center:y_center+old_image_height, \n",
        "          x_center:x_center+old_image_width] = img\n",
        "\n",
        "\n",
        "  img= Image.fromarray(result)\n",
        "  img=img.resize((224,224))\n",
        "  return img"
      ],
      "metadata": {
        "id": "NKEdDlzlr4m8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_images = []\n",
        "images = []\n",
        "texts = []\n",
        "plt.figure(figsize=(16, 5))\n",
        "\n",
        "for filename in [filename for filename in sorted(os.listdir(os.getcwd())) if filename.endswith(\".png\") or filename.endswith(\".jpg\")]:\n",
        "    print(filename)\n",
        "    name = os.path.splitext(filename)[0]\n",
        "\n",
        "    image = Image.open(os.path.join(os.getcwd(), filename)).convert(\"RGB\")\n",
        "    image=padd_image(image)\n",
        "\n",
        "  \n",
        "    plt.subplot(4, 8, len(images) + 1)\n",
        "    plt.imshow(image)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    original_images.append(image)\n",
        "    images.append(preprocess(image))    \n",
        "    texts.append(filename)\n",
        "\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "EyZczgDMr7R-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Build images features </h3>"
      ],
      "metadata": {
        "id": "gTOAjM8KsBrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_input = torch.tensor(np.stack(images)).cuda()\n",
        "text_tokens = clip.tokenize([\"This is \" + desc for desc in texts]).cuda()\n",
        "\n",
        "print(image_input.size())"
      ],
      "metadata": {
        "id": "dfq9Au5UsFuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Stanza NLP analysis </h1>"
      ],
      "metadata": {
        "id": "bTNeb4-fsL5c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to discriminate our predictions we use some Natural Language Processing techniques in order to find the subject of our caption. \n",
        "What we decided to use is the Dependency graph. This is useful because it represent the dependencies between the various tokens inside of the sentence. \n",
        "In fact in each sentence we can define the so called **root** of the sentence, which is the token on which all other tokens in the sentence depend. This can generate two possible scenarios: \n",
        "- **The root is a noun**: this is the best case for our task since we can identify that name as the subject of our caption.\n",
        "- **The root is a verb**:  This create what is called a \"verbal phrase\", which is a sentence in which the root is the verb. Removing it from the sentence would make it lose meaning. When we have such type of sentence, there is high chance of having inside of the dependencies what is called \"NSUBJ\" or  [Nominal Subject](https://universaldependencies.org/en/dep/nsubj.html) (both active and passive). If it is present than this is the subject of our sentence. \n",
        "Otherwise if it is not present, once we have found the root of the sentence as a verb, we go back from the verb to the beginning of the sentence and we select as root of the sentence the first noun that we find. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-XGVBkcjNWXU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://editor.analyticsvidhya.com/uploads/29920Screenshot%20(127).png' >\n",
        "Example of a dependency graph"
      ],
      "metadata": {
        "id": "CHgIlEY2OT-W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Install stanza dependency graph model </h3>"
      ],
      "metadata": {
        "id": "Sct319eOsPYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import stanza NLP model for english language\n",
        "stanza.download('en',model_dir='/models/english',package='partut')\n",
        "nlp = stanza.Pipeline(lang='en')"
      ],
      "metadata": {
        "id": "M1Thgx_4sTeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Preprocess input caption </h3>\n",
        "\n",
        "We noticed that there was a problem with the so called \"not well-formed\" sentences (*In natural language processing, a not well-formed sentence refers to a sentence that does not conform to the rules and conventions of the language, making it difficult or impossible to be accurately processed and understood by a computer program or a human reader*), this was creating the effect of not being able to find the correct root of the sentence. \n",
        "For example one of the cases we encountered was the sentence: \"*boy holding two bears*\" this after we analyzed it with [corenlp](https://corenlp.run/) gave us the following result: \n",
        "![picture](https://drive.google.com/uc?id=1KrY63kV1KwWWsjyyPnIlsOW1rUtU0KXM\n",
        ")\n",
        "\n",
        "This is an example of a \"not well-formed\" sentence, in fact although the root of the sentence is the verb \"holding\", when we apply the NLP parsing we cannot retrieve the noun \"boy\" as the subject of the sentence since it is recognised as an [INTJ](https://universaldependencies.org/u/pos/INTJ.html).\n",
        "\n",
        "In order to solve this problem we noticed that adding a [determiner](https://universaldependencies.org/u/pos/DET.html) at the beginning of the sentence allows us to avoid having such problems. Between all the possible determiners we chose \"*the*\" as it does not change the meaning of the sentence and it is the most general one. \n",
        "\n",
        "After this is applied we obtain this dependecy parsing: \n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1Rt4nU5kXKSn-pq6nRkaCAcyp__m7Z1cd\n",
        ")\n",
        "In which we can see that the word \"boy\" is being parsed as a [noun](https://universaldependencies.org/u/pos/NOUN.html)"
      ],
      "metadata": {
        "id": "Xo6RB66UsWpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_sent=input[\"sentences\"][\"raw\"]\n",
        "# put sentence in lowecase\n",
        "nlp_sent=nlp_sent.lower()\n",
        "# if the sentence does not start with \"a\" or \"the\" insert it\n",
        "x = nlp_sent.split(\" \")\n",
        "if(x[0]!=\"the\" and x[0]!=\"a\"):\n",
        "    nlp_sent=\"the \"+nlp_sent\n",
        "\n",
        "print(nlp_sent)"
      ],
      "metadata": {
        "id": "Bsu_69_Gscfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Analyze caption and compute features of its root </h3>\n",
        "\n",
        "After we obtained the root of the sentence we use the CLIP text encoder to get the features for our root"
      ],
      "metadata": {
        "id": "NucfKCZ2siTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nlp_sent=nlp_sent.lower()\n",
        "print(nlp_sent)\n",
        "# Process the sentence\n",
        "doc = nlp(nlp_sent)\n",
        "# print nlp dependencies\n",
        "doc.sentences[0].print_dependencies()\n",
        "\n",
        "print(input[\"sentences\"][\"raw\"])\n",
        "root=''\n",
        "phrase_upos=[]\n",
        "# get heads of words\n",
        "heads=[sent.words[word.head-1].text for sent in doc.sentences for word in sent.words]\n",
        "for sent in doc.sentences:\n",
        "   for word in sent.words:\n",
        "     #if it is a verbal phrase then take the nominal subject of the phrase\n",
        "     if(word.deprel=='nsubj' or word.deprel=='nsubj:pass'):\n",
        "       root=word.text\n",
        "       print(word.text)\n",
        "       break\n",
        "     print(word)  \n",
        "     phrase_upos.append(word)\n",
        "     # else take the root of the phrase\n",
        "     if(word.head==0):\n",
        "       print(word.text)\n",
        "       root=word.text\n",
        "       # if the root is a verb\n",
        "       if(word.upos=='VERB'):\n",
        "         for w in reversed(phrase_upos):\n",
        "          # go back until you get a noun\n",
        "          if(w.upos=='NN'):\n",
        "             print(w.text)\n",
        "   \n",
        " \n",
        "\n",
        "\n",
        "\n",
        "#print(heads)\n",
        "# this can be used to tokenize an entire phrase\n",
        "'''prompt_tokens=clip.tokenize(nlp_sent,context_length=77,truncate=True).cuda()\n",
        "with torch.no_grad():\n",
        "  prompt_features = model.encode_text(prompt_tokens).float()\n",
        "print(prompt_features.size())'''\n",
        "# this is used to tokenize only the single word\n",
        "prompt_tokens=clip.tokenize(root,context_length=77,truncate=True).cuda()\n",
        "# calculate features\n",
        "with torch.no_grad():\n",
        "  prompt_features = model.encode_text(prompt_tokens).float()\n"
      ],
      "metadata": {
        "id": "kawuB8twspgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Analyze YOLO class prediction </h3>\n",
        "\n",
        "We then encode also the classes that are present inside our YOLO predictions. These encodings will be used to compute the similarity between the root of our sentence and the YOLO classes in order to bring us back to one of the original YOLO classes. Having the subject of our sentence expressed as one of the YOLO classes we can exclude all the bounding boxes that have a label that is different from the one of our subject."
      ],
      "metadata": {
        "id": "_gy2H__xsqpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "names=[]\n",
        "\n",
        "# for each of the YOLO output prediction get the correspondent class\n",
        "for i in range(len(yolo_output.xyxy[0])):\n",
        "    class_index=int(yolo_output.pred[0][i][5])\n",
        "    label=class_names[class_index]\n",
        "    print(label)\n",
        "    names.append(label)\n",
        "\n",
        "\n",
        "# tokenize and create features for the classes\n",
        "tokens=clip.tokenize(names,context_length=77,truncate=True).cuda()\n",
        "with torch.no_grad():\n",
        "  classes_features = model.encode_text(tokens).float()\n",
        "print(classes_features.size())"
      ],
      "metadata": {
        "id": "wgC-vmuWsyzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Compute prompt predictions in order to check similarity between root and YOLO class </h3>"
      ],
      "metadata": {
        "id": "gCfS1f8As2v1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute similarities between YOLO predicrions and the subject of the phrase\n",
        "prompt_features /= prompt_features.norm(dim=-1, keepdim=True)\n",
        "classes_features /= classes_features.norm(dim=-1, keepdim=True)\n",
        "prompt_similarity = classes_features.cpu().numpy() @ prompt_features.cpu().numpy().T"
      ],
      "metadata": {
        "id": "TJREdHYls-1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Take class with maximum similarity </h3>\n",
        "\n",
        "We only want to have the class with the best similarity"
      ],
      "metadata": {
        "id": "8m31oRwTtBOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# take as desired class the one with the highest similarity\n",
        "rappresentation=np.argmax(prompt_similarity)\n",
        "print(root)\n",
        "interested_class=names[rappresentation]\n",
        "\n",
        "print(interested_class)"
      ],
      "metadata": {
        "id": "bvWljUggtG2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Stable Diffusion </h1>"
      ],
      "metadata": {
        "id": "-0A7jtrntNSp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Import stable diffusion model and create pipeline </h3>"
      ],
      "metadata": {
        "id": "iBP_vuhbtRb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load stable diffusion mode\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16)  \n",
        "pipe = pipe.to(\"cuda\")\n"
      ],
      "metadata": {
        "id": "IXC8_N7otW6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Generate images </h3>\n",
        "\n",
        "Since we know that Stable Diffusion has a component of randomness in the image generation we deciced to take into consideration creating more than one image. We decided to create more than one image in order to reduce the weight of a possible \"outlier\" in the images.\n",
        "\n",
        "We then "
      ],
      "metadata": {
        "id": "2UmSqaU6tekv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# insert the prompt the caption we are provided by the dataset\n",
        "prompt= 'Use deep learning algorithms to generate a hyper-realistic portrait of a'+   input[\"sentences\"][\"raw\"] +' Use advanced image processing techniques to make the image appear as if it were a photograph'\n",
        "\n",
        "# create the images with stable diffusion\n",
        "stable_input = pipe(prompt,num_inference_steps=50).images[0]  # image here is in [PIL format](https://pillow.readthedocs.io/en/stable/)\n",
        "stable_input2 = pipe(prompt,num_inference_steps=50).images[0]  # image here is in [PIL format](https://pillow.readthedocs.io/en/stable/)\n",
        "stable_input3 = pipe(prompt,num_inference_steps=50).images[0]  # image here is in [PIL format](https://pillow.readthedocs.io/en/stable/)\n",
        "\n",
        "stable_input"
      ],
      "metadata": {
        "id": "ivmsucXkteFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Create encoding for images </h3>"
      ],
      "metadata": {
        "id": "Nc7aFnTotopi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with torch.no_grad():\n",
        "    # transform image into tensor\n",
        "    convert_tensor = transforms.ToTensor()\n",
        "    # resize image for clip\n",
        "    stable_input=stable_input.resize([224,224])   \n",
        "    stable_input2=stable_input2.resize([224,224])   \n",
        "    stable_input3=stable_input3.resize([224,224])   \n",
        "\n",
        "    # create tensors\n",
        "    image_stable=torch.tensor(np.stack(convert_tensor(stable_input))).cuda()   \n",
        "    image_stable2=torch.tensor(np.stack(convert_tensor(stable_input2))).cuda()   \n",
        "    image_stable3=torch.tensor(np.stack(convert_tensor(stable_input3))).cuda()   \n",
        "    \n",
        "    # stack images into a tensor\n",
        "    img_tens = torch.stack([image_stable,image_stable2,image_stable3])\n",
        "    \n",
        "    print(image_input.size())\n",
        "    print(img_tens.size())\n",
        "    \n",
        "    # encode stable diffusion images\n",
        "    text_features= model.encode_image(img_tens).float()\n",
        "    \n",
        "    # encode YOLO bounding boxes\n",
        "    image_features = model.encode_image(image_input).float()\n",
        "    \n",
        "    print(text_features.size())\n",
        "    print(image_features.size())\n",
        "\n",
        "    #stable_input=stable_input.resize([224,224])   \n",
        "    \n",
        "\n",
        "\n",
        "    #text_features = model.encode_text(text_tokens).float()"
      ],
      "metadata": {
        "id": "MX5y34pRttVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Compute Cosine similarity </h3>"
      ],
      "metadata": {
        "id": "UdRAsmxEu3Jh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute cosine similarity\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T"
      ],
      "metadata": {
        "id": "qBfblfN3u3t_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute mean of similarities\n",
        "similarity_vec=np.mean(similarity,axis=0)\n"
      ],
      "metadata": {
        "id": "h-9J8OwbvCQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Take only the best bounding box </h3>\n",
        "\n",
        "Once we have computed the similarity values, and obtained the mean vecotor of the similarities we want to filter out our results in order to make our prediction. The first thing we do is to filter out all the values in the similarity vector by checking if the corresponding bounding boxes have the class as the one we obtained before.\n",
        "Than inside of the \"filtered\" vector we take the maximum value that will correspond to our prediction."
      ],
      "metadata": {
        "id": "fUk4X77ovEqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "possible_values=[]\n",
        "indexes=[]\n",
        "for i in range(len(yolo_output.xyxy[0])):\n",
        "    class_index=int(yolo_output.pred[0][i][5])\n",
        "    label=class_names[class_index]\n",
        "    # take only the YOLO predictions that have as class the one we computed in the NLP analysis\n",
        "    if(label==interested_class):\n",
        "      possible_values.append(similarity_vec[i])\n",
        "      indexes.append(i)\n",
        "# take only the best class      \n",
        "print(max(possible_values))\n",
        "index=possible_values.index(max(possible_values))\n",
        "index=indexes[index]\n",
        "print(nlp_sent)\n",
        "\n",
        "original_images[index]\n",
        "\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "EPG_fgQZvJLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Image Captioning </h1>"
      ],
      "metadata": {
        "id": "KpHq3rSDvTCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Import image captioning model </h3>"
      ],
      "metadata": {
        "id": "SR9MIHIkvcRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import image captioning model\n",
        "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model_blip = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
      ],
      "metadata": {
        "id": "skGHCzH0vZIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Create captions for YOLO bounding boxes </h3>"
      ],
      "metadata": {
        "id": "mN3Th96pvjK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "description_images=[]\n",
        "plt.figure(figsize=(16, 5))\n",
        "# load the YOLO bounding boxes\n",
        "for filename in [filename for filename in sorted(os.listdir(os.getcwd())) if filename.endswith(\".png\") or filename.endswith(\".jpg\")]:\n",
        "    print(filename)\n",
        "    name = os.path.splitext(filename)[0]\n",
        "    image = Image.open(os.path.join(os.getcwd(), filename)).convert(\"RGB\")\n",
        "    plt.subplot(4, 8, len(description_images) + 1)\n",
        "    plt.imshow(image)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    description_images.append(image)\n",
        "    \n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "tX0z4ZpAvsYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# process images for image captioning\n",
        "inputs = processor(images=description_images, return_tensors=\"pt\")\n",
        "pixel_values = inputs.pixel_values\n",
        "# generate captions\n",
        "generated_ids = model_blip.generate(pixel_values=pixel_values, max_length=50)\n",
        "generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "print(generated_caption)"
      ],
      "metadata": {
        "id": "-B1L-KTevv3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Create features for the generated descriptions </h3>"
      ],
      "metadata": {
        "id": "qQrjN7huvyNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encode the text \n",
        "text_description = clip.tokenize(generated_caption).cuda()\n",
        "text_sent = clip.tokenize([sentence]).cuda()\n",
        "with torch.no_grad():\n",
        "\n",
        "  sent_features = model.encode_text(text_sent).float()\n",
        "  caption_features = model.encode_text(text_description).float()"
      ],
      "metadata": {
        "id": "Y_WZ5eptv35I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute similarity\n",
        "caption_features /= caption_features.norm(dim=-1, keepdim=True)\n",
        "sent_features /= sent_features.norm(dim=-1, keepdim=True)\n",
        "similarity_text = sent_features.cpu().numpy() @ caption_features.cpu().numpy().T"
      ],
      "metadata": {
        "id": "rUYqcrGtv5pG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Create combination of Image to Image similarity with Text to Text similarity </h3>"
      ],
      "metadata": {
        "id": "RTYykojZv9bk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# process the value obtained\n",
        "similarity_text=similarity_text/10\n",
        "similarity_vec+=similarity_text[0]\n"
      ],
      "metadata": {
        "id": "lXFSmFqWwTHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Obtain only the best value </h3>"
      ],
      "metadata": {
        "id": "URhDLZbOwZAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "possible_values=[]\n",
        "indexes=[]\n",
        "\n",
        "for i in range(len(yolo_output.xyxy[0])):\n",
        "    class_index=int(yolo_output.pred[0][i][5])\n",
        "    label=class_names[class_index]\n",
        "    if(label==interested_class):\n",
        "      possible_values.append(similarity_vec[i])\n",
        "      indexes.append(i)\n",
        "print(max(possible_values))\n",
        "index=possible_values.index(max(possible_values))\n",
        "index=indexes[index]\n",
        "print(nlp_sent)\n",
        "\n",
        "original_images[index]"
      ],
      "metadata": {
        "id": "GUjVEwJLwXEG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}