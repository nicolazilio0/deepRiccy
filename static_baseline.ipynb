{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolazilio0/deepRiccy/blob/main/static_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOvgNpY7LwSU"
      },
      "source": [
        "# Deep riccy project\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CzySrjXzHhN"
      },
      "source": [
        "## Env creation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiKX4oZgzKLR"
      },
      "outputs": [],
      "source": [
        "local_path = 'refcocog/images/'\n",
        "local_annotations = 'refcocog/annotations/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4ZuKVoVzdFH"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "import torch\n",
        "from PIL import Image, ImageDraw\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from pkg_resources import packaging\n",
        "import clip\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import skimage\n",
        "import IPython.display\n",
        "\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "import torchmetrics as tm\n",
        "import torchvision\n",
        "from torchvision import ops\n",
        "import stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8K-ozdJYLrqS",
        "outputId": "0661fe24-cb05-4393-dee7-49e4dafa6e9c",
        "colab": {
          "referenced_widgets": [
            "b61d42ba397843de800c7dc9ac4e01f2",
            "424e4706208d4bbaa7175da8be43794c"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b61d42ba397843de800c7dc9ac4e01f2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   â€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-30 11:52:23 INFO: Downloading these customized packages for language: en (English)...\n",
            "==============================\n",
            "| Processor       | Package  |\n",
            "------------------------------\n",
            "| tokenize        | partut   |\n",
            "| mwt             | partut   |\n",
            "| pos             | partut   |\n",
            "| lemma           | partut   |\n",
            "| depparse        | partut   |\n",
            "| backward_charlm | 1billion |\n",
            "| pretrain        | partut   |\n",
            "| forward_charlm  | 1billion |\n",
            "==============================\n",
            "\n",
            "2023-05-30 11:52:23 INFO: File exists: /home/pappol/stanza_resources/en/tokenize/partut.pt\n",
            "2023-05-30 11:52:23 INFO: File exists: /home/pappol/stanza_resources/en/mwt/partut.pt\n",
            "2023-05-30 11:52:23 INFO: File exists: /home/pappol/stanza_resources/en/pos/partut.pt\n",
            "2023-05-30 11:52:23 INFO: File exists: /home/pappol/stanza_resources/en/lemma/partut.pt\n",
            "2023-05-30 11:52:24 INFO: File exists: /home/pappol/stanza_resources/en/depparse/partut.pt\n",
            "2023-05-30 11:52:24 INFO: File exists: /home/pappol/stanza_resources/en/backward_charlm/1billion.pt\n",
            "2023-05-30 11:52:24 INFO: File exists: /home/pappol/stanza_resources/en/pretrain/partut.pt\n",
            "2023-05-30 11:52:24 INFO: File exists: /home/pappol/stanza_resources/en/forward_charlm/1billion.pt\n",
            "2023-05-30 11:52:24 INFO: Finished downloading models and saved to /home/pappol/stanza_resources.\n",
            "2023-05-30 11:52:24 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "424e4706208d4bbaa7175da8be43794c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   â€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-30 11:52:27 INFO: Loading these models for language: en (English):\n",
            "============================\n",
            "| Processor    | Package   |\n",
            "----------------------------\n",
            "| tokenize     | combined  |\n",
            "| pos          | combined  |\n",
            "| lemma        | combined  |\n",
            "| constituency | wsj       |\n",
            "| depparse     | combined  |\n",
            "| sentiment    | sstplus   |\n",
            "| ner          | ontonotes |\n",
            "============================\n",
            "\n",
            "2023-05-30 11:52:27 INFO: Using device: cuda\n",
            "2023-05-30 11:52:27 INFO: Loading: tokenize\n",
            "2023-05-30 11:52:30 INFO: Loading: pos\n",
            "2023-05-30 11:52:30 INFO: Loading: lemma\n",
            "2023-05-30 11:52:30 INFO: Loading: constituency\n",
            "2023-05-30 11:52:31 INFO: Loading: depparse\n",
            "2023-05-30 11:52:31 INFO: Loading: sentiment\n",
            "2023-05-30 11:52:31 INFO: Loading: ner\n",
            "2023-05-30 11:52:32 INFO: Done loading processors!\n"
          ]
        }
      ],
      "source": [
        "stanza.download('en',package='partut')\n",
        "nlp = stanza.Pipeline(lang='en')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKNYTNptzWrx"
      },
      "source": [
        "## Baseline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "418bIOjizjPc"
      },
      "source": [
        "### Utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KmNv2ieLrqU"
      },
      "outputs": [],
      "source": [
        "def remove_of(sentence):\n",
        " if \"the side of\" in sentence:\n",
        "    index=sentence.find(\"the side of\")\n",
        "    sentence=sentence[index+11:]\n",
        "    return sentence\n",
        " if \"the handle of\" in sentence:\n",
        "    index=sentence.find(\"the handle of\")\n",
        "    sentence=sentence[index+13:]\n",
        " if \"the bunch of\" in sentence:\n",
        "    index=sentence.find(\"the bunch of\")\n",
        "    sentence=sentence[index+12:]\n",
        "    return sentence   \n",
        " if \"the corner of\" in sentence:\n",
        "    index=sentence.find(\"the corner of\")\n",
        "    sentence=sentence[index+13:]\n",
        "    return sentence\n",
        " if \"the end of\" in sentence:\n",
        "    index=sentence.find(\"the end of\")\n",
        "    sentence=sentence[index+10:]\n",
        "    return sentence\n",
        " if \"the half of\" in sentence:\n",
        "    index=sentence.find(\"the half of\")\n",
        "    sentence=sentence[index+11:]\n",
        "    return sentence    \n",
        " if \"the edge of\" in sentence:\n",
        "    index=sentence.find(\"the edge of\")\n",
        "    sentence=sentence[index+11:]    \n",
        "    return sentence\n",
        " if \"the back of\" in sentence:\n",
        "    index=sentence.find(\"the back of\")\n",
        "    sentence=sentence[index+11:]\n",
        "    return sentence   \n",
        " if \"the smaller of\" in sentence:\n",
        "    index=sentence.find(\"the smaller of\")\n",
        "    sentence=sentence[index+14:]\n",
        "    return sentence    \n",
        " if \"the piece of\" in sentence:\n",
        "    index=sentence.find(\"the piece of\")\n",
        "    sentence=sentence[index+16:]\n",
        "    return sentence \n",
        " if \"the wing of\" in sentence:\n",
        "    index=sentence.find(\"the wing of\")\n",
        "    sentence=sentence[index+11:]    \n",
        "\n",
        " if \"the front of\" in sentence:\n",
        "    index=sentence.find(\"the front of\")\n",
        "    sentence=sentence[index+12:]   \n",
        "    return sentence \n",
        " if \"the back side of\" in sentence:\n",
        "    index=sentence.find(\"the back side of\")\n",
        "    sentence=sentence[index+16:]   \n",
        "    return sentence\n",
        " if \"the front side of\" in sentence:\n",
        "    index=sentence.find(\"the front side of\")\n",
        "    sentence=sentence[index+17:]   \n",
        "    return sentence \n",
        " if \"the left side of\" in sentence:\n",
        "    index=sentence.find(\"the left side of\")\n",
        "    sentence=sentence[index+16:]   \n",
        "    return sentence\n",
        " if \"the right side of\" in sentence:\n",
        "    index=sentence.find(\"the back side of\")\n",
        "    sentence=sentence[index+17:]   \n",
        "    return sentence\n",
        "  \n",
        " if \"the pile of\" in sentence:\n",
        "    index=sentence.find(\"the pile of\")\n",
        "    sentence=sentence[index+11:]    \n",
        "    return sentence\n",
        " if \"the pair of\" in sentence:\n",
        "    index=sentence.find(\"the pair of\")\n",
        "    sentence=sentence[index+11:] \n",
        "    return sentence   \n",
        " if \"the pieces of\" in sentence:\n",
        "    index=sentence.find(\"the pieces of\")\n",
        "    sentence=sentence[index+13:]   \n",
        "    return sentence \n",
        " if \"the intersection of\" in sentence:\n",
        "    index=sentence.find(\"the intersection of\")\n",
        "    sentence=sentence[index+19:]  \n",
        "    return sentence  \n",
        " if \"the middle of\" in sentence:\n",
        "    index=sentence.find(\"the middle of\")\n",
        "    sentence=sentence[index+13:]\n",
        "    return sentence    \n",
        " if \"the patch of\" in sentence:\n",
        "    index=sentence.find(\"the patch of\")\n",
        "    sentence=sentence[index+12:]    \n",
        "    return sentence\n",
        " if \"the couple of\" in sentence:\n",
        "    index=sentence.find(\"the couple of\")\n",
        "    sentence=sentence[index+12:]    \n",
        "    return sentence\n",
        " if \"the slice of\" in sentence:\n",
        "    index=sentence.find(\"the slice of\")\n",
        "    sentence=sentence[index+12:]    \n",
        "    return sentence\n",
        " if \"the tallest of\" in sentence:\n",
        "    index=sentence.find(\"the tallest of\")\n",
        "    sentence=sentence[index+14:]    \n",
        "    return sentence\n",
        " if \"the kind of\" in sentence:\n",
        "    index=sentence.find(\"the kind of\")\n",
        "    sentence=sentence[index+11:]\n",
        "    return sentence    \n",
        " if \"that is\" in sentence:\n",
        "    index=sentence.find(\"that is\")\n",
        "    sentence=sentence[:index]\n",
        "    return sentence\n",
        " if \"the part of\" in sentence:\n",
        "    index=sentence.find(\"the part of\")\n",
        "    sentence=sentence[index+11:]\n",
        "    return sentence\n",
        " if \"the corner of\" in sentence:\n",
        "    index=sentence.find(\"the corner of\")\n",
        "    sentence=sentence[index+13:]\n",
        "    return sentence\n",
        " if \"the half of\" in sentence:\n",
        "    index=sentence.find(\"the half of\")\n",
        "    sentence=sentence[index+11:]\n",
        "    return sentence\n",
        " if \"the top of\" in sentence:\n",
        "    index=sentence.find(\"the top of\")\n",
        "    sentence=sentence[index+10:]\n",
        "    return sentence\n",
        " return sentence\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQp4X9aELrqV"
      },
      "outputs": [],
      "source": [
        "def sent_stanza_processing(sentence):\n",
        "    sentence = sentence.lower()\n",
        "    if sentence.startswith('there is '):\n",
        "        sentence = 'the ' + sentence[9:]\n",
        "    if sentence.startswith('this is '):\n",
        "        sentence = sentence[9:]\n",
        "    sentence = remove_of(sentence)\n",
        "\n",
        "    nlp_sent = sentence.lower()\n",
        "\n",
        "    # put the sentence in lower case\n",
        "    # if the sentence does not start with \"a\" or \"the\" insert it\n",
        "    x = nlp_sent.split(\" \")\n",
        "    if (x[0] != \"the\" and x[0] != \"a\"):\n",
        "        nlp_sent = \"the \" + nlp_sent\n",
        "\n",
        "    doc = nlp(nlp_sent)\n",
        "    # print nlp dependencies\n",
        "    # doc.sentences[0].print_dependencies()\n",
        "    # print(input[\"sentences\"][\"raw\"])\n",
        "    root = ''\n",
        "    phrase_upos = []\n",
        "    # get heads of words\n",
        "    heads = [sent.words[word.head -\n",
        "                        1].text for sent in doc.sentences for word in sent.words]\n",
        "    for sent in doc.sentences:\n",
        "        for word in sent.words:\n",
        "            # if it is a verbal phrase then take the nominal subject of the phrase\n",
        "            if (word.deprel == 'nsubj' or word.deprel == 'nsubj:pass'):\n",
        "                root = word.text\n",
        "                return word.text\n",
        "                # print(word.text)\n",
        "                break\n",
        "            # print(word)\n",
        "            phrase_upos.append(word)\n",
        "            # else take the root of the phrase\n",
        "            if (word.head == 0):\n",
        "                # print(word.text)\n",
        "                return word.text\n",
        "                # root=word.text\n",
        "                # if the root is a verb\n",
        "                if (word.upos == 'VERB'):\n",
        "                    for w in reversed(phrase_upos):\n",
        "                        # go back until you get a noun\n",
        "                        if (w.upos == 'NN'):\n",
        "                            return word.text\n",
        "                            # print(w.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKBX0Bc7LrqW"
      },
      "outputs": [],
      "source": [
        "def get_root(yolo_output, sentence, model, yolo):\n",
        "    root = sent_stanza_processing(sentence)\n",
        "    # print(root)\n",
        "    prompt_tokens = clip.tokenize(\n",
        "        root, context_length=77, truncate=True).cuda()\n",
        "    with torch.no_grad():\n",
        "        prompt_features = model.encode_text(prompt_tokens).float()\n",
        "\n",
        "    names = []\n",
        "    for a in range(len(yolo_output.xyxy[0])):\n",
        "        class_index = int(yolo_output.pred[0][a][5])\n",
        "        label = yolo.names[class_index]\n",
        "        names.append(label)\n",
        "    tokens = clip.tokenize(names, context_length=77, truncate=True).cuda()\n",
        "    with torch.no_grad():\n",
        "        classes_features = model.encode_text(tokens).float()\n",
        "    prompt_features /= prompt_features.norm(dim=-1, keepdim=True)\n",
        "    classes_features /= classes_features.norm(dim=-1, keepdim=True)\n",
        "    prompt_similarity = classes_features.cpu().numpy() @ prompt_features.cpu().numpy().T\n",
        "    if prompt_similarity.shape[0] == 0:\n",
        "        return \"empty\"\n",
        "    rappresentation = np.argmax(prompt_similarity)\n",
        "\n",
        "    interested_class = names[rappresentation]\n",
        "    return interested_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9Uubvdxz5wG"
      },
      "outputs": [],
      "source": [
        "# given an image pad the image to fit the dimension of clip and fill the padding with the median color of the image\n",
        "\n",
        "def padd_image(img):\n",
        "    avg_color_per_row = np.average(img, axis=0)\n",
        "    avg_color = np.average(avg_color_per_row, axis=0)\n",
        "    old_image_width, old_image_height = img.size\n",
        "    # create new image of desired size and color (blue) for padding\n",
        "    if (old_image_height > 224 or old_image_width > 224):\n",
        "        if (old_image_height > old_image_width):\n",
        "            new_image_width = old_image_height\n",
        "            new_image_height = old_image_height\n",
        "\n",
        "        else:\n",
        "            new_image_width = old_image_width\n",
        "            new_image_height = old_image_width\n",
        "\n",
        "        color = avg_color\n",
        "        # color = (255,0,255)\n",
        "\n",
        "        result = np.full((new_image_height, new_image_width, 3),\n",
        "                         color, dtype=np.uint8)\n",
        "\n",
        "        # compute center offset\n",
        "        x_center = (new_image_width - old_image_width) // 2\n",
        "        y_center = (new_image_height - old_image_height) // 2\n",
        "\n",
        "        # copy img image into center of result image\n",
        "        result[y_center:y_center+old_image_height,\n",
        "               x_center:x_center+old_image_width] = img\n",
        "\n",
        "    else:\n",
        "        new_image_width = 224\n",
        "        new_image_height = 224\n",
        "\n",
        "        color = avg_color\n",
        "        result = np.full((new_image_height, new_image_width, 3),\n",
        "                         color, dtype=np.uint8)\n",
        "        # compute center offset\n",
        "        x_center = (new_image_width - old_image_width) // 2\n",
        "        y_center = (new_image_height - old_image_height) // 2\n",
        "\n",
        "        # copy img image into center of result image\n",
        "        result[y_center:y_center+old_image_height,\n",
        "               x_center:x_center+old_image_width] = img\n",
        "\n",
        "    img = Image.fromarray(result)\n",
        "    img = img.resize((224, 224))\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExVxvNtacKZS"
      },
      "outputs": [],
      "source": [
        "# remove the id in the image name string\n",
        "def split_string(string):\n",
        "    string = string.split(\"_\")\n",
        "    string = string[:-1]\n",
        "    string = \"_\".join(string)\n",
        "    append = \".jpg\"\n",
        "    string = string + append\n",
        "\n",
        "    return string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4rfPXV_zzHz"
      },
      "outputs": [],
      "source": [
        "# convert yolo format bbox into standard type\n",
        "def convert_bbox(bbox, img):\n",
        "    x1, y1, width, height = bbox\n",
        "    x2, y2 = x1 + width, y1 + height\n",
        "\n",
        "    # Verify coordinates\n",
        "    if x1 < 0 or y1 < 0 or x2 > img.width or y2 > img.height:\n",
        "        print(\"Bounding box fuori dai limiti dell'immagine!\")\n",
        "    else:\n",
        "        return x1, y1, x2, y2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVAC33cB5OhY"
      },
      "outputs": [],
      "source": [
        "# yolo bbox include class and precision, drop them\n",
        "def convert_yolo_bbox(bbox):\n",
        "    return bbox[:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5wPfVu-mkeO"
      },
      "outputs": [],
      "source": [
        "# test model on a number N of images and print metrics each tenth of the images\n",
        "def test_on_n_images(model, dataset, N):\n",
        "    model.reset_metrics()\n",
        "\n",
        "    for i in range(0, N):\n",
        "        input = dataset[i]\n",
        "        image_path = split_string(input[\"file_name\"])\n",
        "        sentence = input[\"sentences\"][\"raw\"]\n",
        "        gt = input[\"bbox\"]\n",
        "        original_img = Image.open(local_path + image_path).convert(\"RGB\")\n",
        "        # print img dimensions and box coordinates\n",
        "        bbox, img = model.evaluate(image_path, sentence, gt, original_img)\n",
        "\n",
        "        if i % (N/10) == 0:\n",
        "            print(\"Iteration: \", i)\n",
        "            print(model.get_metrics())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9qGI-Oxq9MQ"
      },
      "outputs": [],
      "source": [
        "# Validation function if given a dataframe runs the model parameter for the whole dataframe and prints the metrics\n",
        "def validate(model, dataframe):\n",
        "    model.reset_metrics()\n",
        "    for i in range(0, len(dataframe)):\n",
        "        input = dataframe.iloc[i]\n",
        "        image_path = split_string(input[\"file_name\"])\n",
        "        sentence = input[\"sentences\"][\"raw\"]\n",
        "        gt = input[\"bbox\"]\n",
        "        original_img = Image.open(local_path + image_path).convert(\"RGB\")\n",
        "        # print img dimensions and box coordinates\n",
        "        model.evaluate(image_path, sentence, gt, original_img)\n",
        "\n",
        "    print(model.get_metrics())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLRfZYnqz4zD"
      },
      "source": [
        "### Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-soPgP6nt47D"
      },
      "outputs": [],
      "source": [
        "# definition of recall metric\n",
        "class Recall:\n",
        "    def __init__(self):\n",
        "        self.true_positives = 0\n",
        "        self.false_negatives = 0\n",
        "\n",
        "    def update(self, correct):\n",
        "        if correct:\n",
        "            self.true_positives += 1\n",
        "        else:\n",
        "            self.false_negatives += 1\n",
        "\n",
        "    def compute(self):\n",
        "        return self.true_positives / (self.true_positives + self.false_negatives)\n",
        "\n",
        "    def reset(self):\n",
        "        self.true_positives = 0\n",
        "        self.false_negatives = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yN9vbC82z8CI"
      },
      "outputs": [],
      "source": [
        "class Metrics:\n",
        "    def __init__(self, model, trasform, treshold):\n",
        "        self.transform = trasform\n",
        "        # initialize torch tensor\n",
        "        self.iou = torch.tensor([]).cuda()\n",
        "        self.recall = Recall()\n",
        "        self.model = model\n",
        "        self.treshold = treshold\n",
        "        self.cosine_similarity = torch.tensor([]).cuda()\n",
        "        self.euclidean_distance = torch.tensor([]).cuda()\n",
        "\n",
        "    def update(self, predicted_bbox, target_bbox, predicted_image, target_image):\n",
        "        predicted_bbox = torch.tensor(predicted_bbox)\n",
        "        target_bbox = torch.tensor(target_bbox)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Preprocess the predicted image and compute the predicted embedding\n",
        "            predicted_image = padd_image(predicted_image)\n",
        "            image_tensor = self.transform(predicted_image)\n",
        "            image_tensor = image_tensor.unsqueeze(\n",
        "                0).cuda()  # Add batch dimension\n",
        "            predicted_embedding = self.model.encode_image(image_tensor)\n",
        "\n",
        "            # Preprocess the target image and compute the target embedding\n",
        "            target_image = padd_image(target_image)\n",
        "            target_image_tensor = self.transform(target_image)\n",
        "            target_image_tensor = target_image_tensor.unsqueeze(\n",
        "                0).cuda()  # Add batch dimension\n",
        "            target_embedding = self.model.encode_image(target_image_tensor)\n",
        "\n",
        "        similarity = torch.nn.functional.cosine_similarity(\n",
        "            predicted_embedding, target_embedding)\n",
        "        distance = torch.nn.functional.pairwise_distance(\n",
        "            predicted_embedding, target_embedding)\n",
        "\n",
        "        # convert bboxes into torch tensors\n",
        "        predicted_bbox = torch.tensor(predicted_bbox)\n",
        "        target_bbox = torch.tensor(target_bbox)\n",
        "        predicted_bbox = convert_yolo_bbox(predicted_bbox)\n",
        "        actual_iou = ops.box_iou(predicted_bbox.unsqueeze(\n",
        "            0).cuda(), target_bbox.unsqueeze(0).cuda())\n",
        "        self.iou = torch.cat((self.iou, actual_iou), 0)\n",
        "        # get iou value of the predicted bbox and the target bbox\n",
        "        if actual_iou > self.treshold:\n",
        "            self.recall.update(True)\n",
        "        else:\n",
        "            self.recall.update(False)\n",
        "        self.cosine_similarity = torch.cat(\n",
        "            (self.cosine_similarity, similarity), 0)\n",
        "        self.euclidean_distance = torch.cat(\n",
        "            (self.euclidean_distance, distance), 0)\n",
        "\n",
        "    def to_string(self):\n",
        "        mean_iou = torch.mean(self.iou)\n",
        "        recall_at_05_iou = self.recall.compute()\n",
        "        mean_cosine_similarity = torch.mean(self.cosine_similarity)\n",
        "        mean_euclidean_distance = torch.mean(self.euclidean_distance)\n",
        "\n",
        "        return f\"Mean IoU: {mean_iou:.4f}, Recall@0.5 IoU: {recall_at_05_iou:.4f}, Mean Cosine Similarity: {mean_cosine_similarity:.4f}, Mean Euclidean Distance: {mean_euclidean_distance:.4f}\"\n",
        "\n",
        "    def reset(self):\n",
        "        self.iou = torch.tensor([]).cuda()\n",
        "        self.recall.reset()\n",
        "        self.cosine_similarity = torch.tensor([]).cuda()\n",
        "        self.euclidean_distance = torch.tensor([]).cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx39cinC0BJb"
      },
      "source": [
        "### Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjC1bIj90D3I"
      },
      "outputs": [],
      "source": [
        "# dataset class definition\n",
        "class Coco(Dataset):\n",
        "    def __init__(self, path_json, path_pickle, train=True):\n",
        "        self.path_json = path_json\n",
        "        self.path_pickle = path_pickle\n",
        "        self.train = train\n",
        "\n",
        "        # load images and annotations\n",
        "        with open(self.path_json) as json_data:\n",
        "            data = json.load(json_data)\n",
        "            self.ann_frame = pd.DataFrame(data['annotations'])\n",
        "            self.ann_frame = self.ann_frame.reset_index(drop=False)\n",
        "\n",
        "        with open(self.path_pickle, 'rb') as pickle_data:\n",
        "            data = pickle.load(pickle_data)\n",
        "            self.refs_frame = pd.DataFrame(data)\n",
        "\n",
        "        # separate each sentence in dataframe\n",
        "        self.refs_frame = self.refs_frame.explode('sentences')\n",
        "        self.refs_frame = self.refs_frame.reset_index(drop=False)\n",
        "\n",
        "        self.size = self.refs_frame.shape[0]\n",
        "\n",
        "        # merge the dataframes\n",
        "        self.dataset = pd.merge(\n",
        "            self.refs_frame, self.ann_frame, left_on='ann_id', right_on='id')\n",
        "        # drop useless columns for cleaner and smaller dataset\n",
        "        self.dataset = self.dataset.drop(columns=['segmentation', 'id', 'category_id_y', 'ref_id', 'index_x',\n",
        "                                         'iscrowd', 'image_id_y', 'image_id_x', 'category_id_x', 'ann_id', 'sent_ids', 'index_y', 'area'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.dataset.iloc[idx]\n",
        "\n",
        "    def get_annotation(self, idx):\n",
        "        return self.ann_frame.iloc[idx]\n",
        "\n",
        "    def get_imgframe(self, idx):\n",
        "        return self.img_frame.iloc[idx]\n",
        "\n",
        "    def get_validation(self):\n",
        "        return self.dataset[self.dataset['split'] == 'val']\n",
        "\n",
        "    def get_test(self):\n",
        "        return self.dataset[self.dataset['split'] == 'test']\n",
        "\n",
        "    def get_train(self):\n",
        "        return self.dataset[self.dataset['split'] == 'train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peTMO0l70KRz",
        "outputId": "2a973a17-0e6c-4298-dd3e-c61196a8a6b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "95010\n"
          ]
        }
      ],
      "source": [
        "# dataset loading test and dimension check\n",
        "dataset = Coco(local_annotations + 'instances.json',\n",
        "               local_annotations + \"refs(umd).p\")\n",
        "\n",
        "print(len(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "ZDV7Hyel0KmQ",
        "outputId": "1c1f59ce-0e05-4225-e322-5e9f802104e7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>split</th>\n",
              "      <th>sentences</th>\n",
              "      <th>file_name</th>\n",
              "      <th>bbox</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>test</td>\n",
              "      <td>{'tokens': ['the', 'man', 'in', 'yellow', 'coa...</td>\n",
              "      <td>COCO_train2014_000000380440_491042.jpg</td>\n",
              "      <td>[374.31, 65.06, 136.04, 201.94]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>test</td>\n",
              "      <td>{'tokens': ['skiier', 'in', 'red', 'pants'], '...</td>\n",
              "      <td>COCO_train2014_000000380440_491042.jpg</td>\n",
              "      <td>[374.31, 65.06, 136.04, 201.94]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>test</td>\n",
              "      <td>{'tokens': ['there', 'is', 'red', 'colored', '...</td>\n",
              "      <td>COCO_train2014_000000419645_398406.jpg</td>\n",
              "      <td>[93.95, 83.29, 504.61, 290.57]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>test</td>\n",
              "      <td>{'tokens': ['a', 'shiny', 'red', 'vintage', 'p...</td>\n",
              "      <td>COCO_train2014_000000419645_398406.jpg</td>\n",
              "      <td>[93.95, 83.29, 504.61, 290.57]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>test</td>\n",
              "      <td>{'tokens': ['a', 'apple', 'desktop', 'computer...</td>\n",
              "      <td>COCO_train2014_000000478885_124383.jpg</td>\n",
              "      <td>[338.8, 82.19, 147.34, 157.37]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95005</th>\n",
              "      <td>train</td>\n",
              "      <td>{'tokens': ['the', 'larger', 'banana', 'is', '...</td>\n",
              "      <td>COCO_train2014_000000003518_1042682.jpg</td>\n",
              "      <td>[141.7, 20.68, 359.02, 144.57]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95006</th>\n",
              "      <td>train</td>\n",
              "      <td>{'tokens': ['a', 'guy', 'in', 'black', 'jacket...</td>\n",
              "      <td>COCO_train2014_000000302199_473946.jpg</td>\n",
              "      <td>[141.34, 86.71, 115.33, 288.75]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95007</th>\n",
              "      <td>train</td>\n",
              "      <td>{'tokens': ['the', 'man', 'in', 'the', 'dark',...</td>\n",
              "      <td>COCO_train2014_000000302199_473946.jpg</td>\n",
              "      <td>[141.34, 86.71, 115.33, 288.75]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95008</th>\n",
              "      <td>train</td>\n",
              "      <td>{'tokens': ['a', 'person', 'in', 'red', 'dress...</td>\n",
              "      <td>COCO_train2014_000000573297_472971.jpg</td>\n",
              "      <td>[31.71, 67.03, 352.29, 572.97]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95009</th>\n",
              "      <td>train</td>\n",
              "      <td>{'tokens': ['man', 'wearing', 'a', 'red', 'cos...</td>\n",
              "      <td>COCO_train2014_000000573297_472971.jpg</td>\n",
              "      <td>[31.71, 67.03, 352.29, 572.97]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>95010 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       split                                          sentences   \n",
              "0       test  {'tokens': ['the', 'man', 'in', 'yellow', 'coa...  \\\n",
              "1       test  {'tokens': ['skiier', 'in', 'red', 'pants'], '...   \n",
              "2       test  {'tokens': ['there', 'is', 'red', 'colored', '...   \n",
              "3       test  {'tokens': ['a', 'shiny', 'red', 'vintage', 'p...   \n",
              "4       test  {'tokens': ['a', 'apple', 'desktop', 'computer...   \n",
              "...      ...                                                ...   \n",
              "95005  train  {'tokens': ['the', 'larger', 'banana', 'is', '...   \n",
              "95006  train  {'tokens': ['a', 'guy', 'in', 'black', 'jacket...   \n",
              "95007  train  {'tokens': ['the', 'man', 'in', 'the', 'dark',...   \n",
              "95008  train  {'tokens': ['a', 'person', 'in', 'red', 'dress...   \n",
              "95009  train  {'tokens': ['man', 'wearing', 'a', 'red', 'cos...   \n",
              "\n",
              "                                     file_name   \n",
              "0       COCO_train2014_000000380440_491042.jpg  \\\n",
              "1       COCO_train2014_000000380440_491042.jpg   \n",
              "2       COCO_train2014_000000419645_398406.jpg   \n",
              "3       COCO_train2014_000000419645_398406.jpg   \n",
              "4       COCO_train2014_000000478885_124383.jpg   \n",
              "...                                        ...   \n",
              "95005  COCO_train2014_000000003518_1042682.jpg   \n",
              "95006   COCO_train2014_000000302199_473946.jpg   \n",
              "95007   COCO_train2014_000000302199_473946.jpg   \n",
              "95008   COCO_train2014_000000573297_472971.jpg   \n",
              "95009   COCO_train2014_000000573297_472971.jpg   \n",
              "\n",
              "                                  bbox  \n",
              "0      [374.31, 65.06, 136.04, 201.94]  \n",
              "1      [374.31, 65.06, 136.04, 201.94]  \n",
              "2       [93.95, 83.29, 504.61, 290.57]  \n",
              "3       [93.95, 83.29, 504.61, 290.57]  \n",
              "4       [338.8, 82.19, 147.34, 157.37]  \n",
              "...                                ...  \n",
              "95005   [141.7, 20.68, 359.02, 144.57]  \n",
              "95006  [141.34, 86.71, 115.33, 288.75]  \n",
              "95007  [141.34, 86.71, 115.33, 288.75]  \n",
              "95008   [31.71, 67.03, 352.29, 572.97]  \n",
              "95009   [31.71, 67.03, 352.29, 572.97]  \n",
              "\n",
              "[95010 rows x 4 columns]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# to test prints the dataframe that composes the dataset\n",
        "dataset.dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJq1udMa0WW-"
      },
      "source": [
        "### Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkfKW4cL0ToQ"
      },
      "outputs": [],
      "source": [
        "# class that defines the baseline model\n",
        "\n",
        "class VisualGrounding(torch.nn.Module):\n",
        "    def __init__(self, yolo_version, clip_version, local_path, img_path):\n",
        "        super(VisualGrounding, self).__init__()\n",
        "        self.local_path = local_path\n",
        "        self.img_path = img_path\n",
        "        # initialize models\n",
        "        self.yolo = torch.hub.load(\n",
        "            'ultralytics/yolov5', yolo_version, pretrained=True)\n",
        "        self.clip, self.preprocess = clip.load(clip_version)\n",
        "\n",
        "        # define metrics\n",
        "        treshold = 0.5\n",
        "        transform = torchvision.transforms.Compose([\n",
        "            torchvision.transforms.ToTensor(),\n",
        "            torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                             std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        self.metrics = Metrics(self.clip, transform, treshold)\n",
        "\n",
        "    def forward(self, img_path, sentence):\n",
        "        max_similarity = 0\n",
        "        max_image = None\n",
        "        max_bbox = None\n",
        "\n",
        "        yolo_output = self.yolo(self.local_path+img_path)\n",
        "\n",
        "        original_img = Image.open(self.local_path+img_path).convert(\"RGB\")\n",
        "\n",
        "        root = get_root(yolo_output, sentence, self.clip, self.yolo)\n",
        "\n",
        "        for i in range(len(yolo_output.xyxy[0])):\n",
        "            if root != \"empty\" and self.yolo.names[int(yolo_output.pred[0][i][5])] != root:\n",
        "                continue\n",
        "            x1 = yolo_output.xyxy[0][i][0].cpu().numpy()\n",
        "            x1 = np.rint(x1)\n",
        "            y1 = yolo_output.xyxy[0][i][1].cpu().numpy()\n",
        "            y1 = np.rint(y1)\n",
        "            x2 = yolo_output.xyxy[0][i][2].cpu().numpy()\n",
        "            x2 = np.rint(x2)\n",
        "            y2 = yolo_output.xyxy[0][i][3].cpu().numpy()\n",
        "            y2 = np.rint(y2)\n",
        "\n",
        "            img_cropped = original_img.crop((x1, y1, x2, y2))\n",
        "\n",
        "            img = self.preprocess(img_cropped).cuda().unsqueeze(0)\n",
        "            text = clip.tokenize([sentence]).cuda()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                image_features = self.clip.encode_image(img).float()\n",
        "                text_features = self.clip.encode_text(text).float()\n",
        "\n",
        "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "            similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
        "\n",
        "            if similarity > max_similarity:\n",
        "                max_similarity = similarity\n",
        "                max_image = img_cropped\n",
        "                max_bbox = yolo_output.xyxy[0][i]\n",
        "\n",
        "        if max_image is None:\n",
        "            #set bbox to the whole image\n",
        "            max_bbox = [0, 0, original_img.width, original_img.height]\n",
        "            max_image = original_img\n",
        "            print(\"No bbox found for sentence: \", sentence)\n",
        "            print(max_bbox)\n",
        "            print(\"Image: \", max_image)\n",
        "\n",
        "        return max_bbox, max_image\n",
        "\n",
        "    def evaluate(self, img_path, sentence, gt, original_img):\n",
        "        bbox = convert_bbox(gt, original_img)\n",
        "        gt_crop = original_img.crop(bbox)\n",
        "        prediction_bbox, prediction_img = self.forward(img_path, sentence)\n",
        "        self.metrics.update(prediction_bbox, bbox, prediction_img, gt_crop)\n",
        "        return prediction_bbox, prediction_img\n",
        "\n",
        "    def reset_metrics(self):\n",
        "        self.metrics.reset()\n",
        "\n",
        "    def get_metrics(self):\n",
        "        return self.metrics.to_string()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHcFXg1R0cgv"
      },
      "source": [
        "### Model evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Csy-JLatEhkf"
      },
      "outputs": [],
      "source": [
        "# dataset load\n",
        "dataset = Coco(local_annotations + 'instances.json',\n",
        "               local_annotations + \"refs(umd).p\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXTbixSEqcQe",
        "outputId": "98f1d75d-93ef-43ef-b4fb-c876dc0682bf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /home/pappol/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 ðŸš€ 2023-5-25 Python-3.11.3 torch-2.0.1 CUDA:0 (NVIDIA GeForce RTX 3070 Ti, 7949MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5x summary: 444 layers, 86705005 parameters, 0 gradients\n",
            "Adding AutoShape... \n"
          ]
        }
      ],
      "source": [
        "# model load\n",
        "model = VisualGrounding('yolov5x', 'ViT-B/32', local_path, local_annotations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJXjlmnb0a80",
        "outputId": "8de29533-5333-4d15-eeec-1cc1c94ffec1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean IoU: 0.5626, Recall@0.5 IoU: 0.5780, Mean Cosine Similarity: 0.8914, Mean Euclidean Distance: 4.3831\n"
          ]
        }
      ],
      "source": [
        "# evaluation of the model\n",
        "validate(model, dataset.get_validation())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvdyH-aLLrqb",
        "outputId": "ba0f3a3b-3f80-4ce1-a095-6d6ee982145d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No bbox found for sentence:  a silver cell phone\n",
            "[0, 0, 640, 480]\n",
            "Image:  <PIL.Image.Image image mode=RGB size=640x480 at 0x7EFD973CB290>\n",
            "No bbox found for sentence:  Blue and grey cell phone on the right side of a purse.\n",
            "[0, 0, 640, 480]\n",
            "Image:  <PIL.Image.Image image mode=RGB size=640x480 at 0x7EFD9750E3D0>\n",
            "Mean IoU: 0.5695, Recall@0.5 IoU: 0.5818, Mean Cosine Similarity: 0.8926, Mean Euclidean Distance: 4.3333\n"
          ]
        }
      ],
      "source": [
        "validate(model, dataset.get_test())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}